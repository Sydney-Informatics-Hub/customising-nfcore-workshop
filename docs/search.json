[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows with nf-core",
    "section": "",
    "text": "This course is currently under development\nThis workshop will set you up with the foundational knowledge required to run and customise nf-core workflows in a reproducible manner. Using the nf-core/rnaseq workflow as an example, we will step through essential features common across all nf-core workflows. We will explore ways to adjust the workflow parameters based on the needs of your dataset and configuration the workflow to run on your computational environment.\n\nTrainers\n\nGeorgie Samaha, Sydney Informatics Hub\nCali Willet, Sydney Informatics Hub\nChris Hakkaart, Seqera Labs\n\n\n\nTarget audience\nThis workshop is suitable for people who are familiar with working at the command line interface and have some experience running Nextflow and nf-core workflows.\n\n\nPrerequisites\n\nExperience navigating the Unix command line\nFamiliarity with Nextflow and nf-core workflows\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nCode of Conduct\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up your computer\nFollow these instructions to install VSCode and login to your Nimbus instance.\n\n\nDay 1: Introduction to nf-core\n\n\n\nDay 2: Customising nf-core\nWrite, run, adjust, and re-run an nf-core workflow as we step through various customisation scenarios.\n\n\n\n\n\nCourse survey\nPlease fill out our course survey before you leave. Help us help you! üòÅ\n\n\nCredits and acknowledgements\nThis workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera Labs, Pawsey Supercomputing Research Centre, and Australia‚Äôs National Research Education Network (AARNet) enabled through the Australian BioCommons (NCRIS via Bioplatforms Australia). This workshop was developed as a part of the Australian BioCommons Bring Your Own Data Platforms project.\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Some tips and tricks",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "In this workshop series, we will be using Pawsey‚Äôs Nimbus cloud. The Pawsey Supercomputing Research Centre is one of two, Tier-1, High Performance Computing facilities in Australia. Their Nimbus cloud platform is an accessible and flexible solution for bioinformatics applications that may not be suitable for large-scale hgih performance computing (HPC) machines including:\n\nDeveloping and refining scalable workflows in prepration for HPC allocation applications.\nWorkflows with long runtimes that excede wall time queue limits on HPC facilities.\nComplex data-bound workflows with variable compute resource profiles that are common in bioinformatics pipelines.\n\nThe main requirements for this workshop are a personal computer with:\n\nA web broswer\nTerminal or integrated development environment (IDE) application\n\nOn this page you will find instructions on how to set up a terminal application and web browser on your computer and how to connect to Nimbus. Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop.\nTo connect to your Nimbus instance, you will need either a terminal or integrated development environment (IDE) application installed on your computer. While we recommend you use the Visual Studio Code IDE for this workshop, we have also provided directions for installing and using a terminal applications below.\n\nOption 1: Install and set up Visual Studio Code\nVisual Studio Code is a lightweight and powerful source code editor available for Windows, macOS and Linux computers.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nLinux\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\n\nInstall the Live Server extension. Click on the blue install button.\n\n\n\nLogin via Visual Studio Code\n\nConnect to your instance with VS code by adding the host details to your ssh config file.\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Open SSH configuraiton file\nAdd new entry, filling out host name and identity file:\n\nHost nfcoreWorkshop\n  HostName 146.118.XX.XXX  \n  User training     \nConnect to this address\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\n\n\n\n\nOption 2: Install and set up a terminal application\nThe terminal applications available to you will depend on your operating system.\n\nLinux terminals\nIf you use Linux, chances are you already know your shell and how to use it. Basically, just open your preferred terminal program and off you go!\n\n\nOS X (Mac)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‚Äòterminal‚Äô. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\n\n\nWindows\nWe recommend MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‚ÄòHome Edition‚Äô select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‚Äòstart local terminal‚Äô (and install Cygwin if prompted)\n\n\n\n\nLogin via Terminal\nTo log in to Nimbus, we will use a Secure Shell (SSH) connection. To connect, you need 3 things: 1. The assigned IP address of your instance (i.e.¬†###.###.##.###). Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop. 2. Your login name. In our case, this will be training for all participants. 3. Your password. All participants will be provided with a password at the beginning of the workshop.\nTo log in, type the following into your terminal, using your login name and the instance‚Äôs IP address:\nssh training@###.###.###.###\nYou will receive a message saying:\nThe authenticity of host 'XXX.XXX.XX.XXX (XXX.XXX.XX.XXX)' can't be established.\nRemember your host address will be different than the one above. There will then be a message saying:\nAre you sure you want to continue connecting (yes/no)?\nIf you would like to skip this message next time you log in, answer ‚Äòyes‚Äô. It will then give a warning:\nWarning: Permanently added 'XXX.XXX.XX.XXX' (ECDSA) to the list of known hosts.\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\n\n\n\n\n\n\nPay Attention\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You‚Äôll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.0_intro.html",
    "href": "notebooks/1.0_intro.html",
    "title": "Welcome to session 1",
    "section": "",
    "text": "In Session 1 we will introduce tools, formats, concepts, and ideas that will be utilized in Session 2.\nMany of the examples used in Session 1 are not ‚Äúreal world‚Äù examples.\n\n1.0.1. Create a new work directory\nIt is good practice to organize projects into their own folders to make it easier to track and replicate experiments over time.\nStart by creating a new directory for all of today‚Äôs activities and move into it:\nmkdir ~/session1 && cd $_\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html",
    "href": "notebooks/1.1_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\nNextflow‚Äôs portability is enabled by its ability to separate workflow implementation from the configuration settings required to execute it. In this lesson we will use configuration files to define specifications required to execute an nf-core pipeline on our compute environment. While nf-core workflows are designed to be portable and work out of the box, sometimes you will need to customise the workflow‚Äôs configuration so that it can run on your environment. The nf-core developer community currently offer a number of ways to configure nf-core workflows.\n\n2.3.1. Default nf-core configuration\nRecall that when a main.nf file is run for any Nextflow workflow, Nextflow looks for configuration files in multiple locations to determine how to execute the workflow and its processes. One of the files Nextflow will always look for is nextflow.config. Currently, all nf-core workflows use a nextflow.config file and a conf/base.config file to define the default execution settings and parameters of a workflow.\nLet‚Äôs take a look at the nf-core/rnaseq nextflow.config file:\ncat nf-core-rnaseq-3.11.1/workflow/nextflow.config\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat is the default aligner parameter being applied?\nWhat default max memory, cpu, and walltime resources have been specified?\nWhat config file is loaded by default for all nf-core workflows?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nInside the Global default params {} section, on line 58 of the nextflow.config under // Alignment:\n\naligner = 'star_salmon'\n\nInside the Global default params {} section, on lines 120-124 of the nextflow.config under // Max resource options:\n\nmax_memory = '128.GB'\nmax_cpus   = 16\nmax_time   = '240.h'\n\nInside Global default params {} section, on line 128 of the nextflow.config under // Load base.config by default for all pipelines:\n\nincludeConfig 'conf/base.config'\n\n\n\n\n\n2.3.2. When to use a custom config file\nThere are a number of situations in which you may want to write a custom configuration file:\n\nTo override the default resource allocations of the workflow specified in the nextflow.config\nTo override the default resource allocations for a process specified in conf/base.config\nTo use a different software installation method than those supported by nf-core\nTo run a workflow on an HPC and interact with a job scheduler like PBSpro or SLURM\n\nUsing a custom configuration file is good practice to ensure that your pipeline runs efficiently and reproducibly on your compute environment. It also allows you to easily share the pipeline with others who can use your custom config file to run it in the same computational environment.\nWe will write a custom configuration file to override the default configurations of the workflow with those that are suitable for our Nimbus instances. We‚Äôre going to replace 3 flags in our run command with this file:\n\n-profile singularity\n--max_memory 6.GB\n--max_cpus 2\n\n\n\n\n\n\n\nWhy should I be concerned with computational efficiency? üåè\n\n\n\nBioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices. This paper makes for an interesting read about the carbon footprint of bioinformatics workflows and tools!\n\n\n\n\n2.3.3. Customise resource configuration\nOpen a new file called custom-nimbus.config and start writing some Nextflow code by adding:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {}\n}\nUsing the profiles scope in a configuration file groups attributes that belong to the same profile, in our case workshop. Inside this workflow profile, let‚Äôs remove the need for the -profile singularity flag from our run command by enabling Singularity by adding another scope called Singularity:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }}\n  }\nNextflow has a number of options for using Singularity that allow you to control how containers are executed. We are using:\n\nenabled to use Singularity to manage containers automatically\nautoMounts to allow Nextflow to automatically mount host paths when a container is executed\ncacheDir to specify the directory Singularity images can be pulled from\n\nNow let‚Äôs address those two resource parameters --max_memory 6.GB and --max_cpus 2. At the same level as the singularity {} scope, add a parameters scope and specify each parameter underneath:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }\n    params {\n      max_cpus   = 2\n      max_memory = '6.GB'      \n    }}\n  }\n\n\n\n\n\n\nTake note!\n\n\n\nIn Nextflow, scope organisation and heirarchy is indicated by curly bracket ({}) notation, not by text indentation!\n\n\nRerun the pipeline:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-3 \\\n  -resume\n\n\n2.3.4. Apply an institutional config file\nWe have created an nf-core config for Pawsey‚Äôs Nimbus cloud and shared it at the nf-core/configs repository. This config file was downloaded with the workflow code. Take a look:\ncat nf-core-rnaseq-3.11.1/configs/conf/pawsey_nimbus.config\nThis configuration file provides a few different profiles that match the Nimbus instance flavours currently available, as well as Docker and Singularity container engines. Take a look at the c2r8 scope, looks a lot like our custom-nimbus.config:\n  c2r8 {\n    params {\n      max_cpus   = 2\n      max_memory = '6.GB'\n    }\n  }\nLet‚Äôs rerun the workflow with the -resume function and the institutional config parameters, instead of our custom file and see if anything changes:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile pawsey_nimbus,singularity,c2r8 \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-3 \\\n  -resume\nIn the nf-core/rnaseq pipeline‚Äôs configuration message printed to the screen at run time, we can see the institutional config has been picked up and our resource parameters are correctly being applied.\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.4_users.html",
    "href": "notebooks/1.4_users.html",
    "title": "nf-core for users",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn more about nf-core tooling for users.\nUse nf-core list to view information about nf-core workflows.\nUse nf-core launch to create a parameters file.\nUse nf-core download to download a workflow and it‚Äôs software.\n\n\n\n\n1.4.1 nf-core tools for users\nnf-core tools has additional commands to help users execute workflows. Although you do not need to use these commands to execute the nf-core workflows, they can greatly assist and improve and simplify your experience.\nThere are also nf-core tools for developers. However, these will not be covered as a part of this workshop. If you are curious to learn more about these tools you can find more information on the tools page on the nf-core website.\n\n\n1.4.2 nf-core list\nThe nf-core list command can used to print a list of remote nf-core workflows along with your local information.\nnf-core list\n\nThe output shows the latest workflow version number and when it was released. You will also be shown if and when a workflow was pulled locally and whether you have the latest version.\nKeywords can be supplied to help filter the workflows based on matches in titles, descriptions, or topics:\nnf-core list dna\n\nOptions can also be used to sort the workflows by latest release (-s release, default), when you last pulled a workflow locally (-s pulled), alphabetically (-s name), or number by the number of GitHub stars (-s stars).\n\n\n\n\n\n\nChallenge\n\n\n\nTry to filter the list of nf-core workflows for those that are for rna and sort them by stars.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the list command, filter it for rna, and sort by stars:\nnf-core list rna -s stars\n\n\n\n\n\n1.4.2 nf-core launch\nNextflow workflows can have a considerable number of optional command line flags. To help manage these, you can use the nf-core launch command.\nThe command takes one argument - either the name of an nf-core workflow which will be pulled automatically or the path to a directory containing a Nextflow workflow:\nnf-core launch nf-core/&lt;workflow&gt;\nWhen running this command, you will first be asked about which version of a workflow you would like to run. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the workflow parameters for your run. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.\n\nThe launch tool uses the nextflow_schema.json file from a workflow to give parameter descriptions, defaults, and grouping. If no file for the workflow is found, one will be automatically generated at runtime.\nThe launch tool will save your parameter variables as a JSON file called nf-params.json and will suggest an execution command that includes the -params-file flag and your new nf-params.json file.\nThe wizard will ask if you want to launch the Nextflow run. You will also be given the run command and a copy of the JSON file for you to copy and paste if you wish.\nAny profiles or Nextflow options that are set using the wizard will also be included in your run command.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to run the nf-core/rnaseq workflow with the nf-core launch command. Use the latest version of the workflow with the the test and singularity profiles, and name your output directory results:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core launch command for the nf-core/rnaseq workflow and follow the prompts:\nnf-core launch nf-core/rnaseq\nYour final run command should look like this:\nnextflow run nf-core/rnaseq -r 3.11.1 -profile test,singularity -params-file nf-params.json\nYour nf-params.json file should look like this:\n{\n    \"outdir\": \"results\"\n}\n\n\n\n\n\n1.4.3 nf-core download\nSometimes you may need to run an nf-core workflow on a server or HPC system that has no internet connection. In this case, you will need to fetch the workflow files and manually transfer them to your system.\nTo make this process easier and ensure accurate retrieval of correctly versioned code and software containers, nf-core has the download helper tool.\nThe nf-core download command will download both the workflow code and the institutional nf-core/configs files. It can also optionally download singularity image file.\nnf-core download\nIf run without any arguments, the download tool will interactively prompt you for the required information. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:\n\nPipeline name\n\nName of workflow you would like to download.\n\nPipeline revision\n\nSelect the revision you would like to download.\n\nPull containers\n\nChoose if you would like to download Singularity images.\nThis will only work if you have Singularity installed.\n\nChoose compression type\n\nChoose compression type for Singularity images.\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "2.1. Design a run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore pipeline deployment and set up\n\n\n\n\n2.1.1. Introducing the case study\nWe are working with a subset dataset from a knockout mouse model study by Corley et al.¬†(2016). The authors used the mouse model to simulate the role of a specific gene (Gtf2ird1) in Williams-Beuren Syndrome (WBS), a rare genetic disease in humans.\nIn this session, we are performing the pre-processing steps to generate a set of files that can be used to perform differential expression analysis. In deciding whether or not the nf-core/rnaseq pipeline was suitable for reproducing the results presented in this study, we considered a number of factors, including:\n\n\n\n\n\n\n\n\nConsideration\nQuestions to ask of our experiment\nWhy\n\n\n\n\nSize of dataset\nNumber of samples and data volume\nScale of data impacts computational efficiency of the pipeline\n\n\nInput data\nType of RNA sequenced, availability of reference files\nDetermines if we meet input requirements of pipeline\n\n\nResearch questions\nSuitability of pipeline outputs\nNeed the right processed data for downstream analysis\n\n\nComputational resources\nCPU, memory, RAM available and minimum requirements of the pipeline\nDetermines if we have enough resources to run the pipeline\n\n\nTool preferences\nSuitability of each tool, required inputs and outputs\nPipeline offers multiple choices for some stages, determines which choices we make\n\n\n\nWe consulted the nf-core/rnaseq documentation to confirm that nf-core/rnaseq is a suitable pipeline for our application. We have sketched out our experimental design, those considerations, and our choices below.\n\n\n\n2.1.2. Download the pipeline code\nStart by creating a new directory for all of today‚Äôs activities and move into it:\nmkdir ~/session2 && cd $_\nWe recommend that you keep a local copy of a pipeline‚Äôs code for the sake of reproducibility and record keeping. There are a number of ways to download a nf-core pipeline to your machine. We recommend using either the nf-core tools utility to download a pipeline for offline use or git. Today, we will download most recent version (3.11.1) of the pipeline using the nf-core tools utility, like we did in Session 1.\nTake a look at the download options available to us:\nnf-core download --help\n\nRun the following command:\nnf-core download rnaseq \\\n  --revision 3.11.1 \\\n  --outdir ~/session2/nf-core-rnaseq-3.11.1 \\\n  --container singularity \\\n  --compress none \\\n  --singularity-cache-only\nBy default the most recent version of a pipeline will be downloaded. We used:\n\n--revision/-r to specify the pipeline version\n--outdir/-o to download the pipeline to our current working directory\n--container/-c to download software containers with Singularity\n--singularity-cache-only to cache and not copy containers to our output directory\n--compress/-x none to not compress downloaded files\n\nYou will be prompted to define a Singularity cache directory to store container images ($NXF_SINGULARITY_CACHEDIR). Specify the path we used in Session 1:\nexport NXF_SINGULARITY_CACHEDIR=/home/training/singularity_cache\nThe nf-core tools utility will download the pipeline files and centralised configs but not the containers, given they already exist in our specified cache.\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nIn situations where you might not wish to use the nf-core tools utility, download the nf-core/rnaseq source code from it‚Äôs GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\n\n‚õî BEWARE ‚õî this method will download a copy of the pipeline with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust some paths specified in the upcoming lessons accordingly.\n\n\n\n\nInside your nf-core-rnaseq-3.11.1 workflow directory, you should see 2 subdirectories:\nls -l nf-core-rnaseq-3.11.1\ntotal 8\ndrwxrwxr-x  7 ubuntu ubuntu 4096 Apr 21 02:08 configs\ndrwxrwxr-x 12 ubuntu ubuntu 4096 Apr 21 02:08 workflow\nRemember, as we ran the download command, a number of INFO logs were printed to the screen. The workflow files from GitHub were downloaded to the workflow directory and the centralised configs from GitHub were downloaded to the configs directory:\n\nFor this session we will be working with the workflow directory:\nls -l nf-core-rnaseq-3.11.1/workflow\ntotal 216\n-rwxrwxr-x 1 ubuntu ubuntu 58889 Apr 21 02:08 CHANGELOG.md\n-rwxrwxr-x 1 ubuntu ubuntu  9681 Apr 21 02:08 CITATIONS.md\n-rwxrwxr-x 1 ubuntu ubuntu  9078 Apr 21 02:08 CODE_OF_CONDUCT.md\n-rwxrwxr-x 1 ubuntu ubuntu  1096 Apr 21 02:08 LICENSE\n-rwxrwxr-x 1 ubuntu ubuntu 10002 Apr 21 02:08 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr 21 02:08 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr 21 02:08 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr 21 02:08 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr 21 02:08 modules\n-rwxrwxr-x 1 ubuntu ubuntu 13970 Apr 21 02:08 modules.json\n-rwxrwxr-x 1 ubuntu ubuntu 10847 Apr 21 02:08 nextflow.config\n-rwxrwxr-x 1 ubuntu ubuntu 42576 Apr 21 02:08 nextflow_schema.json\n-rwxrwxr-x 1 ubuntu ubuntu   359 Apr 21 02:08 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr 21 02:08 subworkflows\n-rwxrwxr-x 1 ubuntu ubuntu  1684 Apr 21 02:08 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 workflows\nWe explored the standard nf-core workflow directory structure in Session 1. The most important files and directories for this session are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq pipeline\n\n\n\n\n\n2.1.3. Build your run command\nAll nf-core pipelines are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. On the command line you can view these options by running:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help\nNotice at the bottom of the print out, there is:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nThree additional parameter sections are hidden from view. This is because they are less commonly used. To view all the pipeline run options on the command line, run:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help --show_hidden_params\n\n\n\n\n\n\nHyphens matter!\n\n\n\nHyphens matter when it comes to parameter flags in nf-core pipelines! Nextflow command-line parameters use one (-), whereas pipeline-specific parameters use two (--). For example: -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\nWe need to customise the default run command, given the following considerations based on our experiment plan above:\n\nWe don‚Äôt need to run the pseudo alignment step (Stage 3)\nWe have chosen to use STAR to align reads\nWe have chosen to use Salmon to estimate transcript abundance\nWe have access to 2 CPUs and 8 Gb of RAM today\nWe have provided the requisite reference data (fasta, gtf, STAR index) for our input dataset\n\n\n\n\n\n\n\nChallenge\n\n\n\nCan you use the nf-core/rnaseq parameters documentation (or the --help flag) and the important considerations above to identify flags we will need to apply to our first workflow run? Don‚Äôt worry about the files required for these flags just yet.\n\nWhat flag would we use to specify our chosen alignment and quantification method?\nWhat flags would we use to specify maximum CPU and memory resources?\nWhat flags would we use to specify the provided fasta, gtf, and STAR index files?\n\nüí° HINT: You will need to look at the reference genome, alignment, and max job request sections.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nGiven we are using STAR and Salmon as our aligner and quantification tool of choice (respectively) and it is the default choice of this pipeline we do not need to provide an --aligner flag. However, if you wanted to provide this for the sake of reproducibility in case things change in the future:\n\n--aligner 'star_salmon'\n\nGiven we have limited computing resources today, we will need to specify a ceiling for both memory and CPUs:\n\n--max_memory '6.GB' \n--max_cpus 2 \n\nGiven we are providing our own subset data for this workshop, we will need to use:\n\n--fasta /path/to/mouse.fa  \n--gtf /path/to/mouse.gtf \n--star_index /path/to/STAR\n\n\n\nFor the sake of expediency, we are using prepared subset data for this session. All the data (including fastqs, reference fasta, gtf, and STAR indexes) are already available on an external file system called CernVM-FS that we can access from our Nimbus instances. Take a look at the files:\n#ls -l /path/to/aarnet-cvmfs/training/workshopMaterials\n\n#Workaround for testing (data not yet on cvmfs)\nwget -O nfcore_materials.tar.gz https://cloudstor.aarnet.edu.au/plus/s/gIBdDhKEwfq2j58/download\ntar -zxvf nfcore_materials.tar.gz\nls -l ~/session2/materials\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Feb 14 05:36 fastqs\ndrwxrwxr-x 3 ubuntu ubuntu 4096 Feb 14 05:46 mm10_reference\n-rw-rw-r-- 1 ubuntu ubuntu  641 Feb 16 05:57 samplesheet.csv\nA sample sheet is a standard input requirement for nf-core pipelines. This sheet specifies:\n\nThe sample name\nRelevant input files for each sample\nOther metadata for each sample\n\n\n\n\n\n\n\nChallenge\n\n\n\nNow that we have the path to our raw sequence files, create the input sample sheet following the nf-core/rnaseq documentation. Your input file will need to be comma-delimited and have the following columns:\n\nsample: custom sample name (use the SRR IDs)\nfastq1: full path to read 1 fastq.gz file\nfastq2: full path to read 2 fastq.gz file\nstrandedness: for this lesson, all should be labelled as forward\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as samplesheet.csv:\nsample,fastq_1,fastq_2,strandedness\nSRR3473989,/home/ubuntu/session2/materials/fastqs/SRR3473989_selected.fastq.gz,,forward\nSRR3473988,/home/ubuntu/session2/materials/fastqs/SRR3473988_selected.fastq.gz,,forward\nSRR3473987,/home/ubuntu/session2/materials/fastqs/SRR3473987_selected.fastq.gz,,forward\nSRR3473985,/home/ubuntu/session2/materials/fastqs/SRR3473985_selected.fastq.gz,,forward\nSRR3473986,/home/ubuntu/session2/materials/fastqs/SRR3473986_selected.fastq.gz,,forward\nSRR3473984,/home/ubuntu/session2/materials/fastqs/SRR3473984_selected.fastq.gz,,forward\n\n\n\n\n\n2.1.4. Run the pipeline\nWe need to store the path to our input and reference data in a variable for our run command:\nmaterials=/home/ubuntu/session2/materials\nGiven we already downloaded the containers, we will point the pipeline to where it can find them so it doesn‚Äôt waste time trying to download them all over again. We can do this by setting the NXF_SINGULARITY_CACHEDIR environmental variable:\nexport NXF_SINGULARITY_CACHEDIR=/home/ubuntu/singularity_cache/\n\n\n\n\n\n\nNextflow has environmental variables!\n\n\n\nEnvironmental variables are used to store information that can be accessed by different programs and scripts running on the system. Nextflow has a number of environmental variables that can be used to override default Nextflow and pipeline behaviours. You can set them on the command line or by using env scopes when writing Nextflow.\n\n\nNow run the pipeline, with the necessary flags and the samplesheet.csv file you just created:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n    --input samplesheet.csv \\\n    -profile singularity \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    --outdir Lesson-1\n\n\n\n\n\n\nü§∑ Zoom check-in! ü§∑\n\n\n\nIs everyone ok?\nYes, move on üëè :clap:\nNo, help! üò¢ :cry:\n\n\n\n\n2.1.5. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 13m 59s\nCPU hours   : 0.4\nSucceeded   : 200\nThe pipeline ran successfully, however note the warning about all samples having failed the strandedness check. We‚Äôll explore that in the next lesson. In the meantime, list (ls -la) the contents of your directory, you‚Äôll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:44 .\ndrwxr-x---  15 ubuntu ubuntu 4.0K Apr 21 01:56 ..\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 03:58 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 371K Apr 21 03:58 .nextflow.log\n-rw-rw-r--   1 ubuntu ubuntu  17K Apr 21 03:50 .nextflow.log.1\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:58 Lesson-1\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 02:08 nf-core-rnaseq-3.11.1\n-rw-rw-r--   1 ubuntu ubuntu  563 Apr 21 03:14 samplesheet.csv\ndrwxrwxr-x 143 ubuntu ubuntu 4.0K Apr 21 03:58 work\nNextflow has created 2 new output directories, work and Lesson-1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe Lesson-1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible defaults. You can adjust some settings as required by applying flags to your run command.\nnf-core pipelines are all built from a template that means they have a standard structure to their code bases\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "2.0. Welcome to session 2",
    "section": "",
    "text": "This session builds on fundamental concepts learned in Session 1 and provide you with hands-on experience in nf-core workflow customisation. Throughout the session we will be working with a case study to apply an nf-core workflow. Each lesson will build on the previous one, so you can gain a deeper understanding of the customisation techniques and the impact they have on the workflow and your results. We will be exploring source code of the nf-core/rnaseq workflow and apply customisations using a parameter file and custom configuration files.\nYou will be writing these files using Nextflow and YAML languages and also running some Nextflow commands and custom Bash code to efficiently extract information from the source code ü§ì.\n\n\n\n\n\n\nNote\n\n\n\nWhile all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to other nf-core workflows and do not require an understanding of RNAseq data processing.\n\n\n\n2.0.1. How to approach open source software communities\nAs with all open source bioinformatics resources, nf-core workflows may not suit all applications. It is important that you understand the needs of your dataset and research questions before deciding on a workflow. nf-core is a community effort powered by life scientists and software developers, who volunteer their time to build, maintain, and support workflows. All nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. What is ‚Äòsensible‚Äô varies dramatically between different experiments, computing environments, and datasets, so these settings might not suit your needs. We recommend approaching the application and customisation of nf-core workflows with the following framework:\nDIAGRAM OF NF-CORE COMMUNITY FEATURE - IMPLICATION FOR WORKFLOWS - Q TO ASK - OUR RECOMMENDATIONS\n\n\n2.0.2. Log in to your instance\n\nOption 1: In Visual Studio Code\nFollow the VSCode set up instructions or use the quick start instructions below to connect to your instance using the command palatte:\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\nOption 2: In a terminal\nFollow the terminal set up instructions or use the quick start instructions below to connect to your instance on the command-line:\n\nRun: ssh training@###.###.###.###\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. These may not always suit your needs.\nTo decide whether an nf-core workflow is the right choice for your experiment you need to understand the needs of your dataset and research questions.\nUse the workflow documentation to understand the requirements for running a workflow.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.3_configEnv.html",
    "href": "notebooks/2.3_configEnv.html",
    "title": "2.3. Configuring a run for your environment",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\nNextflow‚Äôs portability is enabled by its ability to separate workflow implementation from the configuration settings required to execute it. In this lesson we will use configuration files to define specifications required to execute an nf-core pipeline on our compute environment. While nf-core workflows are designed to be portable and work out of the box, sometimes you will need to customise the workflow‚Äôs configuration so that it can run on your environment. The nf-core developer community currently offer a number of ways to configure nf-core workflows.\n\n2.3.1. Default nf-core configuration\nRecall that when a main.nf file is run for any Nextflow workflow, Nextflow looks for configuration files in multiple locations to determine how to execute the workflow and its processes. One of the files Nextflow will always look for is nextflow.config. Currently, all nf-core workflows use a nextflow.config file and a conf/base.config file to define the default execution settings and parameters of a workflow.\nLet‚Äôs take a look at the nf-core/rnaseq nextflow.config file:\ncat nf-core-rnaseq-3.11.1/workflow/nextflow.config\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat is the default aligner parameter being applied?\nWhat default max memory, cpu, and walltime resources have been specified?\nWhat config file is loaded by default for all nf-core workflows?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nInside the Global default params {} section, on line 58 of the nextflow.config under // Alignment:\n\naligner = 'star_salmon'\n\nInside the Global default params {} section, on lines 120-124 of the nextflow.config under // Max resource options:\n\nmax_memory = '128.GB'\nmax_cpus   = 16\nmax_time   = '240.h'\n\nInside Global default params {} section, on line 128 of the nextflow.config under // Load base.config by default for all pipelines:\n\nincludeConfig 'conf/base.config'\n\n\n\n\n\n2.3.2. When to use a custom config file\nThere are a number of situations in which you may want to write a custom configuration file:\n\nTo override the default resource allocations of the workflow specified in the nextflow.config\nTo override the default resource allocations for a process specified in conf/base.config\nTo use a different software installation method than those supported by nf-core\nTo run a workflow on an HPC and interact with a job scheduler like PBSpro or SLURM\n\nUsing a custom configuration file is good practice to ensure that your pipeline runs efficiently and reproducibly on your compute environment. It also allows you to easily share the pipeline with others who can use your custom config file to run it in the same computational environment.\nWe will write a custom configuration file to override the default configurations of the workflow with those that are suitable for our Nimbus instances. We‚Äôre going to replace 3 flags in our run command with this file:\n\n-profile singularity\n--max_memory 6.GB\n--max_cpus 2\n\n\n\n\n\n\n\nWhy should I be concerned with computational efficiency? üåè\n\n\n\nBioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices. This paper makes for an interesting read about the carbon footprint of bioinformatics workflows and tools!\n\n\n\n\n2.3.3. Customise resource configuration\nOpen a new file called custom-nimbus.config and start writing some Nextflow code by adding:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {}\n}\nUsing the profiles scope in a configuration file groups attributes that belong to the same profile, in our case workshop. Inside this workflow profile, let‚Äôs remove the need for the -profile singularity flag from our run command by enabling Singularity by adding another scope called Singularity:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }}\n  }\nNextflow has a number of options for using Singularity that allow you to control how containers are executed. We are using:\n\nenabled to use Singularity to manage containers automatically\nautoMounts to allow Nextflow to automatically mount host paths when a container is executed\ncacheDir to specify the directory Singularity images can be pulled from\n\nNow let‚Äôs address those two resource parameters --max_memory 6.GB and --max_cpus 2. At the same level as the singularity {} scope, add a parameters scope and specify each parameter underneath:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }\n    params {\n      max_cpus   = 2\n      max_memory = '6.GB'      \n    }}\n  }\n\n\n\n\n\n\nTake note!\n\n\n\nIn Nextflow, scope organisation and heirarchy is indicated by curly bracket ({}) notation, not by text indentation!\n\n\nRerun the pipeline:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-3 \\\n  -resume\n\n\n\n\n\n\nBonus exercise: applying an institutional config\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.2_params.html",
    "href": "notebooks/2.2_params.html",
    "title": "2.2. How to use a parameters file",
    "section": "",
    "text": "Objectives\n\n\n\n\nWrite a parameter file\nUnderstand the YAML file format\nRerun the workflow using a params file\nUnderstand the use of the params file for reproducible and transparent research\n\n\n\nIn Nextflow, parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. They are defined by the params scope. Parameters are also used in nf-core workflows to specify input and output files and define other aspects of workflow execution. Each nf-core pipeline comes with a default set of parameters that can be customised to suit specific requirements.\nIn the previous lesson we supplied these parameters as flags in our run command. Nextflow also allows us to pass all parameters to a pipeline‚Äôs run command using the -params-file flag and a JSON or YAML file. In this lesson we‚Äôre going to adjust our run command and rerun the pipeline using a parameter file, rather than specifying all parameters as separate flags in the run command.\n\n2.3.1. Revise the run command\nWhile our pipeline completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 13m 59s\nCPU hours   : 0.4\nSucceeded   : 200\n\n\n\n\n\n\nHandling dodgy error messages ü§¨\n\n\n\nThe first warning message isn‚Äôt very descriptive (see this pull request). You might come across small but inconsequential issues like this when running nf-core pipelines, too. End user feedback is always helpful for software developers, so consider submitting a GitHub issue or starting a discussion in the relevant Slack channel so others are aware and it can be addressed by developers.\n\n\nLet‚Äôs take a look at the MultiQC report, as directed by the second message. You can find this report in the Lesson-1/ directory:\nls -la Lesson-1/multiqc/star_salmon/\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS Code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe issue here is provided strandedness that we specified in our samplesheet.csv and inferred strandedness identified by Salmon do not match. Look‚Äôs like we‚Äôve incorrectly specified strandedness as forward in the samplesheet.csv (my mistake! üòë) when our raw reads actually show an equal distribution of sense and antisense reads. Let‚Äôs check how the nf-core/rnaseq pipeline ran the Salmon quantification process so we can explore whether or not this will potentially have an impact on our results.\n\n\n2.3.2. Identify the run command for a process\nTo understand what command is being run for a process, you can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied, this may not be straight forward. To understand what Salmon is doing, we‚Äôre going to use the nextflow log command and some custom bash code to track down the hidden .command.sh scripts for each Salmon quant process to find out how Salmon quant identified library type.\nUse the Nextflow log command to reveal information about executed pipelines in our working directory:\nnextflow log\nThis will print a list of executed pipelines, by default:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                                                                                                                                                                                                                                                                                                                                                    \n2023-04-21 00:30:30     -               friendly_montalcini     -       f421ddc35d      685266bb-b99b-4945-9a54-981e8f4b1b07    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help                                                                                                                                                                                                                                                                                                 \n2023-04-21 00:40:58     15m 36s         nauseous_lamarck        OK      f421ddc35d      055e7b7f-c3ea-4fd9-a915-02343099939e    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --input samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory 6.GB --max_cpus 2 --outdir Lesson-1\nAll recent runs will be listed in this file, with the most recent at the top. Let‚Äôs query the logs for the Lesson-1 run. Run the command below after filling in your unique run name. For example:\nnextflow log nauseous_lamarck\nThat command will list out all the work subdirectories for all processes run. Recall that the actual tool commands issued by the nexflow processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these command scripts.\nBut how to find them? ü§î\nLet‚Äôs add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=nauseous_lamarck\nAnd let‚Äôs save the tool of interest (salmon) in another bash variable:\ntool=salmon\nNext, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‚Äòsalmon‚Äô. There are multiple salmon steps in the workflow, inlcuding index and an R script. We are looking for salmon quant which performs the read quantification. For example:\n/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh\n/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh\n/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh\n/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh\n/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh\n/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh\n/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh\n/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh\n/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh\n/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh\n/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh\n/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh\n/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh\nCompared with the salmon quant main.nf file, we get more information from the .command.sh process scripts:\ncat rnaseq/modules/nf-core/salmon/quant/main.nf\n\nLooking at the nf-core/rnaseq documentation, we can see:\n. Library type is automatically inferred based on the $strandedness variable . Library type can be adjusted using Salmon‚Äôs --libType= flag and the nf-core $strandedness variable.\nFollowing the recommendations in the Salmon documentation, we‚Äôre going to override this default with the nf-core/rnaseq pipeline‚Äôs --salmon_quant_libtype A parameter.\n\n\n2.3.3. Write a parameter file\nNextflow accepts either YAML or JSON formats for parameter files. YAML and JSON are formats for storing data objects and structures in a file and either is a valid choice for building your parameters file. We will create and apply a YAML file with our inputs for our second run, just because its easier to read. YAML files follow these syntax rules:\n\nFiles use indentation to define nesting and heirarchy\nFiles can use comment lines prefaced by a # character\nFiles consist of key-value pairs\nEach key is followed by a colon (:) and a space\nThe value can be any valid YAML data type (i.e.¬†string, number, boolean, list, or dictionary).\nStrings can be specified using single quotes ('), double quotes (\"), and plain text (boolean).\nYAML is case sensitive\nFiles should have a .yaml or .yml as the file extension\nYAML does not allow the use of tabs, only spaces\nLists begin with a hyphen\nEach key and value must be unique\nThe order of keys or values in a list doesn‚Äôt matter\n\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the syntax rules above:\n\nAdd the appropriate YAML fields from your previous run command to the params file below. You‚Äôll be sharing this file with a collaborator working on a different computational infrastructure but the same input and reference files.\n\noutdir: \"Lesson-2\"\nsalmon_quant_libtype: \"A\"\nsave_trimmed: true \nsave_unaligned: true \nextra_salmon_quant_args: \"--numBootstraps 10\"\nüí° HINT: you will only need to specify nf-core/rnaseq parameters (i.e.¬†-- flags)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as workshop-params.yaml:\ninput: \"samplesheet.csv\" \ngtf: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\"\nfasta: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\"\nstar_index: \"/home/ubuntu/session2/materials/mm10_reference/STAR\" \noutdir: \"Lesson-2\"\nsalmon_quant_libtype: \"A\"\nsave_trimmed: true \nsave_unaligned: true \nextra_salmon_quant_args: \"--numBootstraps 10\"\n\n\n\nAny of the pipeline parameters can be added to the parameters file in this way.\n\n\n2.3.4. Pass the parameter file to the run command\nOnce your params file has been saved, run the following command. Observe:\n\nThe command is shorter thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\n\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-2 \\\n  -resume                        \nAs we‚Äôve used the -resume flag, the initial pre-processing stage and STAR alignments should to be restored from cache and only the Salmon and downstream QC steps will be recomputed. The rerun workflow should complete in a much shorter time and those warnings should have gone away!\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 2m 1s\nCPU hours   : 0.4 (85.9% cached)\nSucceeded   : 15\nCached      : 185\n\n\n2.3.5. Manage parameters with nf-core schema\nnf-core pipelines all have a nextflow_schema.json file in their root directory which explains all the parameters used by the workflow. You can use the nf-core schema tool to validate pipeline parameters, build a pipeline schema, deploy the documentation for a pipeline schema, or add new parameters to a pipeline schema (take a look at this nf-core bytesize talk! for a demonstration). Have a go at these bonus exercises, beware adjusting the nextflow_schema.json file will mean any subsequent workflow run will have to start from scratch again and prevent you applying the -resume function.\n\n\n\n\n\n\nBonus exercise: nf-core schema build to customise help command\n\n\n\n\n\nLet‚Äôs use this utility to simplify the workflow run process for our colleague who is new to nf-core pipelines and RNAseq data processing and works in the same compute environment as us. We‚Äôll customise our local copy of the nextflow_schema.json file to show only required paramerters for our experiment in the --help message to make it easier for them to write their own run command:\n\nRun the command below and follow the prompts to launch the schema web builder:\n\nnf-core schema build -d nf-core-rnaseq-3.11.1/workflow/\n\n\nFor the Input/output options group, tick the Required box for the following and the Hide box for everything else:\n\n\n‚úÖ input\n‚úÖ outdir\n\n\nFor the UMI filtering options group, tick the Hide box at the group level.\nFor the Read filtering options group, tick the Hide box at the group level.\nFor the Reference genome options group, tick the Required box for the following and the Hide box for everything else:\n\n\n‚úÖ fasta\n‚úÖ gtf\n‚úÖ star_index\n\n\nFor the Read trimming options group, tick the Required box for the following and the Hide box for everything else:\n\n\n‚úÖ save_trimmed\n\n\nFor the Alignment options group, tick the Required box for the following and the Hide box for everything else:\n\n\n‚úÖ aligner\n‚úÖ save_unaligned\n\n\nFor the Process skipping options group, tick the Hide box at the group level.\nFor the Institutional config options group, tick the Hide box at the group level.\nFor the Max job request options group, tick the Required box for:\n\n\n‚úÖ max_cpus\n‚úÖ max_memory\n\n\nFor the Generic options group, untick the Required and Hide boxes for:\n\n\nüü© help\nüü© version\nüü© multiqc_config\n\nTick the Hide box for everything else.\n\nNow re-build the schema by hitting the blue ‚òëÔ∏è Finished box in the top right of your browser.\nReturn to your terminal and look at your updated help message:\n\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help\n\n\n\n\n\n\n\n\n\n\nBonus exercise: nf-core schema docs to customise documentation\n\n\n\n\n\nThe schema docs utility can be used to output parameter documentation to html or markdown format that you could include in custom documentation. To print\nnf-core schema docs \\\n  nf-core-rnaseq-3.11.1/workflow/nextflow_schema.json --force \\\n  &gt; custom-params.md\n\n\n\n\n\n\n\n\n\nBonus exercise: nf-core schema validate to check params file\n\n\n\n\n\nThe schema validate utility can be used to validate your specified parameters against the provided pipeline schema. Unfortunately, this won‚Äôt work for your workshop-params.yaml, so we first need to convert this to a JSON file format. To do this, run the following from the session2 directory:\n\nConvert the YAML file to JSON format and save this file as workshop-params.json\n\n{\n    \"input\": \"samplesheet.csv\",\n    \"gtf\": \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\",\n    \"fasta\": \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\",\n    \"star_index\": \"/home/ubuntu/session2/materials/mm10_reference/STAR\",\n    \"outdir\": \"Lesson-2\",\n    \"salmon_quant_libtype\": \"A\",\n    \"save_trimmed\": true,\n    \"save_unaligned\": true,\n    \"extra_salmon_quant_args\": \"--numBootstraps 10\"\n  }\n\nRun the nf-core schema command to validate the specified parameters:\n\nnf-core schema validate rnaseq workshop-params.json\nNote this only confirms your inputs match their specified type (i.e.¬†boolean, string, number).\n\n\n\n\n\n\n\n\n\nBonus exercise: nf-core schema build to add new parameter\n\n\n\n\n\nTODO create activity https://nf-co.re/tools/#add-new-parameters-to-the-pipeline-schema\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nA parameter file can be used to specify input parameters for any Nextflow workflow.\nSpecify parameter files in a workflow run command using the -params-file flag.\nParameter files can be written in YAML or JSON file formats.\nThe nf-core schema utilities can be used to aid parameter documentation and usage.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.3_configure.html",
    "href": "notebooks/1.3_configure.html",
    "title": "Configuring nf-core workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the structure of an nf-core workflow.\nLearn how to customize the execution of an nf-core workflow.\nCustomize a toy example of an nf-core workflow.\n\n\n\n\n1.3.1. Workflow structure\nnf-core workflows follow a set of best practices and standardized conventions. nf-core workflows start from a common template and follow the same structure. Although you won‚Äôt need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\n\nNextflow DSL2 workflows are built up of subworkflows and modules that are stored as separate .nf files.\n\nMost nf-core workflows consist of a single workflow file (there are a few exceptions). This is the main &lt;workflow&gt;.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.\nA subworkflow is a groups of modules that are used in combination with each other and have a common purpose. For example, the SAMTOOLS_STATS, SAMTOOLS_IDXSTATS, and SAMTOOLS_FLAGSTAT modules are all included in the BAM_STATS_SAMTOOLS subworkflow. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. Within a nf-core workflow, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core workflow, an nf-core subworkflow is developed by the community and are shared in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific that are not shared in the nf-core subworkflows repository.\nA module is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives, inputs, outputs, when statements, and a script block. Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility and best practices. Local modules are workflow specific that are not shared in the nf-core modules repository.\n\n\n1.3.2. Configuration\nEach nf-core workflow has its own configuration and parameter defaults. These default parameters are required for tests to pass as the workflow is being developed and when it is pushed to GitHub. While the workflow configuration defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements.\nWhen a workflow is launched, Nextflow will look for configuration files in several locations. As each configuration file can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority:\n\nParameters specified on the command line (--parameter)\nParameters that are provided using the -params-file option\nConfig file that are provided using the -c option\nThe config file named nextflow.config in the current directory\nThe config file named nextflow.config in the workflow project directory\nThe config file $HOME/.nextflow/config\nValues defined within the workflow script itself (e.g., main.nf)\n\n\n\n\n\n\n\nWarning\n\n\n\n‚ö†Ô∏è Workflow parameters must be passed via the command line (--&lt;parameter&gt;) or Nextflow -params-file option. Custom config files, including those provided by the -c option, can be used to provide any configuration except for parameters.\n\n\nNotably, while some of these files are already included in the nf-core workflow repository (e.g., the nextflow.config file in the nf-core workflow repository), others are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file, and -c). Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of it‚Äôs execution.\n\n\n1.3.3. Viewing parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will also have additional text to help you understand when and how a parameter should be used.\n\n\n\n\n\nParameters and their descriptions can also be viewed in the command line using the run command with the --help parameter:\nnextflow run nf-core/&lt;workflow&gt; --help\n\n\n\n\n\n\nChallenge\n\n\n\nView the parameters for the Sydney-Informatics-Hub/nf-core-demo workflow using the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nParameters for the Sydney-Informatics-Hub/nf-core-demo workflow can be printed using the run command and the --help option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --help\n\n\n\n\n\n1.3.4. Parameters in the command line\nAt the highest level, parameters can be customized at execution using the command line. Any parameter can be specified on the command line by prefixing the parameter name with a double dash (--):\nnextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhile Nextflow options are prefixed with a single dash (-) all workflow parameters are prefixed with a double dash (--).\n\n\nDepending on the parameter type you may be required to add additional information. For example, for a string parameter, you would add the string in quotes after the parameter flag:\nnextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt; \"string\"\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite animal using the multiqc_title parameter in the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. You can also add the -resume option to save time:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --multiqc_title \"koala\" -resume \nIn this example, you can check your parameter has been applied by listing the files created in your results folder (my_results):\nls my_results/multiqc/\n\n\n\n\n\n1.3.5. Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or other configuration profile.\nThere are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by default.\nModule-specific configuration options (both mandatory and optional).\n\ntest.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a small test dataset.\n\ntest_full.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a full-size test dataset.\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;workflow&gt; -profile test,singularity\nnf-core workflows are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the workflows with software installed by other methods (e.g., environment modules or manual installation), most users find that Docker and Singularity are most convenient and reproducible.\n\n\n\n\n\n\nTip\n\n\n\nIf you‚Äôre computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‚Äòout of the box‚Äô. The test data profile will pull test data directly from the nf-core/test-data GitHub repository and run it on your local system without you needing to set up your own test data. The test profile is an important control to check the workflow is working as expected. Some workflows have multiple test profiles for you to test.\n\n\n\n\n1.3.6. Shared configuration files\nAn includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core workflows will fetch these configuration profiles from the remote configs repository and make them available.\nFor shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help.\n\n\n\n\n\n\nTip\n\n\n\nPawsey already has a shared profile that can be used to run workflows.\n\n\n\n\n\n\n\n\nTip\n\n\n\nParameters can be included in an institutional config.\n\n\n\n\n1.3.7. Custom configuration files\nNextflow will also look for custom configuration files that are external to the workflow project directory. These files include:\n\nThe config file $HOME/.nextflow/config\nA config file named nextflow.config in the current directory\nCustom files specified using the command line\n\nA parameter file that is provided using the -params-file option\nA config file that are provided using the -c option\n\n\nYou can be clever with the what settings you configure in each of these files.\nParameter files\nParameter files are .json files that can contain an unlimited number of parameters:\n{\n   \"&lt;parameter1_name&gt;\": 1,\n   \"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n   \"&lt;parameter3_name&gt;\": true\n}\nYou can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option.\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -param-file &lt;path/to/params.json&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite food using the multiqc_title parameter in a parameters file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .json file that contains:\n{\n   \"multiqc_title\": \"cheese\"\n}\nInclude the custom .json file in your execution command with the -params-file option.\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -params-file my_custom_params.json\nCheck that it has been applied\nls my_results/multiqc/\n\n\n\nConfiguration files\nConfiguration files are .config files that can contain various workflow properties. Custom paths passed in the command-line using the -c option:\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example:\nalpha.x  = 1\nalpha.y  = 'string value..'\nIs equivalent to:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite color using the multiqc_title parameter in a custom .config file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .config file that contains:\nparams.multiqc_title = \"blue\"\nInclude the custom .config file in your execution command with the -c option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -c my_custom_config.config\nCheck to see if it has been applied:\nls my_results/multiqc/\n\nRemember: You can not use the params scope in custom configuration files. Your favorite color will not be shown in the name of the multiqc report.\n\n\n\n\nImportantly, the process scope allows you to configure workflow processes and is used extensively to define resources and additional arguments for modules.\nBy default, process resources are allocated in the conf/base.config file using the withLabel selector:\nprocess {\n    withLabel: BIG_MEM {\n        cpus = 16\n        memory = 64.GB\n    }\n}\nSimilarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file:\nprocess {\n    withName: MYPROCESS {\n        cpus = 4\n        memory = 8.GB\n    }\n}\nWhile some tool arguments are included as a part of a module. To make modules sharable across workflows, most tool arguments are defined in the conf/modules.conf file in the workflow code under the ext.args entry.\nFor example, if you were trying to overwrite arguments in the TRIMGALORE process in the nf-core/rnaseq workflow, you could use the process scope:\nprocess {\n    withName : \".*:TRIMGALORE\" {\n        ext.args   = { \"&lt;your custom parameter&gt;\" }\n\n    }\nHowever, as the TRIMGALORE process is used multiple times in this workflow, an extended execution path of the module is required to make it more specific:\nprocess {\n    withName: \"NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE\" {\n        ext.args = \"&lt;your custom parameter&gt;\"\n    }\n}\nThe extended execution path is built from the workflows, subworkflows, and modules used to execute the process.\nIn the example above, the nf-core TRIMGALORE module, was called by the FASTQ_FASTQC_UMITOOLS_TRIMGALORE subworkflow, which was called by the RNASEQ workflow, which was called by the NFCORE_RNASEQ workflow in the main.nf file.\n\n\n\n\n\n\nTip\n\n\n\nIt can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the conf/modules.conf file.\n\n\n\n\nChallenge\nCreate a new .config file that uses the process and scope to overwrite the args for the MULTIQC process. Change the args to \"--title \\\"customargs\\\"\"\n:::\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -c my_custom_config.config -params-file my_custom_params.json --multiqc_title \"koala\"\nAs the command line is at the top of the hierarchy, the multiqc_title will be ‚Äúkoala‚Äù\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nDemonstrate the configuration hierarchy using the Sydney-Informatics-Hub/nf-core-demo workflow by modifying the multiqc_title the with a custom config file (-c), a params file (-params-file), and a command line flag (--multiqc_title). You can use the files you have already created:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -c my_custom_config.config -params-file my_custom_params.json --multiqc_title \"koala\"\nAs the command line is at the top of the hierarchy, the multiqc_title will be ‚Äúkoala‚Äù\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_wrapUp.html",
    "href": "notebooks/2.6_wrapUp.html",
    "title": "Workshop wrap up",
    "section": "",
    "text": "Key points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.0_template.html",
    "href": "notebooks/0.0_template.html",
    "title": "Lesson title",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nlearning outcome 1\nlearning outcome 2\n\n\n\nSome intro text to what this lesson is about\n\nSub-section heading\nCommands should be written like this:\ncommand \nWhere relevant include expected standard output:\nstdout here\nAny important notes for attendees should be present in information boxes. For example:\n\n\n\n\n\n\nCopying the code from the grey boxes on training materials\n\n\n\nIn this workshop we need to copy code from the grey boxes in the training materials and run it in the terminal. If you hover your mouse over a grey box on the website, a clipboard icon will appear on the right side. Click on the clipboard logo to copy the code. Test it out with:\nssh training@###.###.###.###\n\n\nChallenges/activites should be provided in challenge boxes:\n\n\n\n\n\n\nChallenge\n\n\n\nQuestion or activity\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution explanation and code where relevant\n\n\n\n\nFor other types of callout blocks see here. Any figures should be placed in figs directory and embeddeded like this: \n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html",
    "href": "notebooks/1.2_nfcore.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of nf-core.\nLearn how to use nf-core tooling.\nUse Nextflow to pull the nf-core/rnaseq workflow\n\n\n\n\n1.2.1. What is nf-core?\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and run them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276‚Äì278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won‚Äôt be left in the dark.\n\nCI Testing\n\nEvery time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to run.\n\nCloud-ready\n\nnf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.\n\n\nIt is important to remember all nf-core workflows are open-source and community driven. Most pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results*.\n\n\n1.2.2. Events\nnf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.\nUpcoming events are listed on the nf-core event page and announced on Slack and Twitter.\n\n\n1.2.3. Join the community!\nThere are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining the nf-core Slack can be especially useful for users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start.\nIf you are joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself!\n\n\n\n\n\n\nQuestions about Nextflow\n\n\n\nIf you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack. It‚Äôs worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before.\n\n\nThe nf-core YouTube channel is also especially helpful. There are lots of great workshops and ByteSize seminars for developers and users.\n\n\n1.2.4. nf-core tools\nThis workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using, developing, and testing workflows.\n\n\n\n\n\n\nHow to download nf-core tools\n\n\n\nnf-core tools is written in Python and is available from the Python Package Index (PyPI):\npip install nf-core\nAlternatively, nf-core tools can be installed from Bioconda:\nconda install -c bioconda nf-core\n\n\nThe nf-core --version option can be used to print your version of nf-core tools:\nnf-core --version\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what version of nf-core tools you have available using the nf-core --version option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core --version option to print your nf-core tools version:\nnf-core --version\n\n\n\n\nnf-core tools are for everyone and has commands to help both users and developers. For users, the tools make it easier to run workflows. For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line.\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what nf-core tools commands and options are available using the --help option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the --help option to list the options, tools for users, and tools for developers\nnf-core --help\n\n\n\n\nnf-core tools is updated with new features and fixes regularly so it‚Äôs best to keep your version of nf-core tools up-to-date.\n\n\n1.2.5. Running an nf-core workflow\nThere are currently 80 workflows (March 2023) available as part of nf-core. These workflows are at various stages of development with 49 released, 19 under development, and 12 archived.\nThe nf-core website has a full list of workflows, as well as their documentation, which can be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to run the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nUnless you are actively developing workflow code, you don‚Äôt need to clone the workflow code from GitHub and can use Nextflow‚Äôs built-in functionality to pull and a workflow. As shown in the introduction to Nextflow, the Nextflow pull command can download and cache workflows from GitHub repositories:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to run a previous version of a workflow once it is released using the -revision or -r flag.\n\n\n\n\n\n\nChallenge\n\n\n\nUse Nextflow to pull the latest version of the nf-core/rnaseq workflow directly from GitHub:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextlfow pull command to download the rnaseq workflow from the nf-core GitHub repository.\nnextflow pull nf-core/rnaseq\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nYou can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter‚Ä¶)\nnf-core has its own tooling that can be used by users and developers.\nNextflow can be used to pull nf-core workflows.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.5_extArgs.html",
    "href": "notebooks/2.5_extArgs.html",
    "title": "2.5. Passing external arguments to a process",
    "section": "",
    "text": "Objectives\n\n\n\n\nWrite a parameter file\nUnderstand the YAML file format\nRerun the workflow using a params file\nUnderstand the use of the params file for reproducible and transparent research\n\n\n\nIn Nextflow, parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Within the Nextflow code base, they are defined by the params{} scope. Parameters are also used in nf-core workflows to specify input and output files and define other aspects of workflow execution. Each nf-core pipeline comes with a default set of parameters that can be customised to suit specific requirements.\nIn the previous lesson we supplied these parameters as flags in our run command. Nextflow also allows us to pass all parameters to a pipeline‚Äôs run command using the -params-file flag and a JSON or YAML file. In this lesson we‚Äôre going to adjust our run command and rerun the pipeline using a parameter file, rather than specifying all parameters as separate flags in the run command.\n\n2.2.1. Revise the run command\nWhile our pipeline completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 14m 18s\nCPU hours   : 0.4\nSucceeded   : 170\n\n\n\n\n\n\nHandling dodgy error messages ü§¨\n\n\n\nThe first warning message isn‚Äôt very descriptive (see this pull request). You might come across small but inconsequential issues like this when running nf-core pipelines, too. End user feedback is always helpful, so consider submitting a GitHub issue or starting a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline‚Äôs developers.\n\n\nLet‚Äôs take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the Lesson-2.1/ directory:\nls -la Lesson-2.1/multiqc/star_salmon/\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS Code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe issue here is provided strandedness that we specified in our samplesheet.csv and inferred strandedness identified by Salmon do not match. Look‚Äôs like we‚Äôve incorrectly specified strandedness as forward in the samplesheet.csv when our raw reads actually show an equal distribution of sense and antisense reads (my mistake! üòë). Let‚Äôs check how the nf-core/rnaseq pipeline ran the Salmon quantification process so we can explore whether or not this will potentially have an impact on our results.\n\n\n2.2.2. Identify the run command for a process\nTo understand what command is being run for a process, you can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied, this may not be straight forward. To understand what Salmon is doing, we‚Äôre going to use the nextflow log command and some custom bash code to track down the hidden .command.sh scripts for each Salmon quant process to find out how Salmon quant identified library type.\nUse the Nextflow log command to reveal information about executed pipelines in our working directory:\nnextflow log\nThis will print a list of executed pipelines, by default:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                                                                                                                                                                                                                                                                                                                                                    \n2023-04-21 00:30:30     -               friendly_montalcini     -       f421ddc35d      685266bb-b99b-4945-9a54-981e8f4b1b07    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help                                                                                                                                                                                                                                                                                                 \n2023-04-21 00:40:58     14m 18s         mighty_swanson        OK      f421ddc35d      055e7b7f-c3ea-4fd9-a915-02343099939e    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --input samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory 6.GB --max_cpus 2 --outdir Lesson-1\nAll recent runs will be listed in this file, with the most recent at the top. Let‚Äôs query the logs for the Lesson-1 run. Run the command below after filling in your unique run name. For example:\nnextflow log mighty_swanson\nThat command will list out all the work subdirectories for all processes run. Recall that the actual tool commands issued by the nexflow processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these command scripts.\nBut how to find them? ü§î\nLet‚Äôs add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=mighty_swanson\nAnd let‚Äôs save the tool of interest (salmon) in another bash variable:\ntool=salmon\nNext, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2>/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‚Äòsalmon‚Äô. There are multiple salmon steps in the workflow, inlcuding index and an R script. We are looking for salmon quant which performs the read quantification. For example:\n/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh\n/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh\n/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh\n/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh\n/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh\n/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh\n/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh\n/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh\n/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh\n/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh\n/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh\n/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh\n/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh\nCompared with the salmon quant main.nf file, we get more information from the .command.sh process scripts:\ncat rnaseq/modules/nf-core/salmon/quant/main.nf\n\nLooking at the nf-core/rnaseq documentation, we can see:\n\nLibrary type is automatically inferred based on the $strandedness variable\nLibrary type can be adjusted using Salmon‚Äôs --libType= flag and the nf-core $strandedness variable.\n\nFollowing the recommendations in the Salmon documentation, we‚Äôre going to override this default with the nf-core/rnaseq pipeline‚Äôs --salmon_quant_libtype U parameter to indicate our data is a mix of both strands. If we want to get rid of the warning message Please check MultiQC report: 6/6 samples failed strandedness check, we‚Äôll have to change the strandedness fields in our samplesheet.csv. Keep in mind, this will cause the pipeline to run from the beginning, without usin.\n\n\n2.2.3. Write a parameter file\nNextflow accepts either YAML or JSON formats for parameter files. YAML and JSON are formats for storing data objects and structures in a file and either is a valid choice for building your parameters file. We will create and apply a YAML file with our inputs for our second run, just because its easier to read.\n\n\n\n\n\n\nChallenge\n\n\n\n\nAdd the appropriate fields from your previous run command to the params file below. You‚Äôll be sharing this file with a collaborator working on a different computational infrastructure but the same input and reference files.\n\ninput: \"\"\ngtf: \"\"\nfasta: \"\"\nstar_index: \"\"\nsalmon_index: \"\"\nsalmon_quant_libtype: \"U\"\nsave_trimmed: true\nsave_unaligned: true\nextra_salmon_quant_args: \"--numBootstraps 10\"\noutdir: \"Lesson-2.2\"\nüí° HINT: you will only need to specify nf-core/rnaseq parameters (i.e.¬†-- flags)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as workshop-params.yaml:\ninput: \"samplesheet.csv\"\ngtf: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\"\nfasta: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\"\nstar_index: \"/home/ubuntu/session2/materials/mm10_reference/STAR\"\nsalmon_index: \"/home/ubuntu/session2/materials/mm10_reference/salmon-index\"\nsalmon_quant_libtype: \"U\"\nsave_trimmed: true\nsave_unaligned: true\nextra_salmon_quant_args: \"--numBootstraps 10\"\n\n\n\nAny of the pipeline parameters can be added to the parameters file in this way.\n\n\n2.2.4. Pass the parameter file to the run command\nOnce your params file has been saved, run the following command. Observe:\n\nThe command is shorter thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\n\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-2.2 \\\n  -resume                        \nAs we‚Äôve used the -resume flag, the initial pre-processing stage and STAR alignments should to be restored from cache and only the Salmon and downstream QC steps will be recomputed. The rerun workflow should complete in a much shorter time and those warnings should have gone away!\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 8m 8s\nCPU hours   : 0.4 (57.2% cached)\nSucceeded   : 147\nCached      : 53\n\n\n2.2.5. Manage parameters with nf-core schema\nnf-core pipelines all have a nextflow_schema.json file in their root directory which explains all the parameters used by the workflow. You can use the nf-core schema tool to validate pipeline parameters, build a pipeline schema, deploy the documentation for a pipeline schema, or add new parameters to a pipeline schema (take a look at this nf-core bytesize talk! for a demonstration). Have a go at these bonus exercises, beware adjusting the nextflow_schema.json file will mean any subsequent workflow run will have to start from scratch again and prevent you applying the -resume function.\n\n\n\n\n\n\nBonus exercise: nf-core schema validate to check params file\n\n\n\n\n\nThe schema validate utility can be used to validate your specified parameters against the provided pipeline schema. Unfortunately, this won‚Äôt work for your workshop-params.yaml, so we first need to convert this to a JSON file format. To do this, run the following from the session2 directory:\n\nConvert the YAML file to JSON format and save this file as workshop-params.json\n\n{\n  \"input\": \"samplesheet.csv\",\n  \"gtf\": \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\",\n  \"fasta\": \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\",\n  \"star_index\": \"/home/ubuntu/session2/materials/mm10_reference/STAR\",\n  \"salmon_index\": \"/home/ubuntu/session2/materials/mm10_reference/salmon-index\",\n  \"salmon_quant_libtype\": \"U\",\n  \"save_trimmed\": true,\n  \"save_unaligned\": true,\n  \"extra_salmon_quant_args\": \"--numBootstraps 10\"\n}\n\nRun the nf-core schema command to validate the specified parameters:\n\nnf-core schema validate rnaseq workshop-params.json\nNote this only confirms your inputs match their specified type (i.e.¬†boolean, string, number). It cannot confirm that a file exists, or is formatted correctly, or if a string is the correct input for a parameter.\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nA parameter file can be used to specify input parameters for any Nextflow workflow.\nSpecify parameter files in a workflow run command using the -params-file flag.\nParameter files can be written in YAML or JSON file formats.\nThe nf-core schema utilities can be used to aid parameter documentation and usage.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "2.0. Introduction to session 2",
    "section": "",
    "text": "This session builds on fundamental concepts learned in Session 1 and provide you with hands-on experience in nf-core workflow customisation. Throughout the session we will be working with a case study to apply an nf-core workflow. Each lesson will build on the previous one, so you can gain a deeper understanding of the customisation techniques and the impact they have on the workflow and your results.\nWe will explore the source code of the nf-core/rnaseq workflow and apply customisations using a parameter file and custom configuration files. You will be writing these files using Nextflow and YAML. You will also be running Nextflow commands and writing some custom Bash code to efficiently extract information from the source code ü§ì.\n\n\n\n\n\n\nApplying what you learn here to other nf-core workflows\n\n\n\nWhile all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to other nf-core workflows and do not require an understanding of RNAseq data processing.\n\n\n\n2.0.1. How to approach open source software communities\nAs with all open source bioinformatics resources, nf-core workflows may not suit all applications. It is always important that you understand the needs of your dataset and research questions before deciding on a workflow. nf-core is a community effort powered by life scientists and software developers, who volunteer their time to build, maintain, and support workflows. All nf-core workflows are provided with ‚Äòsensible‚Äô default settings that have broad applicability. What is ‚Äòsensible‚Äô varies dramatically between different experiments, users, computing environments, and datasets, so these settings might not suit your needs.\nWe recommend approaching the application and customisation of nf-core workflows to your research with the following framework:\nDIAGRAM OF NF-CORE COMMUNITY FEATURE - IMPLICATION FOR WORKFLOWS - Q TO ASK - OUR RECOMMENDATIONS\n\n\n2.0.2. Log in to your instance\n\nOption 1: In Visual Studio Code\nFollow the VSCode set up instructions or use the quick start instructions below to connect to your instance using the command palatte:\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\nOption 2: In a terminal\nFollow the terminal set up instructions or use the quick start instructions below to connect to your instance on the command-line:\n\nRun: ssh training@###.###.###.###\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. These may not always suit your needs.\nTo decide whether an nf-core workflow is the right choice for your experiment you need to understand the needs of your dataset and research questions.\nUse the workflow documentation to understand the requirements for running a workflow.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_launch.html",
    "href": "notebooks/2.6_launch.html",
    "title": "Using nf-core launch",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUse the nf-core launch web GUI to adjust parameters\nUnderstand different options to using launch and when each would be applicable\nRun a workflow in an offline mode\n\n\n\n\nConstruct an execution command\nOpen the nf-core launch website and select the rnaseq pipeline from the select a pipeline menu and select pipeline release 3.11.1. Then select üöÄ Launch.\n\nOn the far right, unhide Show hidden params and then fill out the Nextflow command-line flags section:\n\nLeave -name blank. Doing this will ensure a random name will be applied, like it has been for our CLI runs so far\nFor -profile, enter singularity,c2r8\nLeave -work-dir as default\nToggle -resume to true\n\n\nNext, fill out the input section:\n\nCopy and paste the full path to the samplesheet.csv\nFor -outdir, specify Exercise6\nEnter your email address\nLeave the other sections blank\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nNow scroll through the remaining options, and fill in all of the parameters we have applied via our params file (including the multiqc_yaml file!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nOnce you are satisified that the params are all accounted for (remember that the extra_args.config should have the quality decreased from 40 to 30), click Launch workflow\n\n\nLaunch the workflow without internet access\nBecause we are on a VM, which has internet connection, we could use the first option ‚ÄòIf your system has an internet connection‚Äô. However, as this was covered in Part 1, we will practice with the ‚Äòno internet connection‚Äô option, which is usually the case if you are running on a HPC. Follow the instructions for this method:\n\nCopy the JSON params to a file in your working directory and save it as nf-params.json\n\n{\n    \"input\": \"\\/home\\/ubuntu\\/materials\\/samplesheet.csv\",\n    \"outdir\": \"Exercise6\",\n    \"email\": \"georgina.samaha@sydney.edu.au\",\n    \"fasta\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.fa\",\n    \"gtf\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.gtf\",\n    \"star_index\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/STAR\",\n    \"save_trimmed\": true,\n    \"salmon_quant_libtype\": \"A\",\n    \"extra_salmon_quant_args\": \"'--numBootstraps 10'\",\n    \"save_unaligned\": true,\n    \"config_profile_name\": \"pawsey_nimbus.config,extra_args.config\",\n    \"max_cpus\": 2,\n    \"max_memory\": \"6.GB\",\n    \"multiqc_config\": \"\\/home\\/ubuntu\\/run_then_launch\\/multiqc_config.yaml\",\n    \"show_hidden_params\": true\n}\n\nCopy the nextflow run command and execute it in your VM\n\nnextflow run nf-core/rnaseq -r 3.11.1 -profile singularity,c2r8 -resume -params-file nf-params.json\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_wrapUp.html",
    "href": "notebooks/2.6_wrapUp.html",
    "title": "Workshop wrap up",
    "section": "",
    "text": "Key points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "2.1. Design a run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore pipeline deployment and set up\n\n\n\nIn this lesson, we will design our pipeline run command by selecting parameters suitable for our experiment and our computational environment. We will observe how a pipeline is deployed when we execute the run command.\n\n2.1.1. Download the pipeline code\nStart by creating a new directory for all of today‚Äôs activities and move into it:\nmkdir ~/session2 && cd $_\nWe recommend that you keep a local copy of a pipeline‚Äôs code for the sake of reproducibility and record keeping. There are a number of ways to download a nf-core pipeline to your machine. We recommend using either the nf-core tools utility to download a pipeline for offline use or git.\nTake a look at the download options available to us:\nnf-core download --help\n\nRun the following command:\nnf-core download rnaseq \\\n  --revision 3.11.1 \\\n  --outdir ~/session2/nf-core-rnaseq-3.11.1 \\\n  --container singularity \\\n  --compress none \\\n  --singularity-cache-only\nYou will be prompted to define a Singularity cache directory to store container images ($NXF_SINGULARITY_CACHEDIR). Specify the path we used in Session 1:\nexport NXF_SINGULARITY_CACHEDIR=/home/training/singularity_cache\n\n\n\n\n\n\nNextflow has environmental variables!\n\n\n\nEnvironmental variables are used to store information that can be accessed by different programs and scripts running on the system. Nextflow has a number of environmental variables that can be used to override default Nextflow and pipeline behaviours. You can set them on the command line or by using env scopes when writing Nextflow.\n\n\nThe nf-core tools utility will download the pipeline files and centralised configs but not the containers, given they already exist in our specified cache.\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nIn situations where you might not wish to use the nf-core tools utility, download the nf-core/rnaseq source code from it‚Äôs GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\n\n‚õî BEWARE ‚õî this method will download a copy of the pipeline with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust some paths specified in the upcoming lessons accordingly.\n\n\n\n\nInside your nf-core-rnaseq-3.11.1 workflow directory, you should see 2 subdirectories:\nls -l nf-core-rnaseq-3.11.1\ntotal 8\ndrwxrwxr-x  7 ubuntu ubuntu 4096 Apr 21 02:08 configs\ndrwxrwxr-x 12 ubuntu ubuntu 4096 Apr 21 02:08 workflow\nRemember, as we ran the download command, a number of INFO logs were printed to the screen. The workflow files from GitHub were downloaded to the workflow directory and the centralised configs from GitHub were downloaded to the configs directory:\n\nFor this session we will be working with the workflow directory:\nls -l nf-core-rnaseq-3.11.1/workflow\ntotal 216\n-rwxrwxr-x 1 ubuntu ubuntu 58889 Apr 21 02:08 CHANGELOG.md\n-rwxrwxr-x 1 ubuntu ubuntu  9681 Apr 21 02:08 CITATIONS.md\n-rwxrwxr-x 1 ubuntu ubuntu  9078 Apr 21 02:08 CODE_OF_CONDUCT.md\n-rwxrwxr-x 1 ubuntu ubuntu  1096 Apr 21 02:08 LICENSE\n-rwxrwxr-x 1 ubuntu ubuntu 10002 Apr 21 02:08 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr 21 02:08 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr 21 02:08 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr 21 02:08 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr 21 02:08 modules\n-rwxrwxr-x 1 ubuntu ubuntu 13970 Apr 21 02:08 modules.json\n-rwxrwxr-x 1 ubuntu ubuntu 10847 Apr 21 02:08 nextflow.config\n-rwxrwxr-x 1 ubuntu ubuntu 42576 Apr 21 02:08 nextflow_schema.json\n-rwxrwxr-x 1 ubuntu ubuntu   359 Apr 21 02:08 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr 21 02:08 subworkflows\n-rwxrwxr-x 1 ubuntu ubuntu  1684 Apr 21 02:08 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr 21 02:08 workflows\nWe explored the standard nf-core workflow directory structure in Session 1. The most important files and directories for this session are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq pipeline\n\n\n\n\n\n2.1.2. Build your run command\nAll nf-core pipelines are provided with sensible default settings that have broad applicability and comprehensive documentation that explains pipeline usage and available parameters. Before we can decide which parameters to apply, we need to identify the default settings of the pipeline and where we can customise for our own needs, if desired.\nPipeline structure and possible customisation options will vary between nf-core pipelines. Generally, pipelines can be customised at a few different levels:\n\nThe workflow structure: selecting which path to follow (i.e.¬†the vaious method options below)\nThe process: choosing one tool over another to perform a process (i.e.¬†TrimGalore! vs FastP below)\n\nTool settings: applying specific thresholds or optional flags for a tool\n\nCompute resources: specifying CPU and memory thresholds or execution and software methods based on the infrastructure you‚Äôre working on.\n\n\nLooking at the nf-core/rnaseq pipeline structure above, we can see that the developers have organised the workflow into 5 stages and given users 1 location where we can make workflow-level customisations and 2 locations where we can make process-level customisations. The default nf-core/rnaseq method uses STAR to perform read alignment and Salmon to perform quantification. To understand tool and resource customisations, we‚Äôll need to take a look at the parameter documentation. We will explore these in depth in the following lessons.\nFor now, take a look at all the options available for the pipeline by running:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help \nNotice the top of the print out, there is a ‚Äòtypical‚Äô run command provided. In the diagram below, we can compare the typical run command with the run command we will be executing. Our command applies the default method of STAR and Salmons for alignment and quantification but we‚Äôve also diverged from the typical command where we have:\n\nProvided our own reference files via file specific parameters instead of using the iGenomes repository\nAdded some optional flags to customise some processes\nAdded some optional resource specifications suitable for our environment.\n\n\nUnderneath the typical run command suggestion, there are lots parameters, grouped under headings.\n\n\n\n\n\n\nOverwhelmed by all the parameters?\n\n\n\nFor larger nf-core pipelines it can be challenging to pick which parameters are applied at which of the levels mentioned above. nf-core pipeline parameters are grouped under specific headings that identify which level of customisation, stage of the pipeline, or tool they apply to.\n\n\nNotice at the bottom of the print out, there is:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nThree additional parameter sections are hidden from view. This is because they are less commonly used. To view all the pipeline run options on the command line, run:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help --show_hidden_params\n\n\n\n\n\n\nHyphens matter!\n\n\n\nHyphens matter when it comes to parameter flags in nf-core pipelines! Nextflow command-line parameters use one (-), whereas pipeline-specific parameters use two (--). For example: -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\nAll the data (including fastqs, reference fasta, gtf, Salmon and STAR indexes) are already available on an external file system called CernVM-FS that we can access from our Nimbus instances. Take a look at the files:\nls -l /cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/\n\n#Workaround for testing (data not yet on cvmfs)\nwget -O nfcore_materials.tar.gz https://cloudstor.aarnet.edu.au/plus/s/gIBdDhKEwfq2j58/download\ntar -zxvf nfcore_materials.tar.gz\nls -l ~/session2/materials\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Feb 14 05:36 fastqs\ndrwxrwxr-x 3 ubuntu ubuntu 4096 Feb 14 05:46 mm10_reference\n-rw-rw-r-- 1 ubuntu ubuntu  641 Feb 16 05:57 samplesheet.csv\nA sample sheet is a standard input requirement for nf-core pipelines. This sheet specifies:\n\nThe sample name\nRelevant input files for each sample\nOther metadata for each sample\n\nCopy and save the text below as samplesheet.csv:\nsample,fastq_1,fastq_2,strandedness\nSRR3473989,/home/ubuntu/session2/materials/fastqs/SRR3473989_selected.fastq.gz,,forward\nSRR3473988,/home/ubuntu/session2/materials/fastqs/SRR3473988_selected.fastq.gz,,forward\nSRR3473987,/home/ubuntu/session2/materials/fastqs/SRR3473987_selected.fastq.gz,,forward\nSRR3473985,/home/ubuntu/session2/materials/fastqs/SRR3473985_selected.fastq.gz,,forward\nSRR3473986,/home/ubuntu/session2/materials/fastqs/SRR3473986_selected.fastq.gz,,forward\nSRR3473984,/home/ubuntu/session2/materials/fastqs/SRR3473984_selected.fastq.gz,,forward\nThis file has a header and one row for each sample, as directed by the nf-core samplesheet specifications. Note we only have one single-end read file for each sample, as such we have left the 3rd column empty.\n\n\n2.1.3. Run the pipeline\nWe need to store the path to our input and reference data in a variable for our run command:\nmaterials=/home/ubuntu/session2/materials #for cloudstor\nmaterials=/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523 #for cvmfs\nNow run the pipeline, with the necessary flags and the samplesheet.csv file you just created:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n    --input samplesheet.csv \\\n    -profile singularity \\\n    --skip_markduplicates \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --salmon_index $materials/mm10_reference/salmon-index \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --extra_salmon_quant_args '--numBootstraps 10' \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    --outdir Lesson-2.1\n\n\n\n\n\n\nü§∑ Zoom check-in! ü§∑\n\n\n\nIs everyone ok?\nYes, move on üëè :clap:\nNo, help! üò¢ :cry:\n\n\nTake a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\nAs the workflow starts, you will also see a number of processes spawn out underneath this. Recall from session 1 that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. Take a look at the image below which explains a workflow‚Äôs process status output provided by Nextflow using the data dependencies for the STAR_ALIGN process as an example.\n\n\n\n2.1.4. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 14m 18s\nCPU hours   : 0.4\nSucceeded   : 170\nThe pipeline ran successfully, however note the warning about all samples having failed the strandedness check. We‚Äôll explore that in the next lesson. In the meantime, list (ls -la) the contents of your directory, you‚Äôll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:44 .\ndrwxr-x---  15 ubuntu ubuntu 4.0K Apr 21 01:56 ..\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 03:58 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 371K Apr 21 03:58 .nextflow.log\n-rw-rw-r--   1 ubuntu ubuntu  17K Apr 21 03:50 .nextflow.log.1\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:58 Lesson-2.1\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 02:08 nf-core-rnaseq-3.11.1\n-rw-rw-r--   1 ubuntu ubuntu  563 Apr 21 03:14 samplesheet.csv\ndrwxrwxr-x 143 ubuntu ubuntu 4.0K Apr 21 03:58 work\nNextflow has created 2 new output directories, work and Lesson-2.1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe Lesson-2.1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible defaults. You can adjust some settings as required by applying flags to your run command.\nnf-core pipelines are all built from a template that means they have a standard structure to their code bases\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows with nf-core",
    "section": "",
    "text": "This course is currently under development\nThis workshop will set you up with the foundational knowledge required to run and customise nf-core workflows in a reproducible manner. Using the nf-core/rnaseq workflow as an example, we will step through essential features common across all nf-core workflows. We will explore ways to adjust the workflow parameters based on the needs of your dataset and configuration the workflow to run on your computational environment.\n\nTrainers\n\nGeorgie Samaha, Sydney Informatics Hub, University of Sydney\nCali Willet, Sydney Informatics Hub, University of Sydney\nChris Hakkaart, Seqera Labs\n\n\n\nTarget audience\nThis workshop is suitable for people who are familiar with working at the command line interface and have some experience running Nextflow and nf-core workflows.\n\n\nPrerequisites\n\nExperience navigating the Unix command line\nFamiliarity with Nextflow and nf-core workflows\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nCode of Conduct\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up your computer\nFollow these instructions to install VSCode and login to your Nimbus instance.\n\n\nDay 1: Introduction to nf-core\n\n\n\nDay 2: Customising nf-core\nWrite, run, adjust, and re-run an nf-core workflow as we step through various customisation scenarios.\n\n\n\n\n\nCourse survey\nPlease fill out our course survey before you leave. Help us help you! üòÅ\n\n\nCredits and acknowledgements\nThis workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera Labs, Pawsey Supercomputing Research Centre, and Australia‚Äôs National Research Education Network (AARNet) enabled through the Australian BioCommons (NCRIS via Bioplatforms Australia). This workshop was developed as a part of the Australian BioCommons Bring Your Own Data Platforms project.\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  }
]