[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows with nf-core",
    "section": "",
    "text": "This course is currently under development\nThis workshop will set you up with the foundational knowledge required to run and customise nf-core workflows in a reproducible manner. Using the nf-core/rnaseq workflow as an example, we will step through essential features common across all nf-core workflows. We will explore ways to adjust the workflow parameters based on the needs of your dataset and configuration the workflow to run on your computational environment.\n\nTrainers\n\nCali Willet, Sydney Informatics Hub\nChris Hakkaart, Seqera Labs\nGeorgie Samaha, Sydney Informatics Hub\n\n\n\nTarget audience\nThis workshop is suitable for people who are familiar with working at the command line interface and have some experience running Nextflow and nf-core workflows.\n\n\nPrerequisites\n\nExperience navigating the Unix command line\nFamiliarity with Nextflow and nf-core workflows\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nCode of Conduct\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up your computer\nFollow these instructions to install VSCode and login to your Nimbus instance.\n\n\nDay 1: Introduction to nf-core\n\n\n\nDay 2: Customising nf-core\n\n\n\n\n\n\nCourse survey\nPlease fill out our course survey before you leave. Help us help you! üòÅ\n\n\nCredits and acknowledgements\nThis workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera Labs, Pawsey Supercomputing Research Centre, and Australia‚Äôs National Research Education Network (AARNet) enabled through the Australian BioCommons (NCRIS via Bioplatforms Australia).\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Some tips and tricks",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "Questions\n\nHow do I install a terminal or IDE application on my computer?\nHow do I log in to a Nimbus instance?\n\n\nIn this workshop series, we will be using Pawsey‚Äôs Nimbus cloud. The Pawsey Supercomputing Research Centre is one of two, Tier-1, High Performance Computing facilities in Australia. Their Nimbus cloud platform is an accessible and flexible solution for bioinformatics applications that may not be suitable for large-scale HPC machines including:\n\nDeveloping and refining scalable workflows in prepration for HPC allocation applications.\nWorkflows with long runtimes that excede wall time queue limits on HPC facilities.\nComplex data-bound workflows with variable compute resource profiles that are common in bioinformatics pipelines.\n\nThe main requirements for this workshop are a personal computer with:\n\nA web broswer\nTerminal or IDE application\n\nOn this page you will find instructions on how to set up a terminal application and web browser on your computer and how to connect to Nimbus. Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop.\nTo connect to your Nimbus instance, you will need either a terminal or integrated development environment (IDE) application installed on your computer. While we recommend you use the Visual Studio Code IDE for this workshop, we have also provided directions for installing and using a terminal applications below.\n\nInstall and set up Visual Studio Code\nVisual Studio Code is a lightweight and powerful source code editor available for Windows, macOS and Linux computers.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nLinux\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\n\nInstall the Live Server extension. Click on the blue install button.\n\n\n\nLogin via Visual Studio Code\n\nConnect to your instance with VS code by adding the host details to your ssh config file.\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Open SSH configuraiton file\nAdd new entry, filling out host name and identity file:\n\nHost nfcoreWorkshop\n  HostName 146.118.XX.XXX  \n  User training     \nConnect to this address\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\n\n\n\n\nInstall and set up a terminal application\nThe terminal applications available to you will depend on your operating system.\n\nLinux terminals\nIf you use Linux, chances are you already know your shell and how to use it. Basically, just open your preferred terminal program and off you go!\n\n\nOS X (Mac)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‚Äòterminal‚Äô. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\n\n\nWindows\nWe recommend MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‚ÄòHome Edition‚Äô select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‚Äòstart local terminal‚Äô (and install Cygwin if prompted)\n\n\n\n\nLogin via Terminal\nTo log in to Nimbus, we will use a Secure Shell (SSH) connection. To connect, you need 3 things: 1. The assigned IP address of your instance (i.e.¬†###.###.##.###). Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop. 2. Your login name. In our case, this will be training for all participants. 3. Your password. All participants will be provided with a password at the beginning of the workshop.\nTo log in, type the following into your terminal, using your login name and the instance‚Äôs IP address:\nssh training@###.###.###.###\nYou will receive a message saying:\nThe authenticity of host 'XXX.XXX.XX.XXX (XXX.XXX.XX.XXX)' can't be established.\nRemember your host address will be different than the one above. There will then be a message saying:\nAre you sure you want to continue connecting (yes/no)?\nIf you would like to skip this message next time you log in, answer ‚Äòyes‚Äô. It will then give a warning:\nWarning: Permanently added 'XXX.XXX.XX.XXX' (ECDSA) to the list of known hosts.\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\n\n\n\n\n\n\nPay Attention\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You‚Äôll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.4_multiConfig.html",
    "href": "notebooks/2.4_multiConfig.html",
    "title": "Using multiple configuration files at once",
    "section": "",
    "text": "Objectives\n\nUnderstand the heirarchy of configuration files specified by Nextflow\nWrite a custom configuration file for MultiQC in the YAML file format\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.3_configEnv.html",
    "href": "notebooks/2.3_configEnv.html",
    "title": "Configuring a run for your environment",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\nNextflow‚Äôs portability is enabled by its ability to separate workflow implementation from the configuration settings required to execute it. In the previous lesson we used a parameter file to define some workflow implementation settings, in this lesson we will be using configuration files to define the specifications required for task execution on our compute environment.\nWhile nf-core workflows are designed to be portable and work out of the box, sometimes it will be necessary for you to customise the workflow‚Äôs configuration so that it can run on your computational environment. Bioinformatics workflows, like those provided by nf-core, comprise multiple steps that have varying computational resource needs. These workflows can also be long running, especially when you‚Äôre working at scale with multiple samples and organisms with complex and large genomes. To ensure you can successfully run your workflow, it is important to understand the computational resources it will be consuming and configure your workflows computational resource useage accordingly.\n\n\n\n\n\n\nWhat are compute resources?\n\n\n\n\n\nCompute resources refer to the Central Processing Unit (CPU), Random Access Memory (RAM), and disk requirements of the workflow, which can vary depending on the size and complexity of our input data, and the specific work being performed. These are critical resources that determine the efficiency and scalability of our workflows. Consider the following analogy for how CPUs, RAM, and disk space work together to process data.\n\nImagine Nigella Lawson preparing a Christmas dinner in her kitchen. Like Nigella in the kitchen, the CPU is the main processing unit in a computer that performs the various tasks in your compute environment.\nLike Nigella‚Äôs countertop that acts as her working space in the kitchen, RAM is the temporary working space used to store and manipulate data during computation. The amount of RAM required by a task depends on the size of the input data, as well as the complexity of the task being performed. Insufficient RAM can cause the workflow to fail or significantly slow down due to excessive disk I/O.\nLike Nigella‚Äôs fridge and pantry that hold all her ingredients, disk space is required to store all the input/output data and intermediate files generated by a task. In bioinformatics, well organised and large amounts of disk space is required because biological data sets can be large, complex, and varied in format.\n\n\n\n\nWhen to use a config file\nWhile nf-core workflows\n\n\nHow to use a config file\n\n\nWrite a custom config\n\n\n\n\n\n\nWhy should I be concerned with computational efficiency? üåè\n\n\n\nBioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices.\nBy using cloud computing services and high-performance computing clusters (like those provided by Pawsey), we can take advantage of energy-efficient infrastructure. We can also use tools like Nextflow that provide features like intelligent task scheduling, automatic job parallelisation, and task-level resource allocation. If you‚Äôd like to learn more about the carbon footprint of some bioinformatics workflows and tools, this paper makes for an interesting read!\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.0_intro.html",
    "href": "notebooks/1.0_intro.html",
    "title": "Welcome to session 1",
    "section": "",
    "text": "In Session 1 we will introduce tools, formats, concepts, and ideas that will be utilized in Session 2.\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_troubleshoot.html",
    "href": "notebooks/2.6_troubleshoot.html",
    "title": "Troubleshooting issues and errors",
    "section": "",
    "text": "Objectives\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "Designing your run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore workflow deployment and set up\n\n\n\n\nDownload the workflow code\nIt can be very easy to lose track while working on the command line, especially when we‚Äôre working with large datasets and complex commands as we do with bioinformatics workflows. To make sure we work reproducibly, we will be organising our workspace and using a local copy of the nf-core/rnaseq workflow for all exercises.\nStart by creating a new directory for all of today‚Äôs activities and move into it:\nmkdir ~/nfcore-workshop/session2 && cd $_\nThere are a number of ways to download a nf-core workflow to your machine. We recommend using git or the nf-core tools utility. Today, we will download most recent version of the workflow from it‚Äôs GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\nCheck the workflow has been downloaded:\nls -l rnaseq\nInside your nf-core-rnaseq workflow directory, you should see a number of files and subdirectories:\ntotal 216\n-rw-rw-r-- 1 ubuntu ubuntu 58889 Apr  4 03:40 CHANGELOG.md\n-rw-rw-r-- 1 ubuntu ubuntu  9681 Apr  4 03:40 CITATIONS.md\n-rw-rw-r-- 1 ubuntu ubuntu  9078 Apr  4 03:40 CODE_OF_CONDUCT.md\n-rw-rw-r-- 1 ubuntu ubuntu  1096 Apr  4 03:40 LICENSE\n-rw-rw-r-- 1 ubuntu ubuntu 10002 Apr  4 03:40 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr  4 03:40 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 modules\n-rw-rw-r-- 1 ubuntu ubuntu 13970 Apr  4 03:40 modules.json\n-rw-rw-r-- 1 ubuntu ubuntu 10903 Apr  4 03:40 nextflow.config\n-rw-rw-r-- 1 ubuntu ubuntu 42576 Apr  4 03:40 nextflow_schema.json\n-rw-rw-r-- 1 ubuntu ubuntu   359 Apr  4 03:40 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 subworkflows\n-rw-rw-r-- 1 ubuntu ubuntu  1684 Apr  4 03:40 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 workflows\nThis may look very different to other Nextflow workflows you may have run or written before. The most important files and directories for us to understand, are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq workflow\n\n\n\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nUsing the nf-core tools utility, search for the rnaseq pipeline:\nnf-core list rnaseq\nThen, download the correct pipeline:\nnf-core download nf-core/rnaseq\nYou will be prompted to select a version. Use your arrow keys to specify 3.10.1 and hit enter.\n\n‚õî BEWARE ‚õî this method will download a copy of the workflow with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust commands in the upcoming lessons accordingly.\n\n\n\n\n\n\nBuild your run command\nAll nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. What is ‚Äòsensible‚Äô varies dramatically between different experiments, computing environments, and datasets, so these settings might not suit your needs. Having asked ourselves those questions earlier, consider the most important design choices we have made for this workflow and structuring our run command:\n\nWe don‚Äôt need to run the pseudo alignment step (Stage 3)\nWe have chosen to use STAR to align reads\nWe have chosen to use Salmon to estimate transcript abundance\nWe only have access to 2 CPUs and 8Gb of RAM today\nWe are working with our own subset data today (including reference data)\n\n\nFor the sake of expediency, we are using prepared subset data for this session. All the data (including fastqs, input manifest, reference fasta, gtf, and STAR indexes) are available on an external file system called CernVM-FS. CernVM-FS is a read-only file system that Pawsey have used to store files such as containerised tools (Biocontainers), reference datasets, and other shared resources that are commonly used by many researchers. Take a look here for more information on bioinformatics resources provided by Pawsey on Nimbus.\nTake a quick look at the workshop data we‚Äôre working with today:\nls /path/to/aarnet-cvmfs/training/workshopMaterials\nWe need to store this path in a variable for our run command:\nmaterials=/path/to/aarnet-cvmfs/training/workshopMaterials\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the nf-core/rnaseq documentation and the important considerations above, can you decide which flags you may need to add to this command for this experiment?\nnextflow run rnaseq/main.nf \\\n  --input &lt;samples.tsv&gt; \\\n  -profile singularity \\\n  -with-report execution_report_exercise2_1.html \\\n  -with-trace execution_trace_exercise2_1.txt \\\n  -with-timeline timeline_exercise2_1.html \\\n  -with-dag dag_exercise2_1.png\nüí° You will need to look at the reference genome, alignment, and max job request sections.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven we are using STAR and Salmon as our aligner and quantification tool of choice (respectively) and it is the default choice of this workflow we will not need to provide an --aligner flag. However, if you wanted to provide this for the sake of reproducibility in case things change in the future:\n--aligner 'star_salmon'\nGiven we are providing our own subset data for this workshop, we will need to use:\n--fasta /path/to/mouse.fa  \n--gtf /path/to/mouse.gtf \n--star_index /path/to/STAR\nGiven we have limited computing resources today, we will need to specify a ceiling for both memory and CPUs:\n--max_memory '6.GB' \n--max_cpus 2 \n\n\n\n\n\nRun the workflow\nNow run the workflow:\nnextflow run rnaseq/main.nf \\\n  --input $materials/samples.tsv \\\n  -profile singularity \\\n  --fasta $materials/mm10_chr18.fa \\\n  --gtf $materials/mm10_chr18.gtf \\\n  --star_index $materials/STAR \\\n  --max_memory '6 GB' --max_cpus 2 \\\n  --outdir ex1_results \\\n  -with-report execution_report_exercise2_1.html \\\n  -with-trace execution_trace_exercise2_1.txt \\\n  -with-timeline timeline_exercise2_1.html \\\n  -with-dag dag_exercise2_1.png\n\n\n\n\n\n\nZoom reaction check in!\n\n\n\nIs everyone ok? Is your workflow running?\nüëè (clap) yes, let‚Äôs move on.\nüò¢ (cry) no, please help.\n\n\n\n\nExplore the workflow code\nWhile the workflow runs (~15 mins), let‚Äôs take a closer look at the rnaseq/workflows/rnaseq.nf file to see where our flags are influencing the workflow structure. Open a new terminal window so we don‚Äôt disturb our running workflow, navigate back to ~/nfcore-workshop/session2, then open the rnaseq/workflows/rnaseq.nf file. If you‚Äôre working on the command-line rather than VScode, run:\ncat rnaseq/workflows/rnaseq.nf\nThis file is huge and has a lot going on. TODO tie this in with Session 1 content.\nExtract the headers:\nsed -n '/\\/\\*/,/\\*\\//p' rnaseq/workflows/rnaseq.nf\nWe‚Äôve got 7 main section:\n\nValidate inputs\nConfig files\nImport local modules/subworkflows\nImport nf-core modules/subworkflows\nRun main workflow\nCompletion email and summary\nThe end\n\n\nValidate inputs\nHere, the developers have set some rules around defining valid input parameters and structuring the workflow to accommodate specified parameters before the workflow is run.\n\n\nConfig files\nThis is just some channels for creating the MultiQC reports, nothing important.\n\n\nImport local modules/subworkflows\n\n\nImport nf-core modules/subworkflows\n\n\nRun main workflow\n\n\nCompletion email and summary\n\n\nThe end\n\n\n\nKey points\n\nnf-core workflows are provided with sensible defaults. You can adjust some settings as required by applying flags to your run command.\nnf-core workflows are all built from a template that means they have a standard structure to their code bases\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html",
    "href": "notebooks/1.1_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nIt is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment, based on the dataflow programming model.\nNextflow‚Äôs core features are:\n\nWorkflow portability and reproducibility\nScalability of parallelization and deployment\nIntegration of existing tools, systems, and industry standards\n\nWhether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your workflow and improve your productivity.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#installing-nextflow",
    "href": "notebooks/1.1_nextflow.html#installing-nextflow",
    "title": "Introduction to Nextflow",
    "section": "Installing Nextflow",
    "text": "Installing Nextflow\nNextflow is distributed as a self-installing package and does not require any special installation procedure. If you do not already have Nextflow available, it can be installed using a few easy steps:\n\nDownload the executable package using either wget -qO- https://get.nextflow.io | bash or curl -s https://get.nextflow.io | bash\nMake the binary executable on your system by running chmod +x nextflow.\nMove the nextflow file to a directory accessible by your $PATH variable, e.g, mv nextflow ~/bin/"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#nextflow-options-and-commands",
    "href": "notebooks/1.1_nextflow.html#nextflow-options-and-commands",
    "title": "Introduction to Nextflow",
    "section": "Nextflow options and commands",
    "text": "Nextflow options and commands\nNextflow provides a robust command line interface for the management and execution of workflows. The top-level interface consists of options and commands.\nA list of options and commands can be viewed using the -h option:\nnextflow -h\n\nOptions for a specific command can also be viewed by appending the help option to a Nextflow command. For example, the run command:\nnextflow run -h\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind out which version of Nextflow you are using using the Nextflow version option.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe version of Nextflow you are using can be printed using the -v option:\nnextflow -v"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#managing-your-environment",
    "href": "notebooks/1.1_nextflow.html#managing-your-environment",
    "title": "Introduction to Nextflow",
    "section": "Managing your environment",
    "text": "Managing your environment\nYou can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a workflow and will be interpreted by Nextflow.\nFor most users, Nextflow will work without setting any environment variables. However, for consistency, it is good practice to pin the version of Nextflow you are using when running a workflow using the NXF_VER variable.\nexport NXF_VER=&lt;version number&gt;\nSimilarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR variables:\nexport NXF_CONDA_CACHEDIR=&lt;custom/path/to/conda/cache&gt;\nYou may want to include these in your .bashrc file (or alternate) that is loaded when you log in so you don‚Äôt need to export variables every session.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to pin the version of Nextflow to 22.04.5 using the NXF_VER environmental variable and check that it has been applied.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExport the version using the NXF_VER environmental variable:\nexport NXF_VER=22.04.5\nCheck that the new version has been applied using the -v option:\nnextflow -v\n\n\n\n\nA complete list of environmental variables can be found here."
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#executing-a-workflow",
    "href": "notebooks/1.1_nextflow.html#executing-a-workflow",
    "title": "Introduction to Nextflow",
    "section": "Executing a workflow",
    "text": "Executing a workflow\nNextflow seamlessly integrates with code repositories such as GitHub. This feature allows you to manage your project code and use public Nextflow workflows quickly, consistently, and transparently.\nThe Nextflow pull command will download a workflow from a hosting platform into your global cache $HOME/.nextflow/assets folder.\nIf you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL. The qualified name is formed by two parts - the owner name and the repository name separated by a / character. For example, if a Nextflow project foo is hosted in a GitHub repository bar at the address http://github.com/foo/bar, it could be pulled using:\nnextflow pull foo/bar\nOr by using the complete URL:\nnextflow pull http://github.com/foo/bar\nAlternatively, the Nextflow clone command can be used to download a workflow into a local directory of your choice:\nnextflow clone foo/bar &lt;your/path&gt;\nThe Nextflow run command is used to initiate the execution of a workflow :\nnextflow run foo/bar\nIt is worth noting that you run a workflow, it will look for a local file with the workflow name you‚Äôve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically pull the workflow to your global cache and execute it.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to run the hello workflow directly from nextflow-io GitHub.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the nextflow-io/hello workflow:\nnextflow run nextflow-io/hello\n\n\n\n\nA full list of Nextflow run options can be found here."
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#executing-a-revision",
    "href": "notebooks/1.1_nextflow.html#executing-a-revision",
    "title": "Introduction to Nextflow",
    "section": "Executing a revision",
    "text": "Executing a revision\nWhen a Nextflow workflow is created or updated, a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the workflow and to ensure that the same version of the workflow is used consistently across different runs.\nThe Nextflow info command can be used to view workflow properties, such as the project name, repository, local path, main script, and revisions. The * indicates which revision of the workflow you have stickied and will be executed when using the run command.\nnextflow info &lt;workflow&gt;\nTo use a specific revision, you simply need to add it to the command line with the --revision or -r flag. For example, to run a workflow with the v1.0 revision, you would use the following command:\nnextflow run &lt;workflow&gt; -r v1.0\nNextflow provides built-in support for version control using Git, which allows users to easily manage and track changes made to a workflow over time. A revision can be a git branch, tag or commit SHA number.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to run the hello workflow directly from nextflow-io GitHub using the v1.1 revision tag.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the nextflow-io/hello workflow with the revision -r option:\nnextflow run nextflow-io/hello -r v1.1\n\n\n\n\nIf your local version of a workflow is not the latest you be shown a warning and will be required to use a revision flag when executing the workflow. You can update a workflow with the Nextflow pull command."
  },
  {
    "objectID": "notebooks/1.1_nextflow.html#listing-and-dropping-workflows",
    "href": "notebooks/1.1_nextflow.html#listing-and-dropping-workflows",
    "title": "Introduction to Nextflow",
    "section": "Listing and dropping workflows",
    "text": "Listing and dropping workflows\nOver time you might want to remove a stored workflows. Nextflow also has functionality to help you to view and remove workflows that have been pulled locally.\nThe Nextflow list command prints the projects stored in your global cache $HOME/.nextflow/assets. These are the workflows that were pulled when you executed either of the Nextflow pull or run commands:\nnextflow list\n\nIf you want to remove a workflow from your cache you can remove it using the Nextflow drop command:\nnextflow drop &lt;workflow&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nSee which workflows you have stored and the remove them with the drop command.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nList your workflow assets:\nnextflow list\nDrop the nextflow-io/hello workflow:\nnextflow drop nextflow-io/hello\nCheck it has been removed:\nnextflow list\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nEnvironment variables can be used to control your Nextflow runtime and the underlying Java virtual machine.\nNextflow supports version control and has automatic integrations with online code repositories.\nYou can manage workflows with Nextflow commands (e.g., pull, clone, list, and drop)."
  },
  {
    "objectID": "notebooks/1.4_users.html",
    "href": "notebooks/1.4_users.html",
    "title": "nf-core for users",
    "section": "",
    "text": "nf-core tools has additional commands to help users execute workflows. Although you do not need to use these commands to execute the nf-core workflows, they can greatly assist and improve and simplify your experience.\nThere are also nf-core tools for developers - however these will not be covered as a part of this workshop. If you are curious to learn more about these tools you can find more information on tools page of the nf-core website.\n\nnf-core list\nThe nf-core list command can used to print a list of remote nf-core workflows along with your local information.\nnf-core list\n\nThe output shows the latest workflow version number and when it was released. You will also be shown if and when a workflow was pulled locally and whether you have the latest version.\nKeywords can be supplied to help filter the workflows based on matches in titles, descriptions, or topics:\nnf-core list dna\n\nOptions can also be used to sort the workflows by latest release (-s release, default), when you last pulled a workflow locally (-s pulled), alphabetically (-s name), or number by the number of GitHub stars (-s stars).\n\n\n\n\n\n\nChallenge\n\n\n\nTry to filter the list of nf-core workflows for those that are for rna and sort them by stars.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the list command, filter it for rna, and sort by stars:\nnf-core list rna -s stars\n\n\n\n\n\nnf-core launch\nNextflow workflows can have a considerable number of optional command line flags. To help manage these, you can use the nf-core launch command.\nThe command takes one argument - either the name of an nf-core workflow which will be pulled automatically or the path to a directory containing a Nextflow workflow:\nnf-core launch nf-core/&lt;workflow&gt;\nWhen running this command, you will first be asked about which version of a workflow you would like to run. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the workflow parameters for your run. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.\n\nThe launch tool uses the nextflow_schema.json file from a workflow to give parameter descriptions, defaults, and grouping. If no file for the workflow is found, one will be automatically generated at runtime.\nThe launch tool will save your parameter variables as a JSON file called nf-params.json and will suggest an execution command that includes the -params-file flag and your new nf-params.json file.\nThe wizard will ask if you want to launch the Nextflow run. You will also be given the run command and a copy of the JSON file for you to copy and paste if you wish.\nAny profiles or Nextflow options that are set using the wizard will also be included in your run command.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to run the nf-core/rnaseq workflow with the nf-core launch command. Use the latest version of the workflow with the the test and singularity profiles, and name your output directory results:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core launch command for the nf-core/rnaseq workflow and follow the prompts:\nnf-core launch nf-core/rnaseq\nYour final run command should look like this:\nnextflow run nf-core/rnaseq -r 3.11.1 -profile test,singularity -params-file nf-params.json\nYour nf-params.json file should look like this:\n{\n    \"outdir\": \"results\"\n}\n\n\n\n\n\nnf-core download\nSometimes you may need to run an nf-core workflow on a server or HPC system that has no internet connection. In this case, you will need to fetch the workflow files and manually transfer them to your system.\nTo make this process easier and ensure accurate retrieval of correctly versioned code and software containers, nf-core has the download helper tool.\nThe nf-core download command will download both the workflow code and the institutional nf-core/configs files. It can also optionally download singularity image file.\nnf-core download\nIf run without any arguments, the download tool will interactively prompt you for the required information. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:\n\nPipeline name\n\nName of workflow you would like to download\n\nPipeline revision\n\nSelect the revision you would like to download\n\nPull containers\n\nChoose if you would like to download Singularity images\nThis will only work if you have Singularity installed\n\nChoose compression type\n\nChoose compression type for Singularity images\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.2_params.html",
    "href": "notebooks/2.2_params.html",
    "title": "Using a parameter file",
    "section": "",
    "text": "Objectives\n\n\n\n\nWrite a parameter file\nUnderstand the YAML file format\nRerun the workflow using a params file\nUnderstand the use of the params file for reproducible and transparent research\n\n\n\nIn Nextflow, parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Parameters are used in nf-core workflows to specify input and output files and define other aspects of workflow execution. Each nf-core workflow comes with a default set of parameters that can be customised to suit specific requirements. In the previous lesson we supplied these parameters in our run command, on the command line. Specifying multiple parameters like this can be messy and hard to keep track of.\nNextflow allows us to pass all parameters to a workflow‚Äôs run command using the -params-file flag and a JSON or YAML file. Using a parameter file makes it easier to rerun and reproduce our code, we can also share these files with our collaborators and provide as supplementary file in a publication. In this lesson we‚Äôre going to adjust our run command and rerun the workflow using a parameter file, rather than specifying all parameters on the command line.\n\nRevising the run command\nWhile our workflow completed successfully, all fastq files failed the strandedness check:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\n\nCompleted at: 12-Apr-2023 04:13:09\nDuration    : 16m 18s\nCPU hours   : 0.4\nSucceeded   : 200\nLet‚Äôs take a look at the MultiQC report, as directed. You can find this report in the results/ directory:\nls -la Exercise1/multiqc/star_salmon\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe issue here is provided strandedness that we specified in our samplesheet.csv and inferred strandedness do not match. Look‚Äôs like we‚Äôve incorrectly specified strandedness as forward, when our reads show an equal distribution of sense and antisense reads. Given our previous experience with using Salmon to perform transcript quantification, we know it has an option to automatically detect the library type from the data, rather than infer it from the strandedness metadata we provided. Let‚Äôs check how the nf-core/rnaseq workflow ran the Salmon quantification process.\n\nHow can we know what tool flags are applied by default?\nTo understand what command is being run for a process, you can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied, this may not be straight forward. To understand what Salmon is doing, we‚Äôre going to use the nextflow log command and some custom bash code to track down the hidden .command.sh scripts for each Salmon quant process to find out how Salmon quant identified library type.\nUse the Nextflow log command to reveal information about executed pipelines in our working directory:\nnextflow log\nThis will print a list of executed pipelines, by default:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                          COMMAND                                                                                                                                                                                                                                                                                                                                                               \n2023-04-12 03:56:51     16m 19s         irreverent_allen        OK      f421ddc35d      93c2078a-15b0-4882-a9b1-ac7435bdcc57nextflow run rnaseq/main.nf --input /home/ubuntu/session2/materials/samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory '6 GB' --max_cpus 2 --outdir Exercise1\nAll recent runs will be listed, with the most recent last (i.e.¬†closest to your returned command prompt). Let‚Äôs query the logs for the previous lesson run. Run the command below after filling in your unique run name. For example:\nnextflow log irreverent_allen\nThat command listed out all the work sub-directories for all processes run. Recall that the actual tool commands issued by the nexflow processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these comamnd scripts. But how to find them?! Let‚Äôs add some custom bash code to query a Nextflow run with the run name from the previous lesson.\nFirst, save your run name in a bash variable:\nrun_name=<ENTER_YOUR_RUN_NAME>\nAnd let‚Äôs save the tool of interest (salmon) in another bash variable:\ntool=salmon\nNext, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n    cmd=$(ls ${line}/.command.sh 2>/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‚Äòsalmon‚Äô. There are multiple salmon steps in the workflow, inlcuding index and an R script. We are looking for salmon quant which performs the read quantification.\n/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh\n/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh\n/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh\n/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh\n/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh\n/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh\n/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh\n/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh\n/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh\n/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh\n/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh\n/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh\n/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh\nCompare the salmon quant command with the command run in the Salmon quant process script block:\ncat rnaseq/modules/nf-core/salmon/quant/main.nf\nCompared with the salmon quant main.nf file, we get more information from the .command.sh process scripts:\n\nLooking at the nf-core/rnaseq documentation, we can see library type is automatically inferred based on provided strandedness and this can be overridden using Salmon‚Äôs --libType=$strandedness flag. Following the recommendations in the Salmon documentation, we‚Äôre going to override this default with the nf-core/rnaseq workflow‚Äôs --salmon_quant_libtype A parameter.\n\n\n\n\n\n\nFor those not familiar with Salmon\n\n\n\nSalmon is a tool for transcript quantification using RNA-seq data. Library type is important for transcript quantification as it determines how reads are aligned to the reference transcriptome, how expression levels are estimated. Salmon can accurately determine library type based on alignment files. To avoid potential input errors in future runs, we‚Äôll allow Salmon to automatically detect the library type. You can read more about how Salmon performs library detection here.\n\n\n\n\n\nWriting a parameter file\nNextflow accepts either YAML or JSON formats for parameter files. YAML and JSON are formats for storing data objects and structures in a file and either is a valid choice for building your parameters file. We will create and apply a YAML file with our inputs for our second run, just because its easier to read. YAML files use a .yml and .yaml extension and follow these syntax rules:\n\nUses 3 dashes (---) to indicate the start of a document and 3 dots (‚Ä¶) to indicate the end\nUses an indentation heirarchy like Python to show a heirarchy in the data\nKey/value pairs are separated by a colon (:)\nLists begin with a hyphen\nEach key and value must be unique\nThe order of keys or values in a list doesn‚Äôt matter\n\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the syntax rules above:\n\nWrite a YAML file for the parameters run command that can be run by a collaborator working on a different computational infrastructure but the same input and reference files.\nAdd a key for the --salmon_quant_libtype A flag, we have added to the workflow.\n\nnextflow run rnaseq/main.nf \\\n    --input $materials/samplesheet.csv \\\n    -profile singularity \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --max_memory '6 GB' \\\n    --max_cpus 2 \n    --outdir Exercise1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as params.yaml:\n# experiment: WT vs KO mouse model\n# workflow: nf-core/rnaseq/3.11.1 \n---\ninput: \"/home/ubuntu/session2/materials/samplesheet.csv\" \noutdir: \"Exercise2\"\ngtf: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\"\nfasta: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\"\nstar_index: \"/home/ubuntu/session2/materials/mm10_reference/STAR\" \nsalmon_quant_libtype : A\n...\n\n\n\nAny of the workflow parameters can be added to the parameters file in this way.\n\n\nPassing an input parameter file\nOnce your params file has been saved, run the following, observing how the command is now shorter thanks to offloading some parameters to the params file. Note the use of a single - for ‚Äòresume‚Äô and ‚Äòparams-file‚Äô as these are Nextflow flags and not nf-core parmeters. Nextflow can use cached output! If we apply the -resume flag to the run, Nextflow will only compute what has not been changed. Rerun the workflow:\nnextflow run rnaseq/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -resume \\\n  -params-file params.yaml \\\n  --outdir Exercise2                                 \nAs we‚Äôve used the -resume flag, the initial pre-processing stage and STAR alignments should to be restored from cache and the Salmon and downstream QC steps will be recomputed. The rerun workflow should complete in ~ minutes\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 12-Apr-2023 04:29:39\nDuration    : 2m 5s\nCPU hours   : 0.4 (85% cached)\nSucceeded   : 15\nCached      : 185\n\n\n\n\n\n\nKey points\n\n\n\n\nA parameter file can be used to specify input parameters for any Nextflow workflow.\nSpecify parameter files in a workflow run command using the -params-file flag.\nParameter files can be written in YAML or JSON file formats.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.2_params.html",
    "href": "notebooks/2.2_params.html",
    "title": "Using a params file",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore workflow deployment and set up\n\n\n\nNow that the workflow is\n\n\n\n\n\n\nChallenge\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZoom reaction check in!\n\n\n\nIs everyone ok?\n:clap: (clap) yes, let‚Äôs move on.\n:cry: (cry) no, please help.\n\n\n\nRun the workflow\nMake a new working directory for this lesson and move into it:\nmkdir ~/nfcore-workshop/session2/exercise1 && cd $_\nFor the sake of expediency, we are using prepared subset data for this session. All the data (including fastqs, input manifest, reference fasta, gtf, and STAR indexes) are available on the CernVM-FS file system. CernVM-FS is a read-only file system that Pawsey have used to store files such as containerised tools (Biocontainers), reference datasets, and other shared resources that are commonly used by many researchers. Take a look here for more information on bioinformatics resources provided by Pawsey on Nimbus.\nTake a quick look at the workshop data we‚Äôre working with today:\nls /path/to/aarnet-cvmfs/training/workshopMaterials\nWe need to store this path in a variable for our run command:\nmaterials=/path/to/aarnet-cvmfs/training/workshopMaterials\nNow run the workflow:\nnextflow run nf-core-rnaseq-3.10.1/workflow/main.nf \\\n  --input $materials/samples.tsv \\\n  -profile singularity \\\n  --fasta $materials/mm10_chr18.fa \\\n  --gtf $materials/mm10_chr18.gtf \\\n  --star_index $materials/STAR \\\n  --max_memory '6 GB' --max_cpus 2 \\\n  --outdir ex1_results \\\n  -with-report execution_report_exercise2_1.html \\\n  -with-trace execution_trace_exercise2_1.txt \\\n  -with-timeline timeline_exercise2_1.html \\\n  -with-dag dag_exercise2_1.png\n\n\nReproducibility is a state of mind\nWe have to wait for the workflow to run (this should take ~17 mins) before we can get on with the other exercises. While we wait, lets discuss how we manage reproducibility in our own practices, and share some useful resources with one another:\n\n\n\nKey points\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.3_configEnv.html",
    "href": "notebooks/2.3_configEnv.html",
    "title": "Configuring a run for your environment",
    "section": "",
    "text": "Objectives\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.5_extArgs.html",
    "href": "notebooks/2.5_extArgs.html",
    "title": "Specifying external arguments to a process",
    "section": "",
    "text": "Objectives\n\nUnderstand how to use the ext.args feature to pass additional command-line arguments to a process\nImplement additional external arguments to a process that are not hardcoded in the process script\nWrite a custom configuration file for MultiQC in the YAML file format\nObserve the behaviour of Nextflow‚Äôs cache functionality\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "Welcome to session 2",
    "section": "",
    "text": "In this session we will be writing, running, adjusting, and re-running the nf-core/rnaseq workflow as we step through various customisation scenarios. While all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to (most) other nf-core workflows and do not require an understanding of rnaseq data processing. Before starting this session, there are a few things you should keep in mind as you proceed through the lessons and apply these techniques to your own research.\nAs with all open source bioinformatics resources, nf-core workflows may not suit all applications. It is important that you understand the needs of your dataset and research questions before deciding on a workflow. All nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. What is ‚Äòsensible‚Äô varies dramatically between different experiments, computing environments, and datasets, so these settings might not suit your needs.\n\nIntroducing the case study\nFor the purposes of this session, we are working with a (subset) dataset from a knockout mouse model study by Corley et al.¬†(2016). The authors used the mouse model to simulate the role of a specific gene (Gtf2ird1) in Williams-Beuren Syndrome (WBS), a rare genetic disease in people. Today, we are performing the pre-processing steps in a slightly different way from the authors, to generate a set of files that can be analysed downstream. In deciding whether or not the nf-core/rnaseq workflow was suitable for reproducing the results presented in this study, we considered a number of factors, including:\n\n\n\n\n\n\n\n\nConsideration\nQuestions to ask of our experiment\nWhy\n\n\n\n\nSize of dataset\nNumber of samples and data volume\nScale of data impacts computational efficiency of the workflow\n\n\nInput data\nType of RNA sequenced, availability of reference files\nDetermines if we meet input requirements of workflow\n\n\nResearch questions\nSuitability of workflow outputs\nNeed the right processed data for downstream analysis\n\n\nComputational resources\nCPU, memory, RAM available and minimum requirements of the workflow\nDetermines if I have enough resources to run the workflow\n\n\nTool preferences\nSuitability of each tool, required inputs and outputs\nWorkflow offers multiple tools for some steps, determines which choices I make\n\n\n\nWe consulted the nf-core/rnaseq documentation to confirm that nf-core/rnaseq is a suitable workflow for our application. W‚Äôve sketched out our experimental design, those considerations, and our choices below. We will discuss these further, shortly.\n\n\n\nLog back in to your instance\n\nIn Visual Studio Code\nSame as yesterday, connect to your instance using the command palatte:\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\nIn a terminal\nWith a terminal application, run the following on the command-line: default  ssh training@###.###.###.### Enter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\nEnter password:\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. These may not always suit your needs.\nTo decide whether an nf-core workflow is the right choice for your experiment you need to understand the needs of your dataset and research questions.\nUse the workflow documentation to understand the requirements for running a workflow.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.3_configure.html",
    "href": "notebooks/1.3_configure.html",
    "title": "Configuring nf-core workflows",
    "section": "",
    "text": "nf-core workflows follow a set of best practices and standardized conventions. nf-core workflows start from a common template and follow the same structure. Although you won‚Äôt need to edit code in the workflow project directory, having a basic understanding of the project structure will help you understand how to configure its execution and where files of interest are located.\nNotably, while some of these files are already included in the nf-core workflow repository (e.g., the nextflow.config file in the nf-core workflow repository), others are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file, and -c).\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.3_configure.html#command-line-parameters",
    "href": "notebooks/1.3_configure.html#command-line-parameters",
    "title": "Configuring nf-core workflows",
    "section": "Command line parameters",
    "text": "Command line parameters\nAt the highest level, parameters can be customized at execution using the command line. Any parameter can be specified on the command line by prefixing the parameter name with a double dash (--):\nnextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhile Nextflow options are prefixed with a single dash (-) all workflow parameters are prefixed with a double dash (--)."
  },
  {
    "objectID": "notebooks/1.3_configure.html#default-configuration-files",
    "href": "notebooks/1.3_configure.html#default-configuration-files",
    "title": "Configuring nf-core workflows",
    "section": "Default configuration files",
    "text": "Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory.\nThere are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by default.\nModule-specific configuration options (both mandatory and optional).\n\ntest.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a small test dataset.\n\ntest_full.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a full-size test dataset.\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;workflow&gt; -profile test,singularity\nBy default, nf-core workflows are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the workflows with software installed by other methods (e.g., environment modules or manual installation), most users find that Docker and Singularity are most convenient and reproducible.\n\n\n\n\n\n\nTip\n\n\n\nIf you‚Äôre computer has internet access and one of Conda, Singularity, or Docker installed, you will be able to run nf-core workflows with a test profile and the respective software management profile. The test data profile will pull test data directly from the nf-core/test-data GitHub repository and run it on your local system. This can be a test a workflow without setting up your own test data."
  },
  {
    "objectID": "notebooks/1.3_configure.html#shared-configuration-files",
    "href": "notebooks/1.3_configure.html#shared-configuration-files",
    "title": "Configuring nf-core workflows",
    "section": "Shared configuration files",
    "text": "Shared configuration files\nAn includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core workflows will fetch these configuration profiles from the remote configs repository and make them available.\nFor shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help."
  },
  {
    "objectID": "notebooks/1.3_configure.html#custom-configuration-files",
    "href": "notebooks/1.3_configure.html#custom-configuration-files",
    "title": "Configuring nf-core workflows",
    "section": "Custom configuration files",
    "text": "Custom configuration files\nNextflow will also look for custom configuration files that are external to the workflow project directory. These files include:\n\nThe config file $HOME/.nextflow/config\nA config file named nextflow.config in the current directory\nCustom files specified using the command line\n\nA parameter file that is provided using the -params-file option\nA config file that are provided using the -c option\n\n\nYou can be clever with the what settings you configure in each of these files.\nFor example, you might consider configuring settings that are unique to you as a user (such as your name and email address) in your home directory as this will be applied to every execution.\n\nParamteter files\nParameter files are .json files that can contain an unlimited number of parameters:\n{\n   \"&lt;parameter1_name&gt;\": 1,\n   \"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n   \"&lt;parameter3_name&gt;\": true\n}\nYou can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option.\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -param-file &lt;path/to/params.json&gt;\n\n\nConfiguration files\nConfiguration files are .config files that can contain various workflow properties.\nCustom paths passed in the command-line using the -c option:\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example:\nalpha.x  = 1\nalpha.y  = 'string value..'\nIs equivalent to:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster and the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\nImportantly, the process scope allows you to configure workflow processes and is used extensively to define resources and additional arguments for modules.\nBy default, process resources are allocated in the conf/base.config file using the withLabel selector:\nprocess {\n    withLabel: BIG_MEM {\n        cpus = 16\n        memory = 64.GB\n    }\n}\nSimilarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file:\nprocess {\n    withName: HELLO {\n        cpus = 4\n        memory = 8.GB\n    }\n}\nWhile some tool arguments are included as a part of a module. To make modules sharable across workflows, most tool arguments are defined in the conf/modules.conf file in the workflow code under the ext.args entry.\nFor example, if you were trying to overwrite arguments in the TRIMGALORE process in the nf-core/rnaseq workflow, you could use the process scope:\nprocess {\n    withName : \".*:TRIMGALORE\" {\n        ext.args   = { \"&lt;your custom parameter&gt;\" }\n\n    }\nHowever, as the TRIMGALORE process is used multiple times in this workflow, an extended execution path of the module is required to make it more specific:\nprocess {\n    withName: \"NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE\" {\n        ext.args = \"&lt;your custom parameter&gt;\"\n    }\n}\nThe extended execution path is built from the workflows, subworkflows, and modules used to execute the process.\nIn the example above, the nf-core TRIMGALORE module, was called by the FASTQ_FASTQC_UMITOOLS_TRIMGALORE subworkflow, which was called by the RNASEQ workflow, which was called by the NFCORE_RNASEQ workflow in the main.nf file in the nf-core/rnaseq repository.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure of how to build the execution path you can copy it from the conf/modules.conf file.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommand line flags (--&lt;parameter&gt;) or .json files passed with the -params-file option MUST be used to define custom workflow parameters. Parameters that are defined in the parameter block in custom.config files WILL NOT override defaults in nextflow.config for nf-core workflows.\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3"
  },
  {
    "objectID": "notebooks/0.0_template.html",
    "href": "notebooks/0.0_template.html",
    "title": "Lesson title",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore workflow deployment and set up\n\n\n\n\nDownload the workflow code\nIt can be very easy to lose track while working on the command line, especially when we‚Äôre working with large datasets and complex commands as we do with bioinformatics workflows. To make sure we work reproducibly, we will be organising our workspace and using a local copy of the nf-core/rnaseq workflow for all exercises.\nStart by creating a new directory for all of today‚Äôs activities and move into it:\nmkdir ~/nfcore-workshop/session2 && cd $_\nThere are a number of ways to download a nf-core workflow to your machine. We recommend using git or the nf-core tools utility. Today, we will download most recent version (3.11.1) of the workflow from it‚Äôs GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\nInside your nf-core-rnaseq workflow directory, you should see a number of files and subdirectories:\nls -l rnaseq\ntotal 216\n-rw-rw-r-- 1 ubuntu ubuntu 58889 Apr  4 03:40 CHANGELOG.md\n-rw-rw-r-- 1 ubuntu ubuntu  9681 Apr  4 03:40 CITATIONS.md\n-rw-rw-r-- 1 ubuntu ubuntu  9078 Apr  4 03:40 CODE_OF_CONDUCT.md\n-rw-rw-r-- 1 ubuntu ubuntu  1096 Apr  4 03:40 LICENSE\n-rw-rw-r-- 1 ubuntu ubuntu 10002 Apr  4 03:40 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr  4 03:40 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 modules\n-rw-rw-r-- 1 ubuntu ubuntu 13970 Apr  4 03:40 modules.json\n-rw-rw-r-- 1 ubuntu ubuntu 10903 Apr  4 03:40 nextflow.config\n-rw-rw-r-- 1 ubuntu ubuntu 42576 Apr  4 03:40 nextflow_schema.json\n-rw-rw-r-- 1 ubuntu ubuntu   359 Apr  4 03:40 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 subworkflows\n-rw-rw-r-- 1 ubuntu ubuntu  1684 Apr  4 03:40 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 workflows\nThis may look very different to other Nextflow workflows you may have run or written before. The most important files and directories for us to understand, are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq workflow\n\n\n\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nUsing the nf-core tools utility, search for the rnaseq pipeline:\nnf-core list rnaseq\nThen, download the correct pipeline:\nnf-core download nf-core/rnaseq\nYou will be prompted to select a version. Use your arrow keys to specify 3.10.1 and hit enter.\n\n‚õî BEWARE ‚õî this method will download a copy of the workflow with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust commands in the upcoming lessons accordingly.\n\n\n\n\n\n\nBuild your run command\nAll nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. In the case of the nf-core/rnaseq workflow, parameters are grouped based on various stages of the workflow:\n\nInput/output options for specifying which files to process and where to save results\nUMI options for processing reads with unique molecular identifiers (UMI)\nRead filtering options to be run prior to alignment\nReference genome options related to pre-processing of the reference FASTA\nRead trimming options prior to alignment\nAlignment options for read mapping and filtering criteria\nProcess skipping options for adjusting the processes to run with the workflow\n\nOn the command line you can view these options by running:\nnextflow run ../rnaseq/main.nf --help \nNotice at the bottom of the print out, there is:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nThree additional parameter sections are hidden from view. This is because they are less commonly used. They include:\n\nInstitutional config options for various compute environments\nMax job request options for limiting memory and cpu usage based on what is available to you\nGeneric options focused on how the pipeline is run\n\nTo view all the workflow run options on the command line, run:\nnextflow run ../rnaseq/main.nf --help --show_hidden_params\n\n\n\n\n\n\nHyphens matter!\n\n\n\nHyphens matter when it comes to parameter flags in nf-core workflows! Nextflow command-line parameters use one (-), whereas pipeline-specific parameters use two (‚Äì). For example: -profile is a Nextflow parameter, while ‚Äìinput is an nf-core parameter.\n\n\nWe will be using a number of flags from each parameters options section. In addition to the required parameters, we have chosen to use some additional flags that are suitable for our experiment. What is ‚Äòsuitable‚Äô varies dramatically between different experiments, computing environments, and datasets, so these settings might not meet your needs for other experiments. Consider the most important design choices we have made for this workflow and structuring our run command:\n\nWe don‚Äôt need to run the pseudo alignment step (Stage 3)\nWe have chosen to use STAR to align reads\nWe have chosen to use Salmon to estimate transcript abundance\nWe only have access to 2 CPUs and 8Gb of RAM today\nWe have provided the requisite reference data (fasta, gtf, indexes) for our training dataset\n\n\nFor the sake of expediency, we are using prepared subset data for this session. All the data (including fastqs, input manifest, reference fasta, gtf, and STAR indexes) are available on an external file system called CernVM-FS. CernVM-FS is a read-only file system that Pawsey have used to store files such as containerised tools (Biocontainers), reference datasets, and other shared resources that are commonly used by many researchers. Take a look here for more information on bioinformatics resources provided by Pawsey on Nimbus.\n\n\nRun the workflow\nWe need to store the path to our input and reference data in a variable for our run command:\nmaterials=/path/to/aarnet-cvmfs/training/workshopMaterials\nNow run the workflow:\nnextflow run rnaseq/main.nf \\\n    --input $materials/samplesheet.csv \\\n    -profile singularity \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --max_memory '6 GB' \\\n    --max_cpus 2 \\\n    --outdir Exercise1\n\n\n\n\n\n\nChallenge\n\n\n\nCan you use the nf-core/rnaseq documentation and the important considerations above to explain how we designed our run command for this experiment?\nüí° You will need to look at the reference genome, alignment, and max job request sections.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven we are using STAR and Salmon as our aligner and quantification tool of choice (respectively) and it is the default choice of this workflow we will not need to provide an --aligner flag. However, if you wanted to provide this for the sake of reproducibility in case things change in the future:\n--aligner 'star_salmon'\nGiven we are providing our own subset data for this workshop, we will need to use:\n--fasta /path/to/mouse.fa  \n--gtf /path/to/mouse.gtf \n--star_index /path/to/STAR\nGiven we have limited computing resources today, we will need to specify a ceiling for both memory and CPUs:\n--max_memory '6.GB' \n--max_cpus 2 \n\n\n\n\n\nExamine the outputs\nOnce your workflow has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\n\nCompleted at: 12-Apr-2023 04:13:09\nDuration    : 16m 18s\nCPU hours   : 0.4\nSucceeded   : 200\nThe workflow ran successfully, however note the warning about all samples having failed the strandedness check. We‚Äôll explore that in the next lesson. In the meantime, list (ls -la) the contents of your directory, you‚Äôll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu   4096 Apr 12 03:56 .\ndrwxr-x---  13 ubuntu ubuntu   4096 Apr 11 05:16 ..\ndrwxrwxr-x   4 ubuntu ubuntu   4096 Apr 12 04:13 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 389299 Apr 12 04:13 .nextflow.log\ndrwxrwxr-x   7 ubuntu ubuntu   4096 Apr 12 04:13 Exercise1\ndrwxrwxr-x  13 ubuntu ubuntu   4096 Apr  4 03:40 rnaseq\ndrwxrwxr-x 141 ubuntu ubuntu   4096 Apr 12 04:10 work\nNextflow has created 2 new output directories, work and results in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe results directory\nDepending on the nf-core workflow you are working with, all final outputs will be presented in a directory called results by default. You can override this default and output to a directory of your own choosing by using the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a workflow and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. You can adjust some settings as required by applying flags to your run command.\nnf-core workflows are all built from a template that means they have a standard structure to their code bases\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html",
    "href": "notebooks/1.2_nfcore.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and run them using their own data and compute resources.\nThe nf-core community comprises a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276‚Äì278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won‚Äôt be left in the dark.\n\nCI Testing\n\nEvery time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to run.\n\nCloud-ready\n\nnf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.\n\n\nIt is important to remember all nf-core workflows are open-source and community driven. Most pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results.\n\n\nnf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.\nUpcoming events are listed on the nf-core event page and announced on Slack and Twitter.\n\n\n\nThere are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining the nf-core Slack can be especially useful for users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start.\nIf you are joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself!\n\nQuestions about Nextflow that are not related to nf-core can be asked on the Nextflow Slack.\n\n\n\n\nThis workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality using, developing, and testing workflows.\nnf-core tools is written in Python and is available from the Python Package Index (PyPI):\npip install nf-core\nAlternatively, nf-core tools can be installed from Bioconda:\nconda install -c bioconda nf-core\nYou can nf-core --version options can be used to print your version of nf-core tools:\nnf-core --version\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what version of nf-core tools you have available using the nf-core --version option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core --version option to print the nf-core tools version:\nnf-core --version\n\n\n\n\nnf-core tools is for everyone and has commands to help both users and developers. For users, the tools make it easier to run workflows. For developers, the tools make it easier to develop and test your workflows using best practices.\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what other nf-core tools options and commands are available using the --help option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the --help option to list the options, tools for users, and tools for developers\nnf-core --help\n\n\n\n\n\n\n\nThere are currently 80 workflows (March 2023) available as part of nf-core. These workflows are at various stages of development with 49 released, 19 under development, and 12 archived.\nThe nf-core website has a full list of workflows, as well as their documentation, which can be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to run the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nUnless you are actively developing workflow code, you do not need to clone the workflow code from GitHub and can use Nextflow‚Äôs built-in functionality to pull and a workflow. As shown in the Introduction to Nextflow, the Nextflow pull command can download and cache nf-core workflows from the nf-core GitHub repository:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to run a previous version of a workflow once it is released using the -revision or -r flag.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to pull the latest version of the nf-core/rnaseq workflow directly from GitHub using Nextflow:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextlfow pull command to download the rnaseq workflow from the nf-core GitHub repository.\nnextflow pull nf-core/rnaseq\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html#events",
    "href": "notebooks/1.2_nfcore.html#events",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "nf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.\nUpcoming events are listed on the nf-core event page and announced on Slack and Twitter."
  },
  {
    "objectID": "notebooks/1.2_nfcore.html#join-the-community",
    "href": "notebooks/1.2_nfcore.html#join-the-community",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "There are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining the nf-core Slack can be especially useful for users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start.\nIf you are joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself!\n\nQuestions about Nextflow that are not related to nf-core can be asked on the Nextflow Slack."
  },
  {
    "objectID": "notebooks/1.2_nfcore.html#nf-core-tooling",
    "href": "notebooks/1.2_nfcore.html#nf-core-tooling",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "This workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality using, developing, and testing workflows.\nnf-core tools is written in Python and is available from the Python Package Index (PyPI):\npip install nf-core\nAlternatively, nf-core tools can be installed from Bioconda:\nconda install -c bioconda nf-core\nYou can nf-core --version options can be used to print your version of nf-core tools:\nnf-core --version\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what version of nf-core tools you have available using the nf-core --version option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core --version option to print the nf-core tools version:\nnf-core --version\n\n\n\n\nnf-core tools is for everyone and has commands to help both users and developers. For users, the tools make it easier to run workflows. For developers, the tools make it easier to develop and test your workflows using best practices.\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what other nf-core tools options and commands are available using the --help option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the --help option to list the options, tools for users, and tools for developers\nnf-core --help"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html#running-an-nf-core-workflow",
    "href": "notebooks/1.2_nfcore.html#running-an-nf-core-workflow",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "There are currently 80 workflows (March 2023) available as part of nf-core. These workflows are at various stages of development with 49 released, 19 under development, and 12 archived.\nThe nf-core website has a full list of workflows, as well as their documentation, which can be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to run the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nUnless you are actively developing workflow code, you do not need to clone the workflow code from GitHub and can use Nextflow‚Äôs built-in functionality to pull and a workflow. As shown in the Introduction to Nextflow, the Nextflow pull command can download and cache nf-core workflows from the nf-core GitHub repository:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to run a previous version of a workflow once it is released using the -revision or -r flag.\n\n\n\n\n\n\nChallenge\n\n\n\nTry to pull the latest version of the nf-core/rnaseq workflow directly from GitHub using Nextflow:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextlfow pull command to download the rnaseq workflow from the nf-core GitHub repository.\nnextflow pull nf-core/rnaseq\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nPoint 1\nPoint 2\nPoint 3"
  }
]