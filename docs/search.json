[
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Some tips and tricks",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "Questions\n\nHow do I install a terminal or IDE application on my computer?\nHow do I log in to a Nimbus instance?\n\n\nIn this workshop series, we will be using Pawsey’s Nimbus cloud. The Pawsey Supercomputing Research Centre is one of two, Tier-1, High Performance Computing facilities in Australia. Their Nimbus cloud platform is an accessible and flexible solution for bioinformatics applications that may not be suitable for large-scale HPC machines including:\n\nDeveloping and refining scalable workflows in prepration for HPC allocation applications.\nWorkflows with long runtimes that excede wall time queue limits on HPC facilities.\nComplex data-bound workflows with variable compute resource profiles that are common in bioinformatics pipelines.\n\nThe main requirements for this workshop are a personal computer with:\n\nA web broswer\nTerminal or IDE application\n\nOn this page you will find instructions on how to set up a terminal application and web browser on your computer and how to connect to Nimbus. Each participant will be provided with their instance’s IP address at the beginning of the workshop.\nTo connect to your Nimbus instance, you will need either a terminal or integrated development environment (IDE) application installed on your computer. While we recommend you use the Visual Studio Code IDE for this workshop, we have also provided directions for installing and using a terminal applications below.\n\nInstall and set up Visual Studio Code\nVisual Studio Code is a lightweight and powerful source code editor available for Windows, macOS and Linux computers.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nLinux\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\n\nInstall the Live Server extension. Click on the blue install button.\n\n\n\nLogin via Visual Studio Code\n\nConnect to your instance with VS code by adding the host details to your ssh config file.\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Open SSH configuraiton file\nAdd new entry, filling out host name and identity file:\n\nHost nfcoreWorkshop\n  HostName 146.118.XX.XXX  \n  User training     \nConnect to this address\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\n\n\n\n\nInstall and set up a terminal application\nThe terminal applications available to you will depend on your operating system.\n\nLinux terminals\nIf you use Linux, chances are you already know your shell and how to use it. Basically, just open your preferred terminal program and off you go!\n\n\nOS X (Mac)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‘terminal’. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\n\n\nWindows\nWe recommend MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‘Home Edition’ select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‘start local terminal’ (and install Cygwin if prompted)\n\n\n\n\nLogin via Terminal\nTo log in to Nimbus, we will use a Secure Shell (SSH) connection. To connect, you need 3 things: 1. The assigned IP address of your instance (i.e. ###.###.##.###). Each participant will be provided with their instance’s IP address at the beginning of the workshop. 2. Your login name. In our case, this will be training for all participants. 3. Your password. All participants will be provided with a password at the beginning of the workshop.\nTo log in, type the following into your terminal, using your login name and the instance’s IP address:\nssh training@###.###.###.###\nYou will receive a message saying:\nThe authenticity of host 'XXX.XXX.XX.XXX (XXX.XXX.XX.XXX)' can't be established.\nRemember your host address will be different than the one above. There will then be a message saying:\nAre you sure you want to continue connecting (yes/no)?\nIf you would like to skip this message next time you log in, answer ‘yes’. It will then give a warning:\nWarning: Permanently added 'XXX.XXX.XX.XXX' (ECDSA) to the list of known hosts.\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you’ve forgotten it.\n\n\n\n\n\n\nPay Attention\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You’ll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.0_template.html",
    "href": "notebooks/0.0_template.html",
    "title": "Lesson title",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nlearning outcome 1\nlearning outcome 2\n\n\n\nSome intro text to what this lesson is about\n\nSub-section heading\nCommands should be written like this:\ncommand \nWhere relevant include expected standard output:\nstdout here\nAny important notes for attendees should be present in information boxes. For example:\n\n\n\n\n\n\nCopying the code from the grey boxes on training materials\n\n\n\nIn this workshop we need to copy code from the grey boxes in the training materials and run it in the terminal. If you hover your mouse over a grey box on the website, a clipboard icon will appear on the right side. Click on the clipboard logo to copy the code. Test it out with:\nssh training@###.###.###.###\n\n\nChallenges/activites should be provided in challenge boxes:\n\n\n\n\n\n\nChallenge\n\n\n\nQuestion or activity\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution explanation and code where relevant\n\n\n\n\nFor other types of callout blocks see here. Any figures should be placed in figs directory and embeddeded like this: \n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.2_params.html",
    "href": "notebooks/2.2_params.html",
    "title": "Using a parameter file",
    "section": "",
    "text": "Objectives\n\n\n\n\nWrite a parameter file\nUnderstand the YAML file format\nRerun the workflow using a params file\nUnderstand the use of the params file for reproducible and transparent research\n\n\n\nIn Nextflow, parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Parameters are used in nf-core workflows to specify input and output files and define other aspects of workflow execution. Each nf-core workflow comes with a default set of parameters that can be customised to suit specific requirements. In the previous lesson we supplied these parameters in our run command, on the command line. Specifying multiple parameters like this can be messy and hard to keep track of.\nNextflow allows us to pass all parameters to a workflow’s run command using the -params-file flag and a JSON or YAML file. Using a parameter file makes it easier to rerun and reproduce our code, we can also share these files with our collaborators and provide as supplementary file in a publication. In this lesson we’re going to adjust our run command and rerun the workflow using a parameter file, rather than specifying all parameters on the command line.\n\nRevisit the strandedness issue\nWhile our workflow completed successfully, all fastq files failed the strandedness check:\n[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 11-Apr-2023 01:01:54\nDuration    : 17m 4s\nCPU hours   : 0.3\nSucceeded   : 200\nLet’s take a look at the MultiQC report, as directed. You can find this report in the results/ directory:\nls results/multiqc/star_salmon\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 11 01:01 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 11 01:01 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 11 01:01 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 11 01:01 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483384 Apr 11 01:01 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe issue here is provided strandedness that we specified in our samplesheet.csv and inferred strandedness do not match. Look’s like we’ve incorrectly specified strandedness as forward, when our reads show an equal distribution of sense and antisense reads.\n\nHow can we know what tool flags are applied by default?\nIt can be challenging to troubleshoot nf-core workflows because of the scale and structure of the workflow code. To understand what command is being run for a process, you can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied, this may not be straight forward. To understand what Salmon is doing, we’re going to use the nextflow log command and some custom bash code to track down the hidden .command.sh scripts for each Salmon quant process.\nUse the Nextflow log command to reveal information about executed pipelines in our working directory:\nnextflow log\nThis will print a list of executed pipelines, by default:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND \n2023-04-11 00:44:49     17m 5s          golden_yonath           OK      f421ddc35d      db6b8373-5e27-4a79-bf9b-6c56cda4bb6c    nextflow run rnaseq/main.nf --input /home/ubuntu/session2/materials/samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory '6 GB' --max_cpus 2 --outdir Exercise_1\nAll recent runs will be listed, with the most recent last (i.e. closest to your returned command prompt). Let’s query the logs for the previous lesson run. Run the command below after filling in your unique run name. For example:\nnextflow log golden_yonath\nThat command listed out all the work sub-directories for all processes run. Recall that the actual tool commands issued by the nexflow processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these comamnd scripts. But how to find them?! Let’s add some custom bash code to query a Nextflow run with the run name from the previous lesson.\nFirst, save your run name in a bash variable:\nrun_name=<ENTER_YOUR_RUN_NAME>\nAnd let’s save the tool of interest (salmon) in another bash variable:\ntool=salmon\nNext, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n    cmd=$(ls ${line}/.command.sh 2>/dev/null);     \\\n      if grep -q $tool $cmd;     \\\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‘salmon’. There are multiple salmon steps in the workflow, inlcuding index and an R script. We are looking for salmon quant which performs the read quantification.\n/home/ubuntu/session2/work/cb/6959bb97e93a5e433d690233cab9f9/.command.sh\n/home/ubuntu/session2/work/cf/bebfb9626bd3cde6d9f56dd952c5dd/.command.sh\n/home/ubuntu/session2/work/73/345d1d9b74f2b955a07f26f1d8e307/.command.sh\n/home/ubuntu/session2/work/f2/5c28c327f6d7df4d0b89db419cf833/.command.sh\n/home/ubuntu/session2/work/cb/50978a7c19becbdd5e3cc2099ca9b8/.command.sh\n/home/ubuntu/session2/work/c5/66c98171bb83bae482652ad8937e9b/.command.sh\n/home/ubuntu/session2/work/23/b88d62b3bf462930a227d1372dd664/.command.sh\n/home/ubuntu/session2/work/99/6ea0e9670dfea9ded434badcc731a9/.command.sh\n/home/ubuntu/session2/work/c4/d5eeca5174a0d6a6c5c8a677265c30/.command.sh\n/home/ubuntu/session2/work/92/02fab5c1de54025e6189a06017c371/.command.sh\n/home/ubuntu/session2/work/44/fbf4d28bc12e33fe1a7fe44116e2cb/.command.sh\n/home/ubuntu/session2/work/3a/d50a08afc9328aacae246b2a05f267/.command.sh\n/home/ubuntu/session2/work/59/75d9bca41c44698955536c4087b181/.command.sh\nCompared with the salmon quant main.nf file, we get more information from the .command.sh process scripts:\n\nLooking at the nf-core/rnaseq documentation, we can see library type is automatically inferred based on provided strandedness and this can be overridden using the --libType=$strandedness parameter. Following the recommendations in the Salmon documentation, we’re going to override this default with --salmon_quant_libtype A.\n\n\n\n\n\n\nAutomatic library type detection with Salmon\n\n\n\nSalmon is a tool for transcript quantification using RNA-seq data. Library type is important for transcript quantification as it determines how reads are aligned to the reference transcriptome, how expression levels are estimated. Salmon can accurately determine library type based on alignment files. To avoid potential input errors in future runs, we’ll allow Salmon to automatically detect the library type. You can read more about how Salmon performs library detection here.\n\n\n\n\n\nWriting a parameter file\nNextflow accepts either YAML or JSON formats for parameter files. YAML and JSON are data ways of storing data objects and structures in a file and either is a valid choice for building your parameters file. We will create and apply a YAML file with our inputs for our second run, because its easier to read. YAML files use a .yml and .yaml extension and follow these syntax rules:\n\nUses 3 dashes (---) to indicate the start of a document and 3 dots (…) to indicate the end\nUses an indentation heirarchy like Python to show a heirarchy in the data\nKey/value pairs are separated by a colon (:)\nLists begin with a hyphen\nEach key and value must be unique\nThe order of keys or values in a list doesn’t matter\n\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the syntax rules above:\n\nWrite a YAML file for the parameters run command that can be run by a collaborator working on a different computational infrastructure but the same input and reference files\nAdd a key for the --salmon_quant_libtype A flag, we have added to the workflow\n\nnextflow run rnaseq/main.nf \\\n    --input $materials/samplesheet.csv \\\n    -profile singularity \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --max_memory '6 GB' \\\n    --max_cpus 2 \n    --outdir Exercise1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as params.yaml:\n# experiment: WT vs KO mouse model\n# workflow: nf-core/rnaseq/3.11.1 \n---\ninput: \"/home/ubuntu/session2/materials/samplesheet.csv\" \noutdir: \"Exercise2\"\ngtf: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf\"\nfasta: \"/home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa\"\nstar_index: \"/home/ubuntu/session2/materials/mm10_reference/STAR\" \nsalmon_quant_libtype : A\n...\n\n\n\nAny of the workflow parameters can be added to the parameters file in this way.\n\n\nPassing an input parameter file\nOnce your params file has been saved, run the following, observing how the command is now shorter thanks to offloading some parameters to the params file. Note the use of a single - for ‘resume’ and ‘params-file’ as these are Nextflow flags and not nf-core parmeters. Nextflow can use cached output! If we apply the -resume flag to the run, Nextflow will only compute what has not been changed. We should expect the initial fastqc and STAR alignments to be restored from cache and the Salmon steps to be recomputed. Rerun the workflow:\nnextflow run rnaseq/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -resume \\\n  -params-file params.yaml                                            \n\n\n\n\n\n\nKey points\n\n\n\n\n1\n2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcoretools.html",
    "href": "notebooks/1.2_nfcoretools.html",
    "title": "nf-core workflows series",
    "section": "",
    "text": "Using the nf-core tools utility\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.4_multiConfig.html",
    "href": "notebooks/2.4_multiConfig.html",
    "title": "Using multiple configuration files at once",
    "section": "",
    "text": "Objectives\n\nUnderstand the heirarchy of configuration files specified by Nextflow\nWrite a custom configuration file for MultiQC in the YAML file format\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "Welcome to session 2",
    "section": "",
    "text": "In this session we will be writing, running, adjusting, and re-running the nf-core/rnaseq workflow as we step through various customisation scenarios. While all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to (most) other nf-core workflows and do not require an understanding of rnaseq data processing. Before starting this session, there are a few things you should keep in mind as you proceed through the lessons and apply these techniques to your own research.\nAs with all open source bioinformatics resources, nf-core workflows may not suit all applications. It is important that you understand the needs of your dataset and research questions before deciding on a workflow. All nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. What is ‘sensible’ varies dramatically between different experiments, computing environments, and datasets, so these settings might not suit your needs.\n\nIntroducing the case study\nFor the purposes of this session, we are working with a (subset) dataset from a knockout mouse model study by Corley et al. (2016). The authors used the mouse model to simulate the role of a specific gene (Gtf2ird1) in Williams-Beuren Syndrome (WBS), a rare genetic disease in people. Today, we are performing the pre-processing steps in a slightly different way from the authors, to generate a set of files that can be analysed downstream. In deciding whether or not the nf-core/rnaseq workflow was suitable for reproducing the results presented in this study, we considered a number of factors, including:\n\n\n\n\n\n\n\n\nConsideration\nQuestions to ask of our experiment\nWhy\n\n\n\n\nSize of dataset\nNumber of samples and data volume\nScale of data impacts computational efficiency of the workflow\n\n\nInput data\nType of RNA sequenced, availability of reference files\nDetermines if we meet input requirements of workflow\n\n\nResearch questions\nSuitability of workflow outputs\nNeed the right processed data for downstream analysis\n\n\nComputational resources\nCPU, memory, RAM available and minimum requirements of the workflow\nDetermines if I have enough resources to run the workflow\n\n\nTool preferences\nSuitability of each tool, required inputs and outputs\nWorkflow offers multiple tools for some steps, determines which choices I make\n\n\n\nWe consulted the nf-core/rnaseq documentation to confirm that nf-core/rnaseq is a suitable workflow for our application. W’ve sketched out our experimental design, those considerations, and our choices below. We will discuss these further, shortly.\n\n\n\nLog back in to your instance\n\nIn Visual Studio Code\nSame as yesterday, connect to your instance using the command palatte:\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\nIn a terminal\nWith a terminal application, run the following on the command-line: default  ssh training@###.###.###.### Enter the password provided at the beginning of the workshop. Ask one of the demonstrators if you’ve forgotten it.\nEnter password:\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. These may not always suit your needs.\nTo decide whether an nf-core workflow is the right choice for your experiment you need to understand the needs of your dataset and research questions.\nUse the workflow documentation to understand the requirements for running a workflow.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.5_extArgs.html",
    "href": "notebooks/2.5_extArgs.html",
    "title": "Specifying external arguments to a process",
    "section": "",
    "text": "Objectives\n\nUnderstand how to use the ext.args feature to pass additional command-line arguments to a process\nImplement additional external arguments to a process that are not hardcoded in the process script\nWrite a custom configuration file for MultiQC in the YAML file format\nObserve the behaviour of Nextflow’s cache functionality\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.3_configEnv.html",
    "href": "notebooks/2.3_configEnv.html",
    "title": "Configuring a run for your environment",
    "section": "",
    "text": "Objectives\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_troubleshoot.html",
    "href": "notebooks/2.6_troubleshoot.html",
    "title": "Troubleshooting issues and errors",
    "section": "",
    "text": "Objectives\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "Designing your run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore workflow deployment and set up\n\n\n\n\nDownload the workflow code\nIt can be very easy to lose track while working on the command line, especially when we’re working with large datasets and complex commands as we do with bioinformatics workflows. To make sure we work reproducibly, we will be organising our workspace and using a local copy of the nf-core/rnaseq workflow for all exercises.\nStart by creating a new directory for all of today’s activities and move into it:\nmkdir ~/nfcore-workshop/session2 && cd $_\nThere are a number of ways to download a nf-core workflow to your machine. We recommend using git or the nf-core tools utility. Today, we will download most recent version (3.11.1) of the workflow from it’s GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\nInside your nf-core-rnaseq workflow directory, you should see a number of files and subdirectories:\nls -l rnaseq\ntotal 216\n-rw-rw-r-- 1 ubuntu ubuntu 58889 Apr  4 03:40 CHANGELOG.md\n-rw-rw-r-- 1 ubuntu ubuntu  9681 Apr  4 03:40 CITATIONS.md\n-rw-rw-r-- 1 ubuntu ubuntu  9078 Apr  4 03:40 CODE_OF_CONDUCT.md\n-rw-rw-r-- 1 ubuntu ubuntu  1096 Apr  4 03:40 LICENSE\n-rw-rw-r-- 1 ubuntu ubuntu 10002 Apr  4 03:40 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  4 03:40 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr  4 03:40 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 modules\n-rw-rw-r-- 1 ubuntu ubuntu 13970 Apr  4 03:40 modules.json\n-rw-rw-r-- 1 ubuntu ubuntu 10903 Apr  4 03:40 nextflow.config\n-rw-rw-r-- 1 ubuntu ubuntu 42576 Apr  4 03:40 nextflow_schema.json\n-rw-rw-r-- 1 ubuntu ubuntu   359 Apr  4 03:40 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  4 03:40 subworkflows\n-rw-rw-r-- 1 ubuntu ubuntu  1684 Apr  4 03:40 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  4 03:40 workflows\nThis may look very different to other Nextflow workflows you may have run or written before. The most important files and directories for us to understand, are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq workflow\n\n\n\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nUsing the nf-core tools utility, search for the rnaseq pipeline:\nnf-core list rnaseq\nThen, download the correct pipeline:\nnf-core download nf-core/rnaseq\nYou will be prompted to select a version. Use your arrow keys to specify 3.10.1 and hit enter.\n\n⛔ BEWARE ⛔ this method will download a copy of the workflow with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust commands in the upcoming lessons accordingly.\n\n\n\n\n\n\nBuild your run command\nAll nf-core workflows are provided with sensible default settings that have broad applicability and comprensive documentation that explains all available parameters. In the case of the nf-core/rnaseq workflow, parameters are grouped based on various stages of the workflow:\n\nInput/output options for specifying which files to process and where to save results\nUMI options for processing reads with unique molecular identifiers (UMI)\nRead filtering options to be run prior to alignment\nReference genome options related to pre-processing of the reference FASTA\nRead trimming options prior to alignment\nAlignment options for read mapping and filtering criteria\nProcess skipping options for adjusting the processes to run with the workflow\n\nOn the command line you can view these options by running:\nnextflow run ../rnaseq/main.nf --help \nNotice at the bottom of the print out, there is:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nThree additional parameter sections are hidden from view. This is because they are less commonly used. They include:\n\nInstitutional config options for various compute environments\nMax job request options for limiting memory and cpu usage based on what is available to you\nGeneric options focused on how the pipeline is run\n\nTo view all the workflow run options on the command line, run:\nnextflow run ../rnaseq/main.nf --help --show_hidden_params\n\n\n\n\n\n\nHyphens matter!\n\n\n\nHyphens matter when it comes to parameter flags in nf-core workflows! Nextflow command-line parameters use one (-), whereas pipeline-specific parameters use two (–). For example: -profile is a Nextflow parameter, while –input is an nf-core parameter.\n\n\nWe will be using a number of flags from each parameters options section. In addition to the required parameters, we have chosen to use some additional flags that are suitable for our experiment. What is ‘suitable’ varies dramatically between different experiments, computing environments, and datasets, so these settings might not meet your needs for other experiments. Consider the most important design choices we have made for this workflow and structuring our run command:\n\nWe don’t need to run the pseudo alignment step (Stage 3)\nWe have chosen to use STAR to align reads\nWe have chosen to use Salmon to estimate transcript abundance\nWe only have access to 2 CPUs and 8Gb of RAM today\nWe have already provided the requisite reference data (fasta, gtf, indexes) for our training dataset\n\n\nFor the sake of expediency, we are using prepared subset data for this session. All the data (including fastqs, input manifest, reference fasta, gtf, and STAR indexes) are available on an external file system called CernVM-FS. CernVM-FS is a read-only file system that Pawsey have used to store files such as containerised tools (Biocontainers), reference datasets, and other shared resources that are commonly used by many researchers. Take a look here for more information on bioinformatics resources provided by Pawsey on Nimbus.\n\n\nRun the workflow\nWe need to store the path to our input and reference data in a variable for our run command:\nmaterials=/path/to/aarnet-cvmfs/training/workshopMaterials\nNow run the workflow:\nnextflow run rnaseq/main.nf \\\n    --input $materials/samplesheet.csv \\\n    -profile singularity \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --max_memory '6 GB' \\\n    --max_cpus 2 \n\n\n\n\n\n\nChallenge\n\n\n\nCan you use the nf-core/rnaseq documentation and the important considerations above to explain how we designed our run command for this experiment?\n💡 You will need to look at the reference genome, alignment, and max job request sections.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGiven we are using STAR and Salmon as our aligner and quantification tool of choice (respectively) and it is the default choice of this workflow we will not need to provide an --aligner flag. However, if you wanted to provide this for the sake of reproducibility in case things change in the future:\n--aligner 'star_salmon'\nGiven we are providing our own subset data for this workshop, we will need to use:\n--fasta /path/to/mouse.fa  \n--gtf /path/to/mouse.gtf \n--star_index /path/to/STAR\nGiven we have limited computing resources today, we will need to specify a ceiling for both memory and CPUs:\n--max_memory '6.GB' \n--max_cpus 2 \n\n\n\n\n\nExamine the outputs\nOnce your workflow has completed, you should see this message printed to your terminal:\n[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 11-Apr-2023 01:01:54\nDuration    : 17m 4s\nCPU hours   : 0.3\nSucceeded   : 200\nThe workflow ran successfully, however note the warning about all samples having failed the strandedness check. We’ll explore that in the next lesson. In the meantime, list (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu   4096 Apr 11 01:35 .\ndrwxr-x---  12 ubuntu ubuntu   4096 Apr 11 00:47 ..\ndrwxrwxr-x   4 ubuntu ubuntu   4096 Apr 11 01:01 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 392378 Apr 11 01:01 .nextflow.log\ndrwxrwxr-x   7 ubuntu ubuntu   4096 Apr 11 01:01 results\ndrwxrwxr-x  13 ubuntu ubuntu   4096 Apr  4 03:40 rnaseq\ndrwxrwxr-x 138 ubuntu ubuntu   4096 Apr 11 00:57 work\nNextflow has created 2 new output directories, work and results in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe results directory\nDepending on the nf-core workflow you are working with, all final outputs will be presented in a directory called results by default. You can override this default and output to a directory of your own choosing by using the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a workflow and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows are provided with sensible defaults. You can adjust some settings as required by applying flags to your run command.\nnf-core workflows are all built from a template that means they have a standard structure to their code bases\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_nfcore.html",
    "href": "notebooks/1.1_nfcore.html",
    "title": "nf-core workflows series",
    "section": "",
    "text": "Getting started with nf-core\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows with nf-core",
    "section": "",
    "text": "This course is currently under development\nThis workshop will set you up with the foundational knowledge required to run and customise nf-core workflows in a reproducible manner. Using the nf-core/rnaseq workflow as an example, we will step through essential features common across all nf-core workflows. We will explore ways to adjust the workflow parameters based on the needs of your dataset and configuration the workflow to run on your computational environment.\n\nTrainers\n\nCali Willet, Sydney Informatics Hub\nChris Hakkaart, Seqera Labs\nGeorgie Samaha, Sydney Informatics Hub\n\n\n\nTarget audience\nThis workshop is suitable for people who are familiar with working at the command line interface and have some experience running Nextflow and nf-core workflows.\n\n\nPrerequisites\n\nExperience navigating the Unix command line\nFamiliarity with Nextflow and nf-core workflows\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nCode of Conduct\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up your computer\nFollow these instructions to install VSCode and login to your Nimbus instance.\n\n\nDay 1: Introduction to nf-core\n\n\n\nDay 2: Customising nf-core\n\n\n\n\n\n\nCourse survey\nPlease fill out our course survey before you leave. Help us help you! 😁\n\n\nCredits and acknowledgements\nThis workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera Labs, Pawsey Supercomputing Research Centre, and Australia’s National Research Education Network (AARNet) enabled through the Australian BioCommons (NCRIS via Bioplatforms Australia).\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  }
]