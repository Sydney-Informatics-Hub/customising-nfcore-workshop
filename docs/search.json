[
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Nextflow tips and tricks",
    "section": "",
    "text": "Query specific pipeline executions\nnextflow log <run_name> -f  \n\n\nRun Nextflow in the background\nThe Nextflow config command is useful for understanding the resolved profiles and parameters that Nextflow will use run a pipeline.\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf -bg > workshop_tip.log\n\n\nCapture a Nextflow pipeline’s configuration\nnextflow config nf-core-rnaseq-3.11.1/workflow\n\n\nClean Nextflow cache and work directories\nExamples of dry run and remove features for clean\n\n\nAccess private GitHub repositories\nTo interact with private repositories on GitHub or when running into GitHub API rate limits for public repositories (i.e. when using Nextflow Tower), you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the scm configuration file inside your specified .nextflow/ directory.\nproviders {\n\n  github {\n    user = 'georgiesamaha'\n    password = 'my-personal-access-token'\n  }\n\n}\n\n\nSend processes to a job scheduler on HPC\nWrite a custom configuration file for your HPCs job scheduler: specify for all processes, withName {}, withLabel {}.\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "In this workshop series, we will be using Pawsey’s Nimbus cloud. The Pawsey Supercomputing Research Centre is one of two, Tier-1, High Performance Computing facilities in Australia.\nThe main requirements for this workshop are a personal computer with:\n\nA web broswer\nVisual Studio Code or a terminal application\n\nBelow, you will find instructions on how to set up a terminal application and web browser on your computer and how to connect to Nimbus. Each participant will be provided with their instance’s IP address at the beginning of the workshop.\n\nOption 1: Install and set up Visual Studio Code\nVisual Studio Code (VS Code) is a lightweight and powerful source code editor available for Windows, macOS and Linux computers.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nLinux\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\n\nInstall the Live Server extension. Click on the blue install button.\n\n\n\nInstall the Nextflow extension. Click on the blue install button.\n\n\n\nLogin via Visual Studio Code\n\nConnect to your instance with VS code by adding the host details to your ssh config file.\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Open SSH configuraiton file\nAdd new entry, filling out host name and identity file:\n\nHost nfcoreWorkshop\n  HostName 146.118.XX.XXX  \n  User training     \nConnect to this address\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\n\nOption 2: Install and set up a terminal application\nThe terminal applications available to you will depend on your operating system.\n\nLinux terminals\nIf you use Linux, chances are you already know your shell and how to use it. Basically, just open your preferred terminal program and off you go!\n\n\nOS X (Mac)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‘terminal’. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\n\n\nWindows\nWe recommend MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‘Home Edition’ select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‘start local terminal’ (and install Cygwin if prompted)\n\n\n\n\nLogin via Terminal\nTo log in to Nimbus, we will use a Secure Shell (SSH) connection. To connect, you need 3 things: 1. The assigned IP address of your instance (i.e. ###.###.##.###). Each participant will be provided with their instance’s IP address at the beginning of the workshop. 2. Your login name. In our case, this will be training for all participants. 3. Your password. All participants will be provided with a password at the beginning of the workshop.\nTo log in, type the following into your terminal, using your login name and the instance’s IP address:\nssh training@###.###.###.###\nYou will receive a message saying:\nThe authenticity of host 'XXX.XXX.XX.XXX (XXX.XXX.XX.XXX)' can't be established.\nRemember your host address will be different than the one above. There will then be a message saying:\nAre you sure you want to continue connecting (yes/no)?\nIf you would like to skip this message next time you log in, answer ‘yes’. It will then give a warning:\nWarning: Permanently added 'XXX.XXX.XX.XXX' (ECDSA) to the list of known hosts.\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you’ve forgotten it.\n\n\n\n\n\n\nPay Attention\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You’ll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
<<<<<<< HEAD
    "objectID": "notebooks/1.3_configure.html",
    "href": "notebooks/1.3_configure.html",
    "title": "Configuring nf-core workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the structure of an nf-core workflow.\nLearn how to customize the execution of an nf-core workflow.\nCustomize a toy example of an nf-core workflow.\n\n\n\n\n1.3.1. Workflow structure\nnf-core workflows follow a set of best practices and standardized conventions. nf-core workflows start from a common template and follow the same structure. Although you won’t need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\n\nNextflow DSL2 workflows are built up of subworkflows and modules that are stored as separate .nf files.\n\nMost nf-core workflows consist of a single workflow file (there are a few exceptions). This is the main <workflow>.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.\nA subworkflow is a groups of modules that are used in combination with each other and have a common purpose. For example, the SAMTOOLS_STATS, SAMTOOLS_IDXSTATS, and SAMTOOLS_FLAGSTAT modules are all included in the BAM_STATS_SAMTOOLS subworkflow. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. Within a nf-core workflow, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core workflow, an nf-core subworkflow is developed by the community is shared in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific that are not shared in the nf-core subworkflows repository.\nA module is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives, inputs, outputs, when statements, and a script block. Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are workflow specific that are not shared in the nf-core modules repository.\n\n\n1.3.2. Configuration\nEach nf-core workflow has its own configuration and parameter defaults. The default parameters are required for testing as the workflow is being developed and when it is pushed to GitHub. While the workflow configuration defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements.\nWhen a workflow is launched, Nextflow will look for configuration files in several locations. As each configuration file can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority:\n\nParameters specified on the command line (--parameter)\nParameters that are provided using the -params-file option\nConfig file that are provided using the -c option\nThe config file named nextflow.config in the current directory\nThe config file named nextflow.config in the workflow project directory\nThe config file $HOME/.nextflow/config\nValues defined within the workflow script itself (e.g., main.nf)\n\n\n\n\n\n\n\nWarning\n\n\n\nnf-core workflow parameters must be passed via the command line (--<parameter>) or Nextflow -params-file option. Custom config files, including those provided by the -c option, can be used to provide any configuration except for parameters.\n\n\nNotably, while some of these files are already included in the nf-core workflow repository (e.g., the nextflow.config file in the nf-core workflow repository), others are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file, and -c). Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a workflows execution.\n\n\n1.3.3. Viewing parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used.\n\n\n\n\n\nParameters and their descriptions can also be viewed in the command line using the run command with the --help parameter:\nnextflow run nf-core/<workflow> --help\n\n\n\n\n\n\nChallenge\n\n\n\nView the parameters for the Sydney-Informatics-Hub/nf-core-demo workflow using the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Sydney-Informatics-Hub/nf-core-demo workflow parameters can be printed using the run command and the --help option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --help\n\n\n\n\n\n1.3.4. Parameters in the command line\nAt the highest level, parameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--):\nnextflow nf-core/<workflow> --<parameter>\n\n\n\n\n\n\nTip\n\n\n\nNextflow options are prefixed with a single dash (-) and workflow parameters are prefixed with a double dash (--).\n\n\nDepending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag:\nnextflow nf-core/<workflow> --<parameter> string\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite animal using the multiqc_title parameter using a command line flag:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. Use the -resume option to save time:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --multiqc_title koala -resume\nIn this example, you can check your parameter has been applied by listing the files created in the results folder (my_results):\nls my_results/multiqc/\n--multiqc_title is a parameter that directly impacts a result file. For parameters that are not as obvious, you may need to check your log to ensure your changes have been applied. You can not rely on the changes to parameters printed to the command line when you execute your run:\nnextflow log\nnextflow log <run name> -f \"process,script\"\n\n\n\n\n\n1.3.5. Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file.\nThere are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by the workflow by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by the workflow by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by the workflow by default.\nModule-specific configuration options (both mandatory and optional).\n\ntest.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a small test dataset.\n\ntest_full.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a full-size test dataset.\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/<workflow> -profile <profile>\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/<workflow> -profile test,singularity\nnf-core workflows are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the workflows with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more convenient and more reproducible.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‘out of the box’. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the workflow is working as expected and is a great way to trial a workflow. Some workflows have multiple test profiles for you to test.\n\n\n\n\n1.3.6. Shared configuration files\nAn includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core workflows will fetch these configuration profiles from the nf-core config repository and make them available.\nFor shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help.\n\n\n\n\n\n\nTip\n\n\n\nPawsey has a shared profile that can be used to run workflows.\n\n\n\n\n1.3.7. Custom configuration files\nNextflow will also look for custom configuration files that are external to the workflow project directory. These files include:\n\nThe config file $HOME/.nextflow/config\nA config file named nextflow.config in your current directory\nCustom files specified using the command line\n\nA parameter file that is provided using the -params-file option\nA config file that are provided using the -c option\n\n\nYou don’t need to use all of these files to execute your workflow.\nParameter files\nParameter files are .json files that can contain an unlimited number of parameters:\n{\n   \"<parameter1_name>\": 1,\n   \"<parameter2_name>\": \"<string>\",\n   \"<parameter3_name>\": true\n}\nYou can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option.\nnextflow run nf-core/<workflow> -profile test,docker -param-file <path/to/params.json>\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite food using the multiqc_title parameter in a parameters file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .json file that contains your favourite food, e.g., cheese:\n{\n   \"multiqc_title\": \"cheese\"\n}\nInclude the custom .json file in your execution command with the -params-file option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -params-file my_custom_params.json\nCheck that it has been applied:\nls my_results/multiqc/\n\n\n\nConfiguration files\nConfiguration files are .config files that can contain various workflow properties. Custom paths passed in the command-line using the -c option:\nnextflow run nf-core/<workflow> -profile test,docker -c <path/to/custom.config>\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example:\nalpha.x  = 1\nalpha.y  = 'string value..'\nIs equivalent to:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite color using the multiqc_title parameter in a custom .config file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .config file that contains your favourite colour, e.g., blue:\nparams.multiqc_title = \"blue\"\nInclude the custom .config file in your execution command with the -c option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -c my_custom_config.config\nCheck that it has been applied:\nls my_results/multiqc/\nWhy did this fail?\nYou can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While it parameter is listed as a parameter on the STDOUT, it was not applied to the executed command:\nnextflow log\nnextflow log <run name> -f \"process,script\"\n\n\n\nThe process scope allows you to configure workflow processes and is used extensively to define resources and additional arguments for modules.\nBy default, process resources are allocated in the conf/base.config file using the withLabel selector:\nprocess {\n    withLabel: BIG_JOB {\n        cpus = 16\n        memory = 64.GB\n    }\n}\nSimilarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file:\nprocess {\n    withName: MYPROCESS {\n        cpus = 4\n        memory = 8.GB\n    }\n}\nWhile some tool arguments are included as a part of a module. To make modules sharable across workflows, most tool arguments are defined in the conf/modules.conf file in the workflow code under the ext.args entry.\nFor example, if you were trying to add arguments in the MULTIQC process in the Sydney-Informatics-Hub/nf-core-demo workflow, you could use the process scope:\nprocess {\n    withName : \".*:MULTIQC\" {\n        ext.args   = { \"<your custom parameter>\" }\n\n    }\nHowever, if a process is used multiple times in the same workflow, an extended execution path of the module may be required to make it more specific:\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"<your custom parameter>\"\n    }\n}\nThe extended execution path is built from the workflows, subworkflows, and modules used to execute the process.\nIn the example above, the nf-core MULTIQC module, was called by the DEMO workflow, which was called by the NFCORE_DEMO workflow in the main.nf file.\n\n\n\n\n\n\nTip\n\n\n\nIt can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the conf/modules.conf file. How arguments are added to a process can also vary. Be vigilant when you are modifying parameters.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a new .config file that uses the process scope to overwrite the args for the MULTIQC process. Change the args to your favourite month of the year, e.g, \"--title \\\"october\\\"\".\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMake a custom config file that uses the process scope to replace the args for the MULTIQC process:\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"--title \\\"october\\\"\"\n    }\n}\nExecute your run command again with the custom configuration file:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -c my_custom_config.config -resume\nCheck that it has been applied:\nls my_results/multiqc/\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nDemonstrate the configuration hierarchy using the Sydney-Informatics-Hub/nf-core-demo workflow by adding a params file (-params-file), and a command line flag (--multiqc_title) to your execution. You can use the files you have already created.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the .json file you created previously:\n{\n   \"multiqc_title\": \"cheese\"\n}\nExecute your command with your params file (-params-file) and a command line flag (--multiqc_title):\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -params-file my_custom_params.json --multiqc_title \"koala\"\nIn this example, as the command line is at the top of the hierarchy, the multiqc_title will be “koala”.\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows follow a similar structure.\nnf-core workflows are configured using multiple configuration sources.\nConfiguration sources are ranked to decide which settings to apply.\nWorkflow parameters must be passed via the command line (--<parameter>) or Nextflow -params-file option.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
=======
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
    "objectID": "notebooks/1.0_intro.html",
    "href": "notebooks/1.0_intro.html",
    "title": "Welcome to session 1",
    "section": "",
    "text": "During Session 1 we will establish fundamental ideas and skills that are essential for using Nextflow and customizing the execution of nf-core workflows.\nWe will start by discussing the core features Nextflow and learning the fundamental commands and options for executing workflows. Next, we will then learn the core features of nf-core and its tooling. This knowledge will be applied as learn the structure of an nf-core workflow and how customize its execution. Finally we will learn more about nf-core tooling for users and how it can help you execute your workflow using best practises.\nThe ideas and skills you learn as a part of Session 1 will be applied during Session 2 when you explore the source code of the nf-core/rnaseq workflow and customise its execution.\n\n1.0.1. Create a new work directory\nIt is good practice to organize projects into their own folders to make it easier to track and replicate experiments over time.\nStart by creating a new directory for all of today’s activities and move into it:\nmkdir ~/session1 && cd $_\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
<<<<<<< HEAD
    "objectID": "notebooks/2.4_multiConfig.html",
    "href": "notebooks/2.4_multiConfig.html",
    "title": "2.4. Using multiple configuration files at once",
    "section": "",
    "text": "Objectives\n\n\n\n\nCreate and apply a custom configuration file for resource tracing\nCreate and apply a custom configuration file for MultiQC\nAdd a custom configuration file details to the params YAML file\nApply multiple custom configuration files in workflow execution\nObserve the heirarchy of parameter configuration in action\n\n\n\nUsing multiple configuration files allows you to customise nf-core pipelines to your specific needs. Depending on how you work, where you work, and what pipelines you run, you may have configuration files that handle distinct issues, can be applied to specific infrastructures, or shared by multiple nf-core pipelines. In this lesson we will be applying multiple configuration files to the one run command.\n\n2.4.1. Apply multiple configurations\nYou may need to apply multiple configuration files at once to an nf-core pipeline run. A few scenarios in which you would need to do this include:\n\n\n\n\n\n\n\nScenario\nWhy?\n\n\n\n\nDebugging and testing\nIf you are toubleshooting a particular process in the workflow, you may want to create a separate config file with process-specific modifications.\n\n\nResource optimisation\nDepending on the scale and complexity of your data, some steps of the pipeline may require different settings or resources to achieve optimal performance. By using different config files with different parameters or settings, you can fine-tune the pipeline run.\n\n\nMultiQC configuration\nMultiQC allows users to specify configuration options in a YAML file called multiqc_config.yaml. This file allows users to customize the behavior and appearance of the MultiQC report.\n\n\n\n\n2.4.2. Customised resource tracing\nRemember we can use the Nextflow log command to show information about previous pipeline executions. We can customise which fields to print using the -fields flag. This flag accepts options used by the trace report.\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the trace report fields, write a nextflow log command to query the following for a previous workflow run:\n\nThe name of the task\nThe exit status of the task\nThe number of CPUs requested for task execution\nThe disk space requested for the task execution\nThe memory requested for the task execution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow log <runName> -f name,exit,cpus,disk,memory\n\n\n\nLet’s make a portable configuration file for resource tracing called trace-benchmark.config. We can apply this config when we want to collect resource benchmarks. Because this config is not infrastructure- or pipeline-specific, we can share it with collaborators and use it across multiple pipelines:\n// Define timestamp, to avoid overwriting existing trace \ndef trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate custom resource trace file \ntrace {\n  enabled = true \n  file = \"workshop-resource-trace-${trace_timestamp}.txt\"\n  fields = 'name,status,realtime,cpus,%cpu,memory,%mem,rss'\n}\nRerun the workflow, adding trace-benchmark.config to the config flag and observe the extra file that is saved to our session2 directory:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config,trace-benchmark.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-4 \\\n  -resume\nTake a look at the launch log printed to the screen. You can see your profile, custom config, and parameters have all been applied successfully! Once the workflow has run, take a look at the workshop-resource-trace-${trace_timestamp}.txt file:\nname    status  realtime        cpus    %cpu    memory  %mem    rss\nNFCORE_RNASEQ:RNASEQ:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet.csv)    CACHED  1s      1       28.0%   6 GB    0.2%    16.1 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (mm10_chr18.gtf)    CACHED  1s      2       51.3%   6 GB    0.3%    23.8 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (mm10_chr18.fa)        CACHED  1s      1       34.1%   6 GB    0.0%    4.7 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_GENE_FILTER (mm10_chr18.fa)     CACHED  1s      1       34.7%   6 GB    0.1%    13.9 MB\nNFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:FQ_SUBSAMPLE (SRR3473984)        CACHED  2s      1       43.7%   6 GB    0.0%    6.5 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:MAKE_TRANSCRIPTS_FASTA (rsem/mm10_chr18.fa) CACHED  4s      2       42.0%   6 GB    1.2%    94.4 MB\nNFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:FQ_SUBSAMPLE (SRR3473985)        CACHED  2s      1       44.1%   6 GB    0.0%    6.3 MB\n\n\n\n\n\n\n☠️ Don’t confuse double and single quotes! ☠️\n\n\n\nDouble quotes (\"\") allow for variable interpolation, which means that variables can be evaluated and their values are substituted within the string.\nSingle quotes ('') denote a string literally, which means that the string is treated as-is without any variable interpolation.\nTry swapping the double quoted file value in your custom config above for a single-quoted value, rerun the workflow and note the creation of a\nfile = 'workshop-resource-trace-${trace_timestamp}.txt'\n\n\n\n\n\n2.4.2. Configure MultiQC reports\nMany nf-core pipelines use MultiQC to generate a summary report at the end of a workflow. MultiQC is a reporting tool that can aggregate results and statistics output by various bioinformatics tools. It helps to summarise experiments containing multiple samples and multiple analysis steps. In nf-core pipelines, MultiQC is used to summarise statistics and results output by processes in the workflow and its reports can be quite verbose. MultiQC can be configured using custom YAML files to customise the outputs of reports.\nWe are going to configure MultiQC to aid interpretation of the FastQC output it plots. Create a custom MultiQC configuration YAML file called multiqc-config.yaml and add the following:\nfastqc_config:\n  fastqc_theoretical_gc: \"mm10_txome\" \nHere, we’re plotting the theoretical GC content for the mouse reference transcript.\nTo the workshop-params.yaml file, add:\nmultiqc_config: \"multiqc-config.yaml\" \nMake sure both YAML files are saved, then re-run the workflow:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-4 \\\n  -resume\nThe logging information printed to the screen shows us again that the params.yaml file with the output directory set to Lesson-2 has been overridden by the use of the flag --outdir Lesson-4 in the run command. As the workflow runs, notice all the completed processes that are pulled from the cache and not rerun. Our use of the multiqc-config.yaml file adjusted the MultiQC process, as such this process was repeated using the updated settings.\nThe changes we made above added the normal mouse transcriptome GC profile as a track to the fastQC per-sequence GC content plot. Take a look at the changes, open the Exercise4 multiqc_report.html file with Live Server as per previosuly (or use scp to take a copy to your local computer if you are not on VS Code). Compare this report with the MultiQC report from lesson 1 by looking at the section FastQC: Per Sequence GC Content. Compare the two plots to observe the custom track has been successfully added.\n\n\n\n\n\n\n\nChallenge\n\n\n\nGiven the normal mouse transcriptome GC profile indicated by the black dotted line, can you detect any GC bias in the input sequence data?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, all samples’ GC profiles follow a normal distribution.\n\n\n\nNote that if we did detect GC bias, we could go back and correct for this by adding the custom salmon flag --gcBias to the nf-core parameter --extra_salmon_quant_args. If we were to do this, we would have to add the following line to our workflow-params.yaml file:\nextra_salmon_quant_args : '--writeUnmappedNames --gcBias'\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
=======
    "objectID": "notebooks/1.1_nextflow.html",
    "href": "notebooks/1.1_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of Nextflow.\nLearn Nextflow terminology.\nLearn fundamental commands and options for executing workflows.\n\n\n\n\n1.1.1. What is Nextflow?\n\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nIt is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.\nNextflow’s core features are:\n\nWorkflow portability and reproducibility\nScalability of parallelization and deployment\nIntegration of existing tools, systems, and industry standards\n\nWhether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your workflow and improve your productivity.\n\n\n1.1.2. Processes and Channels\nIn Nextflow, processes and channels are the fundamental building blocks of a workflow.\n\nA process is a unit of execution that represents a single computational step in a workflow. It is defined as a block of code that typically performs a one specific task and specifies its input and outputs, as well as any directives and conditional statements required for its execution. Processes can be written in any language that can be executed from the command line, such as Bash, Python, or R.\nProcesses in are executed independently (i.e., they do not share a common writable state) as tasks and can run in parallel, allowing for efficient utilization of computing resources. Nextflow automatically manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.\nA channel is an asynchronous first-in, first-out (FIFO) queue that is used to join processes together. Channels allow data to passed between processes and can be used to manage data, parallelize tasks, and structure workflows. Any process can define one or more channels as an input and output. Ultimately the workflow execution flow itself, is implicitly defined by these declarations.\nImportantly, processes can be parameterized to allow for flexibility in their behavior and to enable their reuse in and between workflows.\n\n\n1.1.3. Execution abstraction\nWhile a process defines what command or script is executed, the executor determines how and where the script is executed.\nNextflow provides an abstraction between the workflow’s functional logic and the underlying execution system. This abstraction allows users to define a workflow once and execute it on different computing platforms without having to modify the workflow definition. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments.\n\nIf not specified, processes are executed on your local computer. The local executor is useful for workflow development and testing purposes. However, for real-world computational workflows, a high-performance computing (HPC) or cloud platform is often required.\nYou can find a full list of supported executors as well as how to configure them here.\n\n\n1.1.4. Nextflow CLI\nNextflow implements a declarative domain-specific language (DSL) that simplifies the writing of complex data analysis workflows as an extension of a general-purpose programming language. As a concise DSL, Nextflow handles recurrent use cases while having the flexibility and power to handle corner cases.\nNextflow is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as “Python for Java” and simplifies the code.\nNextflow provides a robust command line interface for the management and execution of workflows. Nextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and Java 11 (or later, up to 18) to be installed.\nNextflow is distributed as a self-installing package and does not require any special installation procedure.\n\n\n\n\n\n\nHow to install Nextflow\n\n\n\nNextflow can be installed using a few easy steps:\n\nDownload the executable package using either wget -qO- https://get.nextflow.io | bash or curl -s https://get.nextflow.io | bash\nMake the binary executable on your system by running chmod +x nextflow.\nMove the nextflow file to a directory accessible by your $PATH variable, e.g, mv nextflow ~/bin/\n\n\n\n\n\n1.1.5.Nextflow options and commands\nNextflow provides a robust command line interface for the management and execution of workflows. The top-level interface consists of options and commands.\nYou can list Nextflow options and commands with the -h option:\nnextflow -h\n\nOptions for a commands can also be viewed by appending the -help option to a Nextflow command.\nFor example, options for the the run command can be viewed:\nnextflow run -help\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind out which version of Nextflow you are using with the version option.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe version of Nextflow you are using can be printed using the -v option:\nnextflow -version\nOr:\nnextflow -v\n\n\n\n\n\n\n1.1.6. Managing your environment\nYou can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a workflow and will be interpreted by Nextflow. For most users, Nextflow will work without setting any environment variables. However, to improve reproducibility and to optimise your resources, you will benefit from establishing environmental variables.\nFor example, for consistency, it is good practice to pin the version of Nextflow you are using with the NXF_VER variable:\nexport NXF_VER=&lt;version number&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nPin the version of Nextflow to 22.04.5 using the NXF_VER environmental variable and check that it has been applied.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExport the version using the NXF_VER environmental variable:\nexport NXF_VER=22.04.5\nCheck that the new version has been applied using the -v option:\nnextflow -v\n\n\n\n\nSimilarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR variables:\nexport NXF_CONDA_CACHEDIR=&lt;custom/path/to/conda/cache&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a new folder with the path /home/ubuntu/singularity_cache to store your singularity images and export its location using the NXF_SINGULARITY_CACHEDIR environmental variable:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMake a new folder for your singularity images:\nmkdir /home/ubuntu/singularity_cache\nExport your new folder as your cache directory for singularity images using the NXF_SINGULARITY_CACHEDIR environmental variable:\nexport NXF_SINGULARITY_CACHEDIR=/home/ubuntu/singularity_cache\nSingularity images downloaded by workflow executions will now be stored in this directory.\n\n\n\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session.\nA complete list of environmental variables can be found here.\n\n\n1.1.7. Executing a workflow\nNextflow seamlessly integrates with code repositories such as GitHub. This feature allows you to manage your project code and use public Nextflow workflows quickly, consistently, and transparently.\nThe Nextflow pull command will download a workflow from a hosting platform into your global cache $HOME/.nextflow/assets folder.\nIf you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL. The qualified name is formed by two parts - the owner name and the repository name separated by a / character. For example, if a Nextflow project bar is hosted in a GitHub repository foo at the address http://github.com/foo/bar, it could be pulled using:\nnextflow pull foo/bar\nOr by using the complete URL:\nnextflow pull http://github.com/foo/bar\nAlternatively, the Nextflow clone command can be used to download a workflow into a local directory of your choice:\nnextflow clone foo/bar &lt;your/path&gt;\nThe Nextflow run command is used to initiate the execution of a workflow :\nnextflow run foo/bar\nIf you run a workflow, it will look for a local file with the workflow name you’ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically pull the workflow to your global cache and execute it.\nBe aware of what is already in your current working directory where you launch your workflow, if there are other workflows (or configuration files) you may encounter unexpected results.\n\n\n\n\n\n\nChallenge\n\n\n\nExecute the hello workflow directly from nextflow-io GitHub repository.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the run command to execute the nextflow-io/hello workflow:\nnextflow run nextflow-io/hello\n\n\n\n\nMore information about the Nextflow run command can be found here.\n\n\n1.1.8. Executing a revision\nWhen a Nextflow workflow is created or updated using GitHub (or another code repository), a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the workflow and to ensure that the same version of the workflow is used consistently across different runs.\nThe Nextflow info command can be used to view workflow properties, such as the project name, repository, local path, main script, and revisions. The * indicates which revision of the workflow you have stickied and will be executed when using the run command.\nnextflow info &lt;workflow&gt;\nIt is recommended that you use the revision flag every time you execute a workflow to ensure that the version is correct. To use a specific revision, you simply need to add it to the command line with the --revision or -r flag. For example, to run a workflow with the v1.0 revision, you would use the following command:\nnextflow run &lt;workflow&gt; -r v1.0\nNextflow automatically provides built-in support for version control using Git. With this, users can easily manage and track changes made to a workflow over time. A revision can be a git branch, tag or commit SHA number, and can be used interchangeably.\n\n\n\n\n\n\nChallenge\n\n\n\nExecute the hello workflow directly from the nextflow-io GitHub using the v1.1 revision tag.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the nextflow-io/hello workflow with the revision -r option:\nnextflow run nextflow-io/hello -r v1.1\n\nThe warning shown above is expected as the v1.1 workflow revision was written using an older version of Nextflow that uses the depreciated echo method. As both Nextflow and workflows are updated independently over time, workflows and Nextflow functions can get out of sync. While most nf-core workflows are now dsl2 (the current way of writing workflows), some are still written in dsl1 and may require older version of Nextflow to run.\n\n\n\nIf your local version of a workflow is not the latest you be shown a warning and will be required to use a revision flag when executing the workflow. You can update a workflow with the Nextflow pull command with a revision flag.\n\n\n1.1.9. Nextflow log\nIt is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command:\nnextflow log\nThe log command has multiple options to facilitate the queries and is especially useful while debugging a workflow and inspecting execution metadata. You can view all of the possible log options with -h flag:\nnextflow log -h\nTo query a specific execution you can use the RUN NAME or a SESSION ID:\nnextflow log &lt;run name&gt;\nTo get more information, you can use the -f option with named fields. For example:\nnextflow log &lt;run name&gt; -f process,hash,duration\nThere are many other fields you can query. You can view a full list of fields with the -l option:\nnextflow log -l\n\n\n\n\n\n\nChallenge\n\n\n\nUse the log command to view with process, hash, and script fields for your tasks from your most recent Nextflow execution.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the log command to get a list of you recent executions:\nnextflow log\n\nQuery the process, hash, and script using the -f option for the most recent run:\nnextflow log crazy_faggin -f process,hash,script\n\n\n\n\n\n\n1.1.10. Execution cache and resume\nTask execution caching is an essential feature of modern workflow managers. As such, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.\nNextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID’s are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.\nA multi-step workflow is required to demonstrate cache and resume. The Sydney-Informatics-Hub/nf-core-demo workflow was created with the nf-core create command and has the same structure as nf-core workflows. It is a toy example with 3 processes:\n\nSAMPLESHEET_CHECK\n\nExecutes a custom python script to check the input sample sheet is valid.\n\nFASTQC\n\nExecutes FastQC using the .fastq.gz files from the sample sheet as inputs.\n\nMULTIQC\n\nExecutes MultiQC using the FastQC reports generated by the FASTQC process.\n\n\nThe Sydney-Informatics-Hub/nf-core-demo is a very small nf-core workflow. It uses real data and bioinformatics software and requires additional configuration to run successfully. To run this example you will need to include two profiles in your execution command. Profiles are sets of configuration options that can be accessed by Nextflow. Profiles will be explained in greater detail during the Configuring nf-core workflows section of the workshop.\nTo run this workflow, both the test profile and a software management profile (such as singularity) are required:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity\nThe command line output will print something like this:\n\nExecuting this workflow will create a work directory and a my_results directory with selected results files.\nIn the schematic above, the hexadecimal numbers, such as ff/21abfa, identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed. You can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path:\nls -la work/ff/21abfa87cc7cdec037ce4f36807d32/\nThe files that have been selected for publication in the my_results folder can also be explored:\nls my_results\nIf you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed:\n\nThe FASTQC process runs four times, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [1a/3c54ed] represents just one of the four sets of input data that was processed. To print all the relevant paths to the screen, use the -ansi-log option can be used when executing your workflow:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -ansi-log false\nIt’s very likely you will execute a workflow multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) by resuming a workflow from the last step that was completed successfully and/or unmodified. By adding the -resume option to your run command you can use the cache rather than re-running successful tasks:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -resume\nIf you run the Sydney-Informatics-Hub/nf-core-demo workflow again without making any changes you will see that the cache is used:\n\nIn practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results.\nNotably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.\n\n\n\n\n\n\nChallenge\n\n\n\nInvalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the workflow again with the -resume option to show that the cache has been invalidated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the workflow for the first time (if you have not already).\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity\nUse the task ID shown for the FASTQC process and use it to find and touch a the sample1_R1.fastq.gz file:\ntouch work/ff/21abfa87cc7cdec037ce4f36807d32/sample1_R1.fastq.gz\nExecute the workflow again with the -resume command option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -resume\nYou should that 2 of 4 tasks for FASTQC and the MULTIQC task were invalid and were executed again.\nWhy did this happen?\nIn this example, the cache of two FASTQC tasks were invalid. The sample1_R1.fastq.gz file is used by in the samplesheet twice. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions.\n\n\n\nYour work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it’s contents, the Nextflow clean function allows you to selectively remove data associated with specific runs.\nnextflow clean -help\n\nThe -after, -before, and -but options are all very useful to select specific runs to clean. The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command.\n\n\n\n\n\n\nChallenge\n\n\n\nClean your work work directory of staged files but keep your execution logs.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextflow clean command with the -k and -f options:\nnextflow clean -k -f\n\n\n\n\n\n1.1.11. Listing and dropping cached workflows\nOver time, you might want to remove a stored workflows. Nextflow also has functionality to help you to view and remove workflows that have been pulled locally. The Nextflow list command prints the projects stored in your global cache folder ($HOME/.nextflow/assets). These are the workflows that were pulled when you executed either of the Nextflow pull or run commands:\nnextflow list\nIf you want to remove a workflow from your cache you can remove it using the Nextflow drop command:\nnextflow drop &lt;workflow&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nView your cached workflows with the Nextflow list command and remove the nextflow-io/hello workflow with the drop command.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nList your workflow assets:\nnextflow list\nDrop the nextflow-io/hello workflow:\nnextflow drop nextflow-io/hello\nCheck it has been removed:\nnextflow list\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nEnvironment variables can be used to control your Nextflow runtime and the underlying Java virtual machine.\nNextflow supports version control and has automatic integrations with online code repositories.\nNextflow will cache your runs and they can be resumed with the -resume option.\nYou can manage workflows with Nextflow commands (e.g., pull, clone, list, and drop).\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.4_users.html",
    "href": "notebooks/1.4_users.html",
    "title": "nf-core for users",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn more about nf-core tooling for users.\nUse nf-core list to view information about nf-core workflows.\nUse nf-core download to download a workflow and it’s singularity images.\nUse nf-core launch to create a parameters file.\n\n\n\n\n1.4.1 nf-core tools for users\nnf-core tools has additional commands to help users execute workflows. Although you do not need to use these commands to execute the nf-core workflows, they can greatly assist and improve and simplify your experience.\nThere are also nf-core tools for developers. However, these will not be covered as a part of this workshop. If you are curious to learn more about these tools you can find more information on the nf-core websites tools page. There are also lots of excellent ByteSize talks on the nf-core YouTube channel.\n\n\n1.4.2 nf-core list\nThe nf-core list command can be used to print a list of remote nf-core workflows along with your local workflow information.\nnf-core list\n\nThe output shows the latest workflow version number and when it was released. You will also be shown if and when a workflow was pulled locally and whether you have the latest version. Keywords can also be supplied to help filter the workflows based on matches in titles, descriptions, or topics:\nnf-core list dna\n\nOptions can also be used to sort the workflows by latest release (-s release, default), when you last pulled a workflow locally (-s pulled), alphabetically (-s name), or number by the number of GitHub stars (-s stars).\n\n\n\n\n\n\nChallenge\n\n\n\nFilter the list of nf-core workflows for those that are for rna and sort them by stars. Which rna workflow has the most stars?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the list command, filter it for rna, and sort by stars:\nnf-core list rna -s stars\n\n\n\n\n\n1.4.2 nf-core download\nSometimes you may need to execute an nf-core workflow on a server or HPC system that has no internet connection. In this case, you will need to fetch the workflow files and manually transfer them to your offline system. To make this process easier and ensure accurate retrieval of correctly versioned code and software containers, nf-core has the download command.\nThe nf-core download command will download both the workflow code and the institutional nf-core/configs files. It can also optionally download singularity image file.\nnf-core download\nIf run without any arguments, the download tool will interactively prompt you for the required information. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:\n\nPipeline name\n\nName of workflow you would like to download.\n\nPipeline revision\n\nThe revision you would like to download.\n\nPull containers\n\nIf you would like to download Singularity images.\nThe path to a folder where you would like to store these images if you have not set your NXF_SINGULARITY_CACHEDIR.\n\nChoose compression type\n\nThe compression type for Singularity images.\n\n\nAlternatively, you could build your own execution command with the command line options.\n\n\n\n\n\n\n\nChallenge\n\n\n\nUse the nf-core download command to download revision 3.11.1 of the nf-core/rnaseq workflow with it’s uncompressed Singularity images. You can use either the command line options or prompts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core download command for the nf-core/rnaseq workflow and follow the prompts.\nIf you were to write your own command and options it would look like this:\nnf-core download nf-core/rnaseq --revision 3.11.1 --container singularity --compress none\nLet the singularity images download (you will use the during Session 2).\n\n\n\n\n\n1.4.3 nf-core launch\nA workflow can have a large number of optional parameters. To help with this, the nf-core launch command is designed to help you write parameter files for when you launch your workflow.\nThe nf-core launch command takes one argument - either the name of an nf-core workflow which will be pulled automatically or the path to a directory containing a Nextflow workflow:\nnf-core launch nf-core/&lt;workflow&gt;\nWhen running this command, you will first be asked about which version of the workflow you would like to execute. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the workflow parameters. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.\n\nThe nf-core launch tool uses the nextflow_schema.json file from a workflow to give parameter descriptions, defaults, and grouping. If no file for the workflow is found, one will be automatically generated at runtime.\nThe launch tool will save your parameter variables as a .json file called nf-params.json. It will also suggest an execution command that includes the -params-file flag and your new nf-params.json file. The command line wizard will finish by asking if you want to launch the workflow. Any profiles or options that were set using the wizard will be included in your run command.\n\n\n\n\n\n\nChallenge\n\n\n\nGenerate a schema and run command for revision 3.11.1 of the nf-core/rnaseq workflow using the nf-core launch command. Use the test and singularity profiles and name your output folder my_test_output.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core launch command for the nf-core/rnaseq workflow and follow the prompts:\nYour final run command should look like this:\nnextflow run nf-core/rnaseq -r 3.11.1 -profile test,singularity -params-file nf-params.json\nYour nf-params.json file should look like this:\n{\n    \"outdir\": \"my_test_output\"\n}\n\n\n\nYou can also use the launch command directly from the nf-core launch website. In this case, you can configure your workflow using the wizard and then copy the outputs to your terminal or use the run id generated by the wizard. You will need to be connected to the internet to use the run id.\nnf-core launch --id &lt;run_id&gt;\n\n\n\n\n\n\nKey points\n\n\n\n\nThe nf-core list command can be used to view local and remote information about nf-core workflows.\nThe nf-core download command is a powerful way to download a workflow and its Singularity images.\nThe nf-core launch command can be a useful tool for writing parameter files.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "2.1. Design a run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand the levels of customisation available for nf-core pipelines\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore pipeline deployment and outputs\n\n\n\nThe nf-core community values flexibility and has built this into their pipelines by allowing users to customise various aspects of a workflow run using parameters. While nf-core pipelines are designed to run with ‘sensible’ default settings, these may not always suit the needs of your experiment of compute environment. In this lesson, we will design and execute a customised run command using different types of parameters.\n\n2.1.1. Download the pipeline code\nCreate a new directory for all session 2 activities and move into it:\nmkdir ~/session2 && cd $_\nWe recommend that you keep a local copy of a pipeline’s code for the sake of reproducibility and good record keeping. There are a number of ways to download a nf-core pipeline. Using the nf-core tools download tool, take a look at the download options available to us:\nnf-core download --help\n\nRun the following command:\nnf-core download rnaseq \\\n  --revision 3.11.1 \\\n  --outdir ~/session2/nf-core-rnaseq-3.11.1 \\\n  --container singularity \\\n  --compress none \\\n  --singularity-cache-only\nNote that we’ve chosen to download a specific pipeline version by applying the --revision flag. If you would like to download the most recent version of an nf-core pipeline, you can exclude this flag.\nWe have also chosen to store all containers in a specified Singularity cache directory and not store an additional copy of these in the output directory. You will be prompted to define an existing directory as your Singularity cache. Specify:\nNXF_SINGULARITY_CACHEDIR=/home/training/singularity_cache\n\n\n\n\n\n\nNextflow has environmental variables!\n\n\n\n$NXF_SINGULARITY_CACHEDIR is a Nextflow environmental variable that can be used to override default Nextflow and pipeline behaviours. You can set these environmental variables on the command line or by using env scopes when writing Nextflow.\n\n\nThe nf-core tools utility will download the pipeline files and institutional configuration files available at the nf-core/configs repository. Inside your nf-core-rnaseq-3.11.1 workflow directory, you should see 2 subdirectories:\nls -l nf-core-rnaseq-3.11.1\ntotal 8\ndrwxrwxr-x  7 ubuntu ubuntu 4096 Apr 21 02:08 configs\ndrwxrwxr-x 12 ubuntu ubuntu 4096 Apr 21 02:08 workflow\nThe public institutional configs were downloaded to the configs directory. The code base for our pipeline will be stored in the workflow directory:\nls -l nf-core-rnaseq-3.11.1/workflow\nThe files and directories we will be working with in this session are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq pipeline\n\n\n\n\n\n\n\n\n\nGit installation method\n\n\n\n\n\nIn situations where you might not wish to use the nf-core tools utility, download the nf-core/rnaseq source code from it’s GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\n⛔ BEWARE ⛔ this method will download a copy of the pipeline with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust some paths specified in the upcoming lessons accordingly.\n\n\n\n\n\n2.1.2. Design your run command\nAll nf-core pipelines are provided with comprehensive documentation that explains pipeline usage and available parameters. All nf-core pipelines can be run using a single command but the types and number of parameters will vary between pipelines. Generally, pipelines can be customised at a few different levels:\n\n\n\n\n\n\n\nLevel of effect\nCustomisation feature\n\n\n\n\nThe workflow\nWhere diverging workflows are available for a pipeline, you may choose a path to follow\n\n\nA process\nWhere more than one tool is available for a single process, you may choose which to use\n\n\nA tool\nApply specific thresholds or optional flags for a tool on top of the default run command\n\n\nCompute resources\nSpecify resource thresholds or software execution methods for the workflow or a process\n\n\n\nLooking at the nf-core/rnaseq pipeline structure below, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided multiple workflow options and specified which is the default\nProvided multiple tool options for some processes\n\n\nThe number and type of default and optional parameters an nf-core pipeline accepts is at the discretion of it’s developers. However at a minimum, pipelines typically:\n\nRequire users to specify a sample sheet (--input) detailing sample data and relevant metadata\nAutogenerate or acquire missing reference files from iGenomes (--genome) if not provided by the user.\n\nYou can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline by running:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help \nThe typical or recommended run command for this pipeline is provided at the top of the screen.\nnextflow run nf-core/rnaseq --input samplesheet.csv --outdir &lt;OUTDIR&gt; --genome GRCh37 -profile docker\nIt outlines a requirement for a few basic things:\n\nAn input file\nA location to store outputs\nRelevant reference data\nA software management method\n\n\n\n\n\n\n\nHyphens matter in Nextflow!\n\n\n\nNextflow-specific parameters use one (-) hyphen, whereas pipeline-specific parameters use two (--). In the typical run command above -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\nMost of us will need to adjust the command a little more for our experiments though. Today we’ll be adjusting the typical nf-core/rnaseq run command by:\n\nProviding our own reference files\nUsing the Singularity profile, instead of Docker\nCustomising some processes\nSpecifying the computing resource limitations of our instances\n\nYou can see how we’ve customised the typical run command in the diagram below:\n\n\n\n2.1.3. Run the pipeline\nOur input fastq files, reference data, and sample sheet are already available on an external file system called CernVM-FS that we can access from our Nimbus instances. Take a look at the files:\nls -l /cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Feb 14 05:36 fastqs\ndrwxrwxr-x 3 ubuntu ubuntu 4096 Feb 14 05:46 mm10_reference\n-rw-rw-r-- 1 ubuntu ubuntu  641 Feb 16 05:57 samplesheet.csv\nCopy the following into a file called samplesheet.csv:\nsample,fastq_1,fastq_2,strandedness\nSRR3473989,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473989_selected.fastq.gz,,forward\nSRR3473988,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473988_selected.fastq.gz,,forward\nSRR3473987,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473987_selected.fastq.gz,,forward\nSRR3473985,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473985_selected.fastq.gz,,forward\nSRR3473986,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473986_selected.fastq.gz,,forward\nSRR3473984,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473984_selected.fastq.gz,,forward\nStore the path to our prepared data to in a variable for our run command:\nmaterials=/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523\nNow run the pipeline:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n    --input samplesheet.csv \\\n    --outdir Lesson-2.1 \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --salmon_index $materials/mm10_reference/salmon-index \\\n    -profile singularity \\\n    --skip_markduplicates \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\nTake a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\nAs the workflow starts, you will also see a number of processes spawn out underneath this. Recall from session 1 that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. Take a look at the image below which explains a workflow’s process status output provided by Nextflow using the data dependencies for the STAR_ALIGN process as an example.\n\n\n\n2.1.4. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 14m 18s\nCPU hours   : 0.4\nSucceeded   : 170\nThe pipeline ran successfully, however note the warning about all samples having failed the strandedness check. We’ll explore that in the next lesson. In the meantime, list (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:44 .\ndrwxr-x---  15 ubuntu ubuntu 4.0K Apr 21 01:56 ..\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 03:58 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 371K Apr 21 03:58 .nextflow.log\n-rw-rw-r--   1 ubuntu ubuntu  17K Apr 21 03:50 .nextflow.log.1\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:58 Lesson-2.1\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 02:08 nf-core-rnaseq-3.11.1\n-rw-rw-r--   1 ubuntu ubuntu  563 Apr 21 03:14 samplesheet.csv\ndrwxrwxr-x 143 ubuntu ubuntu 4.0K Apr 21 03:58 work\nNextflow has created 2 new output directories, work and Lesson-2.1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe Lesson-2.1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible default settings and required inputs.\nCustomising a pipeline’s run command can be achieved by applying parameters.\nParameters can be used to customise the workflow, some processes, some tools, and compute resources.\nThe number of hyphens indicate if a parameter flag is specific to Nextflow or the pipeline.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "2.0. Introduction to session 2",
    "section": "",
    "text": "This session builds on fundamental concepts learned in Session 1 and provide you with hands-on experience in nf-core workflow customisation. Throughout the session we will be working with a case study to apply an nf-core workflow. Each lesson will build on the previous one, so you can gain a deeper understanding of the customisation techniques and the impact they have on the workflow and your results.\nWe will explore the source code of the nf-core/rnaseq workflow and apply customisations using a parameter file and custom configuration files. You will create these files and run Nextflow commands and some custom bash code to efficiently extract information from the source code 🤓.\n\n\n\n\n\n\nApplying what you learn here to other nf-core workflows\n\n\n\nWhile all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to other nf-core workflows and do not require an understanding of RNAseq data processing.\n\n\n\n2.0.1. Log back in to your instance\n\nOption 1: In Visual Studio Code\nFollow the VS Code set up instructions or use the instructions below to connect to your instance using the command palatte:\n\nCtrl+Shift+P to open command palette\nSelect Remote-SSH: Connect to Host and select name of your host\nSelect Linux from dropdown menu and then continue\n\nHaving successfully logged in, you should see a small green box in the bottom left corner of your screen:\n\n\n\nOption 2: In a terminal\nFollow the terminal set up instructions or use the quick start instructions below to connect to your instance on the command-line:\n\nRun: ssh training@###.###.###.###\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you’ve forgotten it.\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
  },
  {
    "objectID": "notebooks/2.3_configEnv.html",
    "href": "notebooks/2.3_configEnv.html",
    "title": "2.3. Configuring a run for your environment",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand formatting requirements of a config file\nWrite a custom config file for your local environment that overwrites default workflow settings\nRun a workflow using the custom config file and appropriate Nextflow flag\nUse an alternative container source for a workflow process\n\n\n\n\nNextflow’s portability is enabled by its ability to separate workflow implementation from the configuration settings required to execute it. In this lesson we will use configuration files to define specifications required to execute an nf-core pipeline on our compute environment. While nf-core workflows are designed to be portable and work out of the box, sometimes you will need to customise the workflow’s configuration so that it can run on your environment. The nf-core developer community currently offer a number of ways to configure nf-core workflows.\n\n2.3.1. Default nf-core configuration\nRecall that when a main.nf file is run for any Nextflow workflow, Nextflow looks for configuration files in multiple locations to determine how to execute the workflow and its processes. One of the files Nextflow will always look for is nextflow.config. Currently, all nf-core workflows use a nextflow.config file and a conf/base.config file to define the default execution settings and parameters of a workflow.\nLet’s take a look at the nf-core/rnaseq nextflow.config file:\ncat nf-core-rnaseq-3.11.1/workflow/nextflow.config\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat is the default aligner parameter being applied?\nWhat default max memory, cpu, and walltime resources have been specified?\nWhat config file is loaded by default for all nf-core workflows?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nInside the Global default params {} section, on line 58 of the nextflow.config under // Alignment:\n\naligner = 'star_salmon'\n\nInside the Global default params {} section, on lines 120-124 of the nextflow.config under // Max resource options:\n\nmax_memory = '128.GB'\nmax_cpus   = 16\nmax_time   = '240.h'\n\nInside Global default params {} section, on line 128 of the nextflow.config under // Load base.config by default for all pipelines:\n\nincludeConfig 'conf/base.config'\n\n\n\n\n\n2.3.2. When to use a custom config file\nThere are a number of situations in which you may want to write a custom configuration file:\n\nTo override the default resource allocations of the workflow specified in the nextflow.config\nTo override the default resource allocations for a process specified in conf/base.config\nTo use a different software installation method than those supported by nf-core\nTo run a workflow on an HPC and interact with a job scheduler like PBSpro or SLURM\n\nUsing a custom configuration file is good practice to ensure that your pipeline runs efficiently and reproducibly on your compute environment. It also allows you to easily share the pipeline with others who can use your custom config file to run it in the same computational environment.\nWe will write a custom configuration file to override the default configurations of the workflow with those that are suitable for our Nimbus instances. We’re going to replace 3 flags in our run command with this file:\n\n-profile singularity\n--max_memory 6.GB\n--max_cpus 2\n\n\n\n\n\n\n\nWhy should I be concerned with computational efficiency? 🌏\n\n\n\nBioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices. This paper makes for an interesting read about the carbon footprint of bioinformatics workflows and tools!\n\n\n\n\n2.3.3. Customise resource configuration\nOpen a new file called custom-nimbus.config and start writing some Nextflow code by adding:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {}\n}\nUsing the profiles scope in a configuration file groups attributes that belong to the same profile, in our case workshop. Inside this workflow profile, let’s remove the need for the -profile singularity flag from our run command by enabling Singularity by adding another scope called Singularity:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }}\n  }\nNextflow has a number of options for using Singularity that allow you to control how containers are executed. We are using:\n\nenabled to use Singularity to manage containers automatically\nautoMounts to allow Nextflow to automatically mount host paths when a container is executed\ncacheDir to specify the directory Singularity images can be pulled from\n\nNow let’s address those two resource parameters --max_memory 6.GB and --max_cpus 2. At the same level as the singularity {} scope, add a parameters scope and specify each parameter underneath:\n// Nimbus nf-core workshop configuration profile\n\nprofiles {\n  workshop {\n    singularity {\n      enabled     = true\n      autoMounts  = true\n      cacheDir    = \"/home/ubuntu/singularity_cache\"\n    }\n    params {\n      max_cpus   = 2\n      max_memory = '6.GB'      \n    }}\n  }\n\n\n\n\n\n\nTake note!\n\n\n\nIn Nextflow, scope organisation and heirarchy is indicated by curly bracket ({}) notation, not by text indentation!\n\n\nRerun the pipeline:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-3 \\\n  -resume\n\n\n2.3.4. Apply an institutional config file\nWe have created an nf-core config for Pawsey’s Nimbus cloud and shared it at the nf-core/configs repository. This config file was downloaded with the workflow code. Take a look:\ncat nf-core-rnaseq-3.11.1/configs/conf/pawsey_nimbus.config\nThis configuration file provides a few different profiles that match the Nimbus instance flavours currently available, as well as Docker and Singularity container engines. Take a look at the c2r8 scope, looks a lot like our custom-nimbus.config:\n  c2r8 {\n    params {\n      max_cpus   = 2\n      max_memory = '6.GB'\n    }\n  }\nLet’s rerun the workflow with the -resume function and the institutional config parameters, instead of our custom file and see if anything changes:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile pawsey_nimbus,singularity,c2r8 \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-3 \\\n  -resume\nIn the nf-core/rnaseq pipeline’s configuration message printed to the screen at run time, we can see the institutional config has been picked up and our resource parameters are correctly being applied.\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
<<<<<<< HEAD
    "objectID": "notebooks/1.4_users.html",
    "href": "notebooks/1.4_users.html",
    "title": "nf-core for users",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn more about nf-core tooling for users.\nUse nf-core list to view information about nf-core workflows.\nUse nf-core download to download a workflow and it’s singularity images.\nUse nf-core launch to create a parameters file.\n\n\n\n\n1.4.1 nf-core tools for users\nnf-core tools has additional commands to help users execute workflows. Although you do not need to use these commands to execute the nf-core workflows, they can greatly assist and improve and simplify your experience.\nThere are also nf-core tools for developers. However, these will not be covered as a part of this workshop. If you are curious to learn more about these tools you can find more information on the nf-core websites tools page. There are also lots of excellent ByteSize talks on the nf-core YouTube channel.\n\n\n1.4.2 nf-core list\nThe nf-core list command can be used to print a list of remote nf-core workflows along with your local workflow information.\nnf-core list\n\nThe output shows the latest workflow version number and when it was released. You will also be shown if and when a workflow was pulled locally and whether you have the latest version. Keywords can also be supplied to help filter the workflows based on matches in titles, descriptions, or topics:\nnf-core list dna\n\nOptions can also be used to sort the workflows by latest release (-s release, default), when you last pulled a workflow locally (-s pulled), alphabetically (-s name), or number by the number of GitHub stars (-s stars).\n\n\n\n\n\n\nChallenge\n\n\n\nFilter the list of nf-core workflows for those that are for rna and sort them by stars. Which rna workflow has the most stars?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the list command, filter it for rna, and sort by stars:\nnf-core list rna -s stars\n\n\n\n\n\n1.4.2 nf-core download\nSometimes you may need to execute an nf-core workflow on a server or HPC system that has no internet connection. In this case, you will need to fetch the workflow files and manually transfer them to your offline system. To make this process easier and ensure accurate retrieval of correctly versioned code and software containers, nf-core has the download command.\nThe nf-core download command will download both the workflow code and the institutional nf-core/configs files. It can also optionally download singularity image file.\nnf-core download\nIf run without any arguments, the download tool will interactively prompt you for the required information. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:\n\nPipeline name\n\nName of workflow you would like to download.\n\nPipeline revision\n\nThe revision you would like to download.\n\nPull containers\n\nIf you would like to download Singularity images.\nThe path to a folder where you would like to store these images if you have not set your NXF_SINGULARITY_CACHEDIR.\n\nChoose compression type\n\nThe compression type for Singularity images.\n\n\nAlternatively, you could build your own execution command with the command line options.\n\n\n\n\n\n\n\nChallenge\n\n\n\nUse the nf-core download command to download revision 3.11.1 of the nf-core/rnaseq workflow with it’s uncompressed Singularity images. You can use either the command line options or prompts.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core download command for the nf-core/rnaseq workflow and follow the prompts.\nIf you were to write your own command and options it would look like this:\nnf-core download nf-core/rnaseq --revision 3.11.1 --container singularity --compress none\nLet the singularity images download (you will use the during Session 2).\n\n\n\n\n\n1.4.3 nf-core launch\nA workflow can have a large number of optional parameters. To help with this, the nf-core launch command is designed to help you write parameter files for when you launch your workflow.\nThe nf-core launch command takes one argument - either the name of an nf-core workflow which will be pulled automatically or the path to a directory containing a Nextflow workflow:\nnf-core launch nf-core/<workflow>\nWhen running this command, you will first be asked about which version of the workflow you would like to execute. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the workflow parameters. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.\n\nThe nf-core launch tool uses the nextflow_schema.json file from a workflow to give parameter descriptions, defaults, and grouping. If no file for the workflow is found, one will be automatically generated at runtime.\nThe launch tool will save your parameter variables as a .json file called nf-params.json. It will also suggest an execution command that includes the -params-file flag and your new nf-params.json file. The command line wizard will finish by asking if you want to launch the workflow. Any profiles or options that were set using the wizard will be included in your run command.\n\n\n\n\n\n\nChallenge\n\n\n\nGenerate a schema and run command for revision 3.11.1 of the nf-core/rnaseq workflow using the nf-core launch command. Use the test and singularity profiles and name your output folder my_test_output.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core launch command for the nf-core/rnaseq workflow and follow the prompts:\nYour final run command should look like this:\nnextflow run nf-core/rnaseq -r 3.11.1 -profile test,singularity -params-file nf-params.json\nYour nf-params.json file should look like this:\n{\n    \"outdir\": \"my_test_output\"\n}\n\n\n\nYou can also use the launch command directly from the nf-core launch website. In this case, you can configure your workflow using the wizard and then copy the outputs to your terminal or use the run id generated by the wizard. You will need to be connected to the internet to use the run id.\nnf-core launch --id <run_id>\n\n\n\n\n\n\nKey points\n\n\n\n\nThe nf-core list command can be used to view local and remote information about nf-core workflows.\nThe nf-core download command is a powerful way to download a workflow and its Singularity images.\nThe nf-core launch command can be a useful tool for writing parameter files.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html",
    "href": "notebooks/1.2_nfcore.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of nf-core.\nLearn how to use nf-core tooling.\nUse Nextflow to pull the nf-core/rnaseq workflow\n\n\n\n\n1.2.1. What is nf-core?\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won’t be left in the dark.\n\nCI Testing\n\nEvery time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.\n\n\nIt is important to remember all nf-core workflows are open-source and community driven. Most pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results*.\n\n\n1.2.2. Events\nnf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.\nUpcoming events are listed on the nf-core event page and announced on Slack and Twitter.\n\n\n1.2.3. Join the community!\nThere are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe nf-core Slack is one of the primary resources for nf-core users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start.\n\n\n\n\n\n\nQuestions about Nextflow\n\n\n\nIf you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack. It’s worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before.\n\n\nJoining multiple nf-core and Nextflow channels is important to keep up to date with the latest community developments and updates. In particular, following the nf-core and Nextflow Twitter accounts will keep you up-to-date with community announcements. If you are looking for more information about a workflow, the nf-core YouTube channel regularly shares ByteSize seminars about best practises, workflows, and community developments.\n\n\n\n\n\n\nChallenge\n\n\n\nJoin the nf-core Slack and fill in your profile information. If you’re joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself! 👋\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFollow this link to join the nf-core Slack. Follow the instructions to enter your credentials and update your profile. Even if you are already a member of the nf-core Slack, it’s a great time to check your profile is up-to-date.\n\n\n\n\n\n1.2.4. nf-core tools\nThis workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using, developing, and testing workflows.\n\n\n\n\n\n\nHow to download nf-core tools\n\n\n\nnf-core tools is written in Python and is available from the Python Package Index (PyPI):\npip install nf-core\nAlternatively, nf-core tools can be installed from Bioconda:\nconda install -c bioconda nf-core\n\n\nThe nf-core --version option can be used to print your version of nf-core tools:\nnf-core --version\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what version of nf-core tools you have available using the nf-core --version option. If nf-core tools is not installed then install it using the commands above:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core --version option to print your nf-core tools version:\nnf-core --version\nIf you get the message “nf-core: command not found” - install nf-core it using the commands above:\npip install nf-core\nAdd the path to the installed scripts and tools to your PATH:\nexport PATH=$PATH:/home/ubuntu/.local/bin\nUse the nf-core --version option to print your nf-core tools version:\nnf-core --version\n\n\n\nnf-core tools are for everyone and has commands to help both users and developers. For users, the tools make it easier to execute workflows. For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line.\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what nf-core tools commands and options are available using the --help option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the --help option to list the options, commands for users, and commands for developers:\nnf-core --help\n\n\n\n\nnf-core tools is updated with new features and fixes regularly so it’s best to keep your version of nf-core tools up-to-date.\n\n\n1.2.5. Executing an nf-core workflow\nThere are currently 80 workflows (April 2023) available as part of nf-core. These workflows are at various stages of development with 49 released, 19 under development, and 12 archived.\nThe nf-core website has a full list of workflows, as well as their documentation, which can be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to execute the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nUnless you are actively developing workflow code, you don’t need to clone the workflow code from GitHub and can use Nextflow’s built-in functionality to pull and a workflow. As shown in the introduction to Nextflow, the Nextflow pull command can download and cache workflows from GitHub repositories:\nnextflow pull nf-core/<pipeline>\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/<pipeline>\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag.\n\n\n\n\n\n\nChallenge\n\n\n\nUse Nextflow to pull the latest version of the nf-core/rnaseq workflow directly from GitHub:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse Nextlfow to pull the rnaseq workflow from the nf-core GitHub repository:\nnextflow pull nf-core/rnaseq\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nYou can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter…)\nnf-core has its own tooling that can be used by users and developers.\nNextflow can be used to pull nf-core workflows.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.0_template.html",
    "href": "notebooks/0.0_template.html",
    "title": "Lesson title",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nlearning outcome 1\nlearning outcome 2\n\n\n\nSome intro text to what this lesson is about\n\nSub-section heading\nCommands should be written like this:\ncommand \nWhere relevant include expected standard output:\nstdout here\nAny important notes for attendees should be present in information boxes. For example:\n\n\n\n\n\n\nCopying the code from the grey boxes on training materials\n\n\n\nIn this workshop we need to copy code from the grey boxes in the training materials and run it in the terminal. If you hover your mouse over a grey box on the website, a clipboard icon will appear on the right side. Click on the clipboard logo to copy the code. Test it out with:\nssh training@###.###.###.###\n\n\nChallenges/activites should be provided in challenge boxes:\n\n\n\n\n\n\nChallenge\n\n\n\nQuestion or activity\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution explanation and code where relevant\n\n\n\n\nFor other types of callout blocks see here. Any figures should be placed in figs directory and embeddeded like this: \n\n\n\n\n\n\n:shrug: Zoom check-in! :shrug:\n\n\n\nIs everyone ok?\nYes, move on :clap: :clap:\nNo, help! :cry: :cry:\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.5_extArgs.html",
    "href": "notebooks/2.5_extArgs.html",
    "title": "2.5. Passing external arguments to a process",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand how to use the ext.args feature to pass additional command-line arguments to a process\nImplement additional external arguments to a process that are not hardcoded in the process script\nObserve the behaviour of Nextflow’s cache functionality\n\n\n\n\nAll nf-core modules are currently required to provide the mandatory arguments for a tool to run in the process’ main.nf file. Remember that nf-core pipelines generally run with sensible parameters as default, so these processes may also contain commonly changed or applied optional arguments for a tool. Despite this, nf-core pipelines allow you the flexibility to customise the command a process runs, whether it is permitted by the nf-core workflow parameters or not. In this lesson, we will be using Nextflow’s ext directive to specify an optional argument to a process within the workflow.\n\n2.5.1. External directives in action\nnf-core modules provide all optional non-file arguments as a string using the ext directive via the $task.ext.args variable. In the diagram below, on the left is an example of the standard main.nf format of a process stored in the modules/ directory. Above the script block, the $args variable is defined. Inside the script block the $args variable is applied to the process command.\n\n\n\n\n\n\nWhat’s that Nextflow expression doing?\n\n\n\nThe task.ext.args ?: '' expression checks if the ext.args parameter has already been defined already for a task. If it isn’t defined it will assign an empty string by default.\n\n\nBut where is the $ext.args string actually defined? In the middle of the diagram below there is an example of a custom configuration file targeting the modules/example/main.nf process. This configuration file is using the EXAMPLE process ext.args to pass -flag1 to the tool command. Running the pipeline having included -c example-custom.config in the run command, you will observe the application of -flag1 in the .command.sh inside that task’s work directory.\n\n\n\n2.5.2. Customisation at the process level\nTo practice this, we are going to pass an optional flag to the Trim Galore process. It is up to you to decipher what customisations are required to make your analysis suitable for your data and research questions. In order to understand what customisations you may want to specify as extra arguments, we recommend you:\n\nRead the tool documentation to understand all the available parameters for that tool\nView the list of nf-core parameters for that tool\nView which paramteres are hard-coded within the nextflow process\n\nFor the sake of the lesson, let’s assume we want to increase the minimum quality Phred score from the default of 20 to a much more stringent value of 40.\n\n\n\n\n\n\nWarning!\n\n\n\nFor RNAseq datasets, this Phred score is way too high!\n\n\n\nExplore the full list of Trim Galore parameters either using the Trim Galore documentation.\nLook at the nf-core/rnaseq pipeline parameter options in the nf-core/rnaseq pipeline documentation\nView the nf-core/rnaseq pipeline Trim Galore main.nf file\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat Trim Galore flag would you use to change the default Phred score?\nIs this flag offered as an input parameter by the nf-core/rnaseq pipeline?\nWhat optional flags are applied to the Trim Galore process?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n--quality or -q\nNo\n--cores and --gzip\n\n\n\n\nWe will follow the Trim Galore user guide to apply the --quality flag with a Phred score of 40:\n--quality 40\n\n\n2.5.3. Observe the application of ext.args\nTo parse that to the Trim Galore process, we need to specify this in a custom config. This time, our Trim Galore custom configuration file will restrict the usage of the parameter to the only place it applies, the Trim Galore process. We can do this using the Nextflow process {} scope and withName: selector.\nOpen a new file trim-galore.config and add the following content:\n// Trim Galore Phred score custom configuration \n\nprocess {\n    withName: '.*:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE' {   \n        ext.args = '--quality 40'\n    }\n}\n\n\n\n\n\n\nHow did you find that process name?! 🤯\n\n\n\nTracking down process names in nf-core workflow code can be confusing as developers increasingly include subworkflow modules comprising multiple individual modules from the nf-core/modules repository. A process’ name will depend on whether it is included in the workflow as part of a submodule or a module. If we search the workflow code for ‘trimgalore’ we find trimgalore is included in the FASTQ_FASTQC_UMITOOLS_TRIMGALORE subworkflow:\ngrep \"trimgalore\" nf-core-rnaseq-3.11.1/workflow/workflows/rnaseq.nf\nInside that subworkflow, we can see Trim Galore process is named TRIMGALORE:\ncat nf-core-rnaseq-3.11.1/workflow/subworkflows/nf-core/fastq_fastqc_umitools_trimgalore/main.nf\nHence, our arrival at the name:\nFASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE\n\n\nWe need to instruct nextflow to use the custom config, and we do this with the -c flag:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config,trim-galore.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-5 \\\n  -resume\n\n\n\n\n\n\nChallenge\n\n\n\nGiven we have applied the -resume flag, what tasks do you expect to be re-run, and what outputs do you expect to be taken from cache?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTaken from cache: initial FastQC processes for each fastq file\nRerun: read trimming and all downstream processes\n\n\n\n\nThis time, the workflow should run for a while and then stop before performing read alignment with STAR with the following message 😦:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples skipped since they failed 10000 trimmed read threshold.-\nOpen the Lesson 5 multiqc_report.html, like you did in the previous lesson. Note the ‘WARNING: Fail Trimming Check’ on the navigation headings on the left. Click on this warning. Our Phred threshold was way too high, all 6 samples have < 5,000 reads remaining after trimming!\nWhat has caused these samples to fail though? Let’s take a look at the –min_trimmed_reads parameter documentation. Our samples have less reads after trimming than the default value of 10,000.\nOn your multiqc_report.html file, view the section ‘nf-core/rnaseq Workflow Summary’. The quality parameter is not described. We can view the full parameters that were supplied to trimgalore in two ways:\n\nView one of the Trim Galore log files for a sample\n\nmore Lesson-5/trimgalore/SRR3473988.fastq.gz_trimming_report.txt\n\nView the process execution script\n\nDefine the run_name and tool variables, like we did in lesson 2:\nrun_name=<ENTER_YOUR_RUN_NAME>\ntool=trim_galore\nThe run the following custom bash command again:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2>/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nOpen one of the .command.sh files, you’ll see that the quality threshold has been applied.\n\n\n\n\n\n\nStay vigilant!\n\n\n\nThis lesson highlights the need to thoroughly check your output to ensure that the intended anlysis has been run and the results are what you require. The message Pipeline completed successfully printed to your terminal, everytime a run completes (and also exit status of zero for invidual tasks or cluster jobs if you are running on a cluster) indicates that there were no errors running the pipeline, not that your samples have produced the desired output!\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/0.0_welcome.html",
    "href": "notebooks/0.0_welcome.html",
    "title": "Welcome to the workshop!",
    "section": "",
    "text": "In this workshop, we explore Nextflow and nf-core as powerful tools for bioinformatics data processing pipelines. Our goal for this workshop is to provide you with foundational knowledge required to understand the code base of any nf-core pipeline and responsibly apply and customise an nf-core pipelines to your own research.\nThese sessions will be a challenging but rewarding experience that will provide you with an opportunity to develop your skills and get exposure to key concepts in Nextflow. Even if you are already familiar with Nextflow and have some experience with nf-core pipelines, it will take you time and practice beyond this workshop to fully understand the concepts and terminology laid out here.\n\nWorkshop lesson plan\n\n\n\n\n\n\n\nLesson\nOutcomes\n\n\n\n\n1.1. Introduction to Nextflow\nUnderstand core features of Nextflow and learn fundamental Nextflow options and features.\n\n\n1.2. Introduction to nf-core\nUnderstand core features of nf-core and learn how to use nf-core tools utility.\n\n\n1.3. Configuring nf-core workflows\nUnderstand the structure of an nf-core pipeline and the use of customisation options.\n\n\n1.4. Commands for users\nApply the nf-core tools utility’s list, download, and launch commands.\n\n\n2.1. Design and execute a run command\nBuild a run command for the nf-core/rnaseq pipeline using required and optional parameters.\n\n\n2.2. Manage and trace parameters\nTroubleshoot a pipeline warning message and apply a params file to track our parameters.\n\n\n2.3. Use a custom config file\nConfigure compute resources for the workflow using custom and institutional configuration files.\n\n\n2.4. Apply multiple configurations\nApply multiple configuration files to customise various pipeline settings for the same run.\n\n\n2.5. Apply additional arguments\nSupply external arguments to a process that are not preset as pipeline parameters.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.1_nextflow.html",
    "href": "notebooks/1.1_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of Nextflow.\nLearn Nextflow terminology.\nLearn fundamental commands and options for executing workflows.\n\n\n\n\n1.1.1. What is Nextflow?\n\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nIt is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.\nNextflow’s core features are:\n\nWorkflow portability and reproducibility\nScalability of parallelization and deployment\nIntegration of existing tools, systems, and industry standards\n\nWhether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your workflow and improve your productivity.\n\n\n1.1.2. Processes and Channels\nIn Nextflow, processes and channels are the fundamental building blocks of a workflow.\n\nA process is a unit of execution that represents a single computational step in a workflow. It is defined as a block of code that typically performs a one specific task and specifies its input and outputs, as well as any directives and conditional statements required for its execution. Processes can be written in any language that can be executed from the command line, such as Bash, Python, or R.\nProcesses in are executed independently (i.e., they do not share a common writable state) as tasks and can run in parallel, allowing for efficient utilization of computing resources. Nextflow automatically manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.\nA channel is an asynchronous first-in, first-out (FIFO) queue that is used to join processes together. Channels allow data to passed between processes and can be used to manage data, parallelize tasks, and structure workflows. Any process can define one or more channels as an input and output. Ultimately the workflow execution flow itself, is implicitly defined by these declarations.\nImportantly, processes can be parameterized to allow for flexibility in their behavior and to enable their reuse in and between workflows.\n\n\n1.1.3. Execution abstraction\nWhile a process defines what command or script is executed, the executor determines how and where the script is executed.\nNextflow provides an abstraction between the workflow’s functional logic and the underlying execution system. This abstraction allows users to define a workflow once and execute it on different computing platforms without having to modify the workflow definition. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments.\n\nIf not specified, processes are executed on your local computer. The local executor is useful for workflow development and testing purposes. However, for real-world computational workflows, a high-performance computing (HPC) or cloud platform is often required.\nYou can find a full list of supported executors as well as how to configure them here.\n\n\n1.1.4. Nextflow CLI\nNextflow implements a declarative domain-specific language (DSL) that simplifies the writing of complex data analysis workflows as an extension of a general-purpose programming language. As a concise DSL, Nextflow handles recurrent use cases while having the flexibility and power to handle corner cases.\nNextflow is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as “Python for Java” and simplifies the code.\nNextflow provides a robust command line interface for the management and execution of workflows. Nextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and Java 11 (or later, up to 18) to be installed.\nNextflow is distributed as a self-installing package and does not require any special installation procedure.\n\n\n\n\n\n\nHow to install Nextflow\n\n\n\nNextflow can be installed using a few easy steps:\n\nDownload the executable package using either wget -qO- https://get.nextflow.io | bash or curl -s https://get.nextflow.io | bash\nMake the binary executable on your system by running chmod +x nextflow.\nMove the nextflow file to a directory accessible by your $PATH variable, e.g, mv nextflow ~/bin/\n\n\n\n\n\n1.1.5.Nextflow options and commands\nNextflow provides a robust command line interface for the management and execution of workflows. The top-level interface consists of options and commands.\nYou can list Nextflow options and commands with the -h option:\nnextflow -h\n\nOptions for a commands can also be viewed by appending the -help option to a Nextflow command.\nFor example, options for the the run command can be viewed:\nnextflow run -help\n\n\n\n\n\n\n\nChallenge\n\n\n\nFind out which version of Nextflow you are using with the version option.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe version of Nextflow you are using can be printed using the -v option:\nnextflow -version\nOr:\nnextflow -v\n\n\n\n\n\n\n1.1.6. Managing your environment\nYou can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a workflow and will be interpreted by Nextflow. For most users, Nextflow will work without setting any environment variables. However, to improve reproducibility and to optimise your resources, you will benefit from establishing environmental variables.\nFor example, for consistency, it is good practice to pin the version of Nextflow you are using with the NXF_VER variable:\nexport NXF_VER=<version number>\n\n\n\n\n\n\nChallenge\n\n\n\nPin the version of Nextflow to 22.04.5 using the NXF_VER environmental variable and check that it has been applied.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExport the version using the NXF_VER environmental variable:\nexport NXF_VER=22.04.5\nCheck that the new version has been applied using the -v option:\nnextflow -v\n\n\n\n\nSimilarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR variables:\nexport NXF_CONDA_CACHEDIR=<custom/path/to/conda/cache>\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a new folder with the path /home/ubuntu/singularity_cache to store your singularity images and export its location using the NXF_SINGULARITY_CACHEDIR environmental variable:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMake a new folder for your singularity images:\nmkdir /home/ubuntu/singularity_cache\nExport your new folder as your cache directory for singularity images using the NXF_SINGULARITY_CACHEDIR environmental variable:\nexport NXF_SINGULARITY_CACHEDIR=/home/ubuntu/singularity_cache\nSingularity images downloaded by workflow executions will now be stored in this directory.\n\n\n\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session.\nA complete list of environmental variables can be found here.\n\n\n1.1.7. Executing a workflow\nNextflow seamlessly integrates with code repositories such as GitHub. This feature allows you to manage your project code and use public Nextflow workflows quickly, consistently, and transparently.\nThe Nextflow pull command will download a workflow from a hosting platform into your global cache $HOME/.nextflow/assets folder.\nIf you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL. The qualified name is formed by two parts - the owner name and the repository name separated by a / character. For example, if a Nextflow project bar is hosted in a GitHub repository foo at the address http://github.com/foo/bar, it could be pulled using:\nnextflow pull foo/bar\nOr by using the complete URL:\nnextflow pull http://github.com/foo/bar\nAlternatively, the Nextflow clone command can be used to download a workflow into a local directory of your choice:\nnextflow clone foo/bar <your/path>\nThe Nextflow run command is used to initiate the execution of a workflow :\nnextflow run foo/bar\nIf you run a workflow, it will look for a local file with the workflow name you’ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically pull the workflow to your global cache and execute it.\nBe aware of what is already in your current working directory where you launch your workflow, if there are other workflows (or configuration files) you may encounter unexpected results.\n\n\n\n\n\n\nChallenge\n\n\n\nExecute the hello workflow directly from nextflow-io GitHub repository.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the run command to execute the nextflow-io/hello workflow:\nnextflow run nextflow-io/hello\n\n\n\n\nMore information about the Nextflow run command can be found here.\n\n\n1.1.8. Executing a revision\nWhen a Nextflow workflow is created or updated using GitHub (or another code repository), a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the workflow and to ensure that the same version of the workflow is used consistently across different runs.\nThe Nextflow info command can be used to view workflow properties, such as the project name, repository, local path, main script, and revisions. The * indicates which revision of the workflow you have stickied and will be executed when using the run command.\nnextflow info <workflow>\nIt is recommended that you use the revision flag every time you execute a workflow to ensure that the version is correct. To use a specific revision, you simply need to add it to the command line with the --revision or -r flag. For example, to run a workflow with the v1.0 revision, you would use the following command:\nnextflow run <workflow> -r v1.0\nNextflow automatically provides built-in support for version control using Git. With this, users can easily manage and track changes made to a workflow over time. A revision can be a git branch, tag or commit SHA number, and can be used interchangeably.\n\n\n\n\n\n\nChallenge\n\n\n\nExecute the hello workflow directly from the nextflow-io GitHub using the v1.1 revision tag.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the nextflow-io/hello workflow with the revision -r option:\nnextflow run nextflow-io/hello -r v1.1\n\nThe warning shown above is expected as the v1.1 workflow revision was written using an older version of Nextflow that uses the depreciated echo method. As both Nextflow and workflows are updated independently over time, workflows and Nextflow functions can get out of sync. While most nf-core workflows are now dsl2 (the current way of writing workflows), some are still written in dsl1 and may require older version of Nextflow to run.\n\n\n\nIf your local version of a workflow is not the latest you be shown a warning and will be required to use a revision flag when executing the workflow. You can update a workflow with the Nextflow pull command with a revision flag.\n\n\n1.1.9. Nextflow log\nIt is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command:\nnextflow log\nThe log command has multiple options to facilitate the queries and is especially useful while debugging a workflow and inspecting execution metadata. You can view all of the possible log options with -h flag:\nnextflow log -h\nTo query a specific execution you can use the RUN NAME or a SESSION ID:\nnextflow log <run name>\nTo get more information, you can use the -f option with named fields. For example:\nnextflow log <run name> -f process,hash,duration\nThere are many other fields you can query. You can view a full list of fields with the -l option:\nnextflow log -l\n\n\n\n\n\n\nChallenge\n\n\n\nUse the log command to view with process, hash, and script fields for your tasks from your most recent Nextflow execution.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the log command to get a list of you recent executions:\nnextflow log\n\nQuery the process, hash, and script using the -f option for the most recent run:\nnextflow log crazy_faggin -f process,hash,script\n\n\n\n\n\n\n1.1.10. Execution cache and resume\nTask execution caching is an essential feature of modern workflow managers. As such, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.\nNextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID’s are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.\nA multi-step workflow is required to demonstrate cache and resume. The Sydney-Informatics-Hub/nf-core-demo workflow was created with the nf-core create command and has the same structure as nf-core workflows. It is a toy example with 3 processes:\n\nSAMPLESHEET_CHECK\n\nExecutes a custom python script to check the input sample sheet is valid.\n\nFASTQC\n\nExecutes FastQC using the .fastq.gz files from the sample sheet as inputs.\n\nMULTIQC\n\nExecutes MultiQC using the FastQC reports generated by the FASTQC process.\n\n\nThe Sydney-Informatics-Hub/nf-core-demo is a very small nf-core workflow. It uses real data and bioinformatics software and requires additional configuration to run successfully. To run this example you will need to include two profiles in your execution command. Profiles are sets of configuration options that can be accessed by Nextflow. Profiles will be explained in greater detail during the Configuring nf-core workflows section of the workshop.\nTo run this workflow, both the test profile and a software management profile (such as singularity) are required:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity\nThe command line output will print something like this:\n\nExecuting this workflow will create a work directory and a my_results directory with selected results files.\nIn the schematic above, the hexadecimal numbers, such as ff/21abfa, identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed. You can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path:\nls -la work/ff/21abfa87cc7cdec037ce4f36807d32/\nThe files that have been selected for publication in the my_results folder can also be explored:\nls my_results\nIf you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed:\n\nThe FASTQC process runs four times, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [1a/3c54ed] represents just one of the four sets of input data that was processed. To print all the relevant paths to the screen, use the -ansi-log option can be used when executing your workflow:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -ansi-log false\nIt’s very likely you will execute a workflow multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) by resuming a workflow from the last step that was completed successfully and/or unmodified. By adding the -resume option to your run command you can use the cache rather than re-running successful tasks:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -resume\nIf you run the Sydney-Informatics-Hub/nf-core-demo workflow again without making any changes you will see that the cache is used:\n\nIn practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results.\nNotably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.\n\n\n\n\n\n\nChallenge\n\n\n\nInvalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the workflow again with the -resume option to show that the cache has been invalidated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the workflow for the first time (if you have not already).\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity\nUse the task ID shown for the FASTQC process and use it to find and touch a the sample1_R1.fastq.gz file:\ntouch work/ff/21abfa87cc7cdec037ce4f36807d32/sample1_R1.fastq.gz\nExecute the workflow again with the -resume command option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -profile test,singularity -resume\nYou should that 2 of 4 tasks for FASTQC and the MULTIQC task were invalid and were executed again.\nWhy did this happen?\nIn this example, the cache of two FASTQC tasks were invalid. The sample1_R1.fastq.gz file is used by in the samplesheet twice. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions.\n\n\n\nYour work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it’s contents, the Nextflow clean function allows you to selectively remove data associated with specific runs.\nnextflow clean -help\n\nThe -after, -before, and -but options are all very useful to select specific runs to clean. The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command.\n\n\n\n\n\n\nChallenge\n\n\n\nClean your work work directory of staged files but keep your execution logs.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextflow clean command with the -k and -f options:\nnextflow clean -k -f\n\n\n\n\n\n1.1.11. Listing and dropping cached workflows\nOver time, you might want to remove a stored workflows. Nextflow also has functionality to help you to view and remove workflows that have been pulled locally. The Nextflow list command prints the projects stored in your global cache folder ($HOME/.nextflow/assets). These are the workflows that were pulled when you executed either of the Nextflow pull or run commands:\nnextflow list\nIf you want to remove a workflow from your cache you can remove it using the Nextflow drop command:\nnextflow drop <workflow>\n\n\n\n\n\n\nChallenge\n\n\n\nView your cached workflows with the Nextflow list command and remove the nextflow-io/hello workflow with the drop command.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nList your workflow assets:\nnextflow list\nDrop the nextflow-io/hello workflow:\nnextflow drop nextflow-io/hello\nCheck it has been removed:\nnextflow list\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.\nEnvironment variables can be used to control your Nextflow runtime and the underlying Java virtual machine.\nNextflow supports version control and has automatic integrations with online code repositories.\nNextflow will cache your runs and they can be resumed with the -resume option.\nYou can manage workflows with Nextflow commands (e.g., pull, clone, list, and drop).\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
=======
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
    "objectID": "notebooks/2.2_params.html",
    "href": "notebooks/2.2_params.html",
    "title": "2.2. Why and how to use a parameters file",
    "section": "",
<<<<<<< HEAD
    "text": "Objectives\n\n\n\n\nInvestigate a warning message provided by an nf-core pipeline run\nUse the nextflow log command to trace processes cached in the work directory\nObserve how a process is implemented at the task level\nRerun a workflow using a parameter file to specify pipeline-specific parameters\nUnderstand the use of the parameter file for reproducible and transparent research\n\n\n\nIn Lesson 1.3.7, we learned that parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Within the Nextflow code base, they are defined by the params{} scope. They can be suppled as either a --parameter flag to the run command or inside a parameter file.\nIn the previous lesson we supplied pipeline parameters as flags in our run command (--). In this lesson we will add another parameter to our run command and rerun the pipeline using a parameter file.\n\n2.2.1. Why should I use a parameters file?\nUsing a parameter file is advantageous in a number of ways:\n\nCode readability: By using a params file, you can ensure your run command is readable by storing all your parameters customisations in one place and easily make changes or additions as needed.\nReproducibility: You can save the exact parameters used for a particular run of the pipeline in a parameters file. This makes it easier to reproduce the same results and share your pipeline parameters with collaborators.\nFlexibility: If you need to run the same nf-core pipeline with slightly different settings, using a parameters file makes it easier to make those changes without modifying the run command each time.\nVersion control: Using version controlled parameter files allows you to track changes to your pipeline configuration over time and revert to previous versions if needed.\n\n\n\n2.2.2. Troubleshoot the warning message\nWhile our pipeline completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 9m 16s\nCPU hours   : 0.3\nSucceeded   : 66\n\n\n\n\n\n\nHandling dodgy error messages 🤬\n\n\n\nThe first warning message isn’t very descriptive (see this pull request). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline’s developers.\n\n\n➤ Take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the Lesson-2.1/ directory:\nls -la Lesson-2.1/multiqc/star_salmon/\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS Code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe warning we have received is indicating that the read strandedness we specified in our samplesheet.csv and inferred strandedness identified by the RSeqQC process in the pipeline do not match. Look’s like I have incorrectly specified strandedness as forward in the samplesheet.csv when our raw reads actually show an equal distribution of sense and antisense reads (my mistake! 😑).\nFor those not familiar with RNAseq data, incorrectly specified strandedness may negatively impact the read quantification step (process: Salmon quant) and give us inaccurate results. So, let’s clarify how the Salmon quant process is gathering strandedness information for our input files by default and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline.\n\n\n2.2.3. Identify the run command for a process\nTo observe exactly what command is being run for a process, we can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied at the process level, this may not be very clear.\n➤ Take a look at the Salmon quant main.nf file:\ncat nf-core-rnaseq-3.11.1/workflow/modules/nf-core/salmon/quant/main.nf\nIt is very hard to see what is actually happening, given all the different variables, conditional arguments, and loops inside this script. Above the script block we can see strandedness is being applied using a few different conditional arguments. Instead of trying to infer how the $strandedness variable is being defined and applied to the process, let’s use the hidden process files saved to the work/ directory.\n\n\n\n\n\n\nHidden files in the work directory!\n\n\n\nRemember that the pipeline’s results are cached in the work directory. In addition to the cached files, each task execution directories inside the work directory contains a number of hidden files:\n\n.command.sh: The command script run for the task.\n.command.run: The command wrapped used to run the task.\n.command.out: The task’s standard output log.\n.command.err: The task’s standard error log.\n.command.log: The wrapper execution output.\n.command.begin: A file created as soon as the job is launched.\n.exitcode: A file containing the task exit code (0 if successful)\n\n\n\nRecall from lesson 1.1.9 that the nextflow log command has multiple options to facilitate the queries and is especially useful while debugging a pipeline and while inspecting pipeline execution metadata.\nTo understand how Salmon quant is interpreting strandedness, we’re going to use this command to track down the hidden .command.sh scripts for each Salmon quant task that was run. This will allow us to find out how Salmon quant handles strandedness and if there is a way for us to override this.\n➤ Use the Nextflow log command to reveal information about previously executed pipelines:\nnextflow log\nThis will print a list of executed pipelines:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                                                                                                                                                                                                                                                                                                                                                    \n2023-04-21 00:30:30     -               friendly_montalcini     -       f421ddc35d      685266bb-b99b-4945-9a54-981e8f4b1b07    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help                                                                                                                                                                                                                                                                                                 \n2023-04-21 00:40:58     9m 16s         mighty_swanson        OK      f421ddc35d      055e7b7f-c3ea-4fd9-a915-02343099939e    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --input samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory 6.GB --max_cpus 2 --outdir Lesson-1\n➤ All recent runs will be listed in this file, with the most recent at the bottom. Run the command below after filling in your unique run name for our previous lesson. For example:\nnextflow log mighty_swanson\nThat command will list out all the work subdirectories for all processes run. Recall from Session 1 that the actual command issued by a processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these command scripts.\nBut how to find them? 🤔\n➤ Let’s add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=mighty_swanson\n➤ And let’s save the tool of interest (salmon) in another bash variable to pull it from a run command:\ntool=salmon\n➤ Next, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2>/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‘salmon’. There are a few different processes that run Salmon in the workflow. We are looking for Salmon quant which performs the read quantification. For example:\n/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh\n/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh\n/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh\n/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh\n/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh\n/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh\n/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh\n/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh\n/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh\n/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh\n/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh\n/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh\n/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh\nCompared with the salmon quant main.nf file, we get a lot more fine scale details from the .command.sh process scripts:\n\nLooking at the nf-core/rnaseq Parameter documentation and Salmon documentation, we found that we can override this default using the --salmon_quant_libtype U parameter to indicate our data is unstranded and override samplesheet.csv input.\n\n\n\n\n\n\nHow do I get rid of the strandedness check warning message?\n\n\n\nIf we want to get rid of the warning message Please check MultiQC report: 2/2 samples failed strandedness check, we’ll have to change the strandedness fields in our samplesheet.csv. Keep in mind, doing this will invalidate the pipeline’s cache and cause the pipeline to run from the beginning.\n\n\n\n\n2.2.4. Write a parameter file\nNextflow accepts either yaml or json formats for parameter files. Any of the pipeline-specific parameters can be supplied to a Nextflow pipeline in this way. We wrote a parameter file in json format in lesson 1.3.7, let’s use yaml format this time.\n\n\n\n\n\n\nChallenge\n\n\n\nFill in the parameters file below and save as workshop-params.yaml. This time, include the --salmon_quant_libtype U parameter.\n💡 YAML formatting tips!\n\nstrings need to be inside double quotes\nbooleans (true/false) and numbers do not require quotes\n\ninput: \"\"\noutdir: \"\"\nfasta: \"\"\ngtf: \"\"\nstar_index: \"\"\nsalmon_index: \"\"\nskip_markduplicates: \nsave_trimmed: \nsave_unaligned: \nsalmon_quant_libtype: \"\" \n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ninput: \"samplesheet.csv\"\noutdir: \"Lesson-2.2\"\nfasta: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.fa\"\ngtf: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.gtf\"\nstar_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/STAR\"\nsalmon_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/salmon-index\"\nskip_markduplicates: true\nsave_trimmed: true\nsave_unaligned: true\nsalmon_quant_libtype: \"U\"\n\n\n\n\n\n2.2.5. Apply the parameter file\n➤ Once your params file has been saved, run:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -params-file workshop-params.yaml \\\n  -resume                        \nThe number of pipeline-specific parameters we’ve added to our run command has been significantly reduced. The only -- parameters we’ve provided to the run command relate to how the pipeline is executed on our instances. These resource limits won’t be applicable to our imaginary collaborator who will run the pipeline on a different infrastructure.\nAs the workflow runs a second time, you will notice 4 things:\n\nThe command is much tidier thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\nSome processes will be pulled from the cache. These processes remain unaffected by our addition of a new parameter.\n\nThis run of the pipline will complete in a much shorter time.\n\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 1m 51s\nCPU hours   : 0.3 (82.2% cached)\nSucceeded   : 11\nCached      : 55\n\n\n\n\n\n\nKey points\n\n\n\n\nA parameter file can be used to specify input parameters for any Nextflow workflow.\nSpecify parameter files in a workflow run command using the -params-file flag.\nParameter files can be written in YAML or JSON file formats.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.0_intro.html",
    "href": "notebooks/2.0_intro.html",
    "title": "2.0. Introduction to session 2",
    "section": "",
    "text": "This session builds on fundamental concepts learned in Session 1 and provides you with hands-on experience in nf-core workflow customisation. Throughout the session we will be working with data from a published RNAseq study and the nf-core/rnaseq pipeline.\nWe will explore the pipeline source code and apply various customisations using a parameter file and custom configuration files. You will:\n\nCreate these files for our case study\nUse the nf-core tools utility\nRun Nextflow commands to query work directories and configuration files\nWrite some custom Bash code to efficiently extract information from the source code\n\nEach lesson in this session will build on the previous lessons, so you can gain a deeper understanding of the customisation techniques and the impact they have on the workflow and your results 🤓\n\n\n\n\n\n\nApplying what you learn here to other nf-core workflows\n\n\n\nWhile all activities in this session will be performed using the nf-core/rnaseq workflow, all customisation scenarios we explore are applicable to other nf-core workflows and do not require an understanding of RNAseq data processing.\n\n\n\n2.0.1. Log back in to your instance\nFollow set up instructions to log back into your instance.\n\n\n2.0.2. Create a new work directory\nCreate a new directory for all session 2 activities and move into it:\nmkdir ~/session2 && cd $_\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
=======
    "text": "Objectives\n\n\n\n\nInvestigate a warning message provided by an nf-core pipeline run\nUse the nextflow log command to trace processes cached in the work directory\nObserve how a process is implemented at the task level\nRerun a workflow using a parameter file to specify pipeline-specific parameters\nUnderstand the use of the parameter file for reproducible and transparent research\n\n\n\nIn Nextflow, parameters are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Within the Nextflow code base, they are defined by the params{} scope.\nIn the previous lesson we supplied parameters as flags in our run command. Alternatively, we can pass pipeline-specific parameters to any pipeline using Nextflow’s -params-file flag and a JSON or YAML file. This feature has been implemented by nf-core developers and it can enable reproducibility and collaboration. In this lesson we will adjust our run command and rerun the pipeline using a parameter file, rather than specifying all parameters as separate flags in the run command.\n\n2.2.1. Revise the run command\nWhile our pipeline completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 14m 18s\nCPU hours   : 0.4\nSucceeded   : 170\n\n\n\n\n\n\nHandling dodgy error messages 🤬\n\n\n\nThe first warning message isn’t very descriptive (see this pull request). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline’s developers.\n\n\nTake a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the Lesson-2.1/ directory:\nls -la Lesson-2.1/multiqc/star_salmon/\ntotal 1468\ndrwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .\ndrwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..\ndrwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data\ndrwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots\n-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html\nOpen the multiqc_report.html the file navigator panel on the left side of your VS Code window by clicking on it. Then open the rendered html file using the Live Server extension:\n\nCtrl+Shift+P to open the command palette\nSelect Live Server: Open with Live Server to open html file in your browser window.\n\nTake a look a the section labelled WARNING: Fail Strand Check\n\nThe warning we have received is indicating that the read strandedness we specified in our samplesheet.csv and inferred strandedness identified by the pipeline do not match. Look’s like I have incorrectly specified strandedness as forward in the samplesheet.csv when our raw reads actually show an equal distribution of sense and antisense reads (my mistake! 😑).\nFor those not familiar with RNAseq data, this could have a big impact on our results at the read quantification step (Salmon quant). So, let’s clarify what the Salmon quant process is doing and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline.\n\n\n2.2.2. Identify the run command for a process\nTo understand what command is being run for a process, we can attempt to infer this information from a process main.nf script in the modules/ directory. However, given all the different parameters that may be applied at the process level, this may not be very clear.\nTo understand what Salmon is doing, we’re going to use the nextflow log command and some custom bash code to track down the hidden .command.sh scripts for each Salmon quant process to find out how Salmon quant handles strandedness and if there is a way for us to override this.\nUse the Nextflow log command to reveal information about previously executed pipelines:\nnextflow log\nThis will print a list of executed pipelines:\nTIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                                                                                                                                                                                                                                                                                                                                                    \n2023-04-21 00:30:30     -               friendly_montalcini     -       f421ddc35d      685266bb-b99b-4945-9a54-981e8f4b1b07    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help                                                                                                                                                                                                                                                                                                 \n2023-04-21 00:40:58     14m 18s         mighty_swanson        OK      f421ddc35d      055e7b7f-c3ea-4fd9-a915-02343099939e    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --input samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory 6.GB --max_cpus 2 --outdir Lesson-1\nAll recent runs will be listed in this file, with the most recent at the bottom. Run the command below after filling in your unique run name for our previous lesson. For example:\nnextflow log mighty_swanson\nThat command will list out all the work subdirectories for all processes run. Recall from Session 1 that the actual command issued by a processes are all recorded in hidden script files called .command.sh within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these command scripts.\nBut how to find them? 🤔\nLet’s add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=mighty_swanson\nAnd let’s save the tool of interest (salmon) in another bash variable to pull it from a run command:\ntool=salmon\nNext, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‘salmon’. There are a few different processes that run Salmon in the workflow. We are looking for Salmon quant which performs the read quantification. For example:\n/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh\n/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh\n/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh\n/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh\n/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh\n/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh\n/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh\n/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh\n/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh\n/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh\n/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh\n/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh\n/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh\nCompared with the salmon quant main.nf file, we get more information from the .command.sh process scripts:\ncat rnaseq/modules/nf-core/salmon/quant/main.nf\n\nLooking at the nf-core/rnaseq documentation, we can see:\n\nLibrary type is automatically inferred based on the $strandedness variable\nLibrary type can be adjusted using Salmon’s --libType= flag and the nf-core $strandedness variable.\n\nFollowing the Salmon documentation, we can override this default with the nf-core/rnaseq pipeline’s --salmon_quant_libtype U parameter to indicate our data is unstrands.\nIf we want to get rid of the warning message Please check MultiQC report: 6/6 samples failed strandedness check, we’ll have to change the strandedness fields in our samplesheet.csv. Keep in mind, this will cause the pipeline to run from the beginning.\n\n\n2.2.3. Write a parameter file\nNextflow accepts either YAML or JSON formats for parameter files. Any of the pipeline-specific parameters can be supplied to a pipeline run command in this way.\n\n\n\n\n\n\nChallenge\n\n\n\n\nAdd the appropriate fields from your previous run command to the params file below. You’ll be sharing this file with a collaborator working on a different computational infrastructure but the same input and reference files.\n\ninput: \"samplesheet.csv\"\noutdir: \"Lesson-2.2\"\nfasta: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.fa\"\ngtf: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.gtf\"\nstar_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/STAR\"\nsalmon_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/salmon-index\"\nsalmon_quant_libtype: \"U\"\nskip_markduplicates\nsave_trimmed: true\nsave_unaligned:\n💡 You only need to specify pipeline-specific parameters (i.e. -- flags)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSave this file as workshop-params.yaml:\ninput: \"samplesheet.csv\"\noutdir: \"Lesson-2.2\"\nfasta: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.fa\"\ngtf: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.gtf\"\nstar_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/STAR\"\nsalmon_index: \"/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/salmon-index\"\nsalmon_quant_libtype: \"U\"\nskip_markduplicates\nsave_trimmed: true\nsave_unaligned: true\n\n\n\n\n\n2.2.4. Use a parameter file in a pipeline run\nOnce your params file has been saved, run:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  --max_memory 6.GB \\\n  --max_cpus 2 \\\n  -profile singularity \\\n  -params-file workshop-params.yaml \\\n  -resume                        \nThe number of pipeline-specific parameters we’ve added to our run command has been significantly reduced. The only -- parameters we’ve provided to the run command relate to how the pipeline is executed on our instances. These resource limits won’t be applicable to our imaginary collaborator who will run the pipeline on a different infrastructure.\nAs the workflow runs a second time, you will notice 4 things:\n\nThe command is much tidier thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\nSome processes will be pulled from the cache. These processes remain unaffected by our addition of a new parameter.\n\nThis run of the pipline will complete in a much shorter time.\n\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 8m 8s\nCPU hours   : 0.4 (57.2% cached)\nSucceeded   : 147\nCached      : 53\n\n\n\n\n\n\nKey points\n\n\n\n\nA parameter file can be used to specify input parameters for any Nextflow workflow.\nSpecify parameter files in a workflow run command using the -params-file flag.\nParameter files can be written in YAML or JSON file formats.\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
  },
  {
    "objectID": "notebooks/2.6_launch.html",
    "href": "notebooks/2.6_launch.html",
    "title": "Using nf-core launch",
    "section": "",
<<<<<<< HEAD
    "text": "Learning objectives\n\n\n\n\nUse the nf-core launch web GUI to adjust parameters\nUnderstand different options to using launch and when each would be applicable\nRun a workflow in an offline mode\n\n\n\n\nConstruct an execution command\nOpen the nf-core launch website and select the rnaseq pipeline from the select a pipeline menu and select pipeline release 3.11.1. Then select 🚀 Launch.\n\nOn the far right, unhide Show hidden params and then fill out the Nextflow command-line flags section:\n\nLeave -name blank. Doing this will ensure a random name will be applied, like it has been for our CLI runs so far\nFor -profile, enter singularity,c2r8\nLeave -work-dir as default\nToggle -resume to true\n\n\nNext, fill out the input section:\n\nCopy and paste the full path to the samplesheet.csv\nFor -outdir, specify Exercise6\nEnter your email address\nLeave the other sections blank\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nNow scroll through the remaining options, and fill in all of the parameters we have applied via our params file (including the multiqc_yaml file!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nOnce you are satisified that the params are all accounted for (remember that the extra_args.config should have the quality decreased from 40 to 30), click Launch workflow\n\n\nLaunch the workflow without internet access\nBecause we are on a VM, which has internet connection, we could use the first option ‘If your system has an internet connection’. However, as this was covered in Part 1, we will practice with the ‘no internet connection’ option, which is usually the case if you are running on a HPC. Follow the instructions for this method:\n\nCopy the JSON params to a file in your working directory and save it as nf-params.json\n\n{\n    \"input\": \"\\/home\\/ubuntu\\/materials\\/samplesheet.csv\",\n    \"outdir\": \"Exercise6\",\n    \"email\": \"georgina.samaha@sydney.edu.au\",\n    \"fasta\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.fa\",\n    \"gtf\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.gtf\",\n    \"star_index\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/STAR\",\n    \"save_trimmed\": true,\n    \"salmon_quant_libtype\": \"A\",\n    \"extra_salmon_quant_args\": \"'--numBootstraps 10'\",\n    \"save_unaligned\": true,\n    \"config_profile_name\": \"pawsey_nimbus.config,extra_args.config\",\n    \"max_cpus\": 2,\n    \"max_memory\": \"6.GB\",\n    \"multiqc_config\": \"\\/home\\/ubuntu\\/run_then_launch\\/multiqc_config.yaml\",\n    \"show_hidden_params\": true\n}\n\nCopy the nextflow run command and execute it in your VM\n\nnextflow run nf-core/rnaseq -r 3.11.1 -profile singularity,c2r8 -resume -params-file nf-params.json\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
=======
    "text": "Objectives\n\n\n\n\nLearn about the structure of an nf-core workflow.\nLearn how to customize the execution of an nf-core workflow.\nCustomize a toy example of an nf-core workflow.\n\n\n\n\n1.3.1. Workflow structure\nnf-core workflows follow a set of best practices and standardized conventions. nf-core workflows start from a common template and follow the same structure. Although you won’t need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\n\nNextflow DSL2 workflows are built up of subworkflows and modules that are stored as separate .nf files.\n\nMost nf-core workflows consist of a single workflow file (there are a few exceptions). This is the main &lt;workflow&gt;.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.\nA subworkflow is a groups of modules that are used in combination with each other and have a common purpose. For example, the SAMTOOLS_STATS, SAMTOOLS_IDXSTATS, and SAMTOOLS_FLAGSTAT modules are all included in the BAM_STATS_SAMTOOLS subworkflow. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. Within a nf-core workflow, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core workflow, an nf-core subworkflow is developed by the community is shared in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific that are not shared in the nf-core subworkflows repository.\nA module is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives, inputs, outputs, when statements, and a script block. Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are workflow specific that are not shared in the nf-core modules repository.\n\n\n1.3.2. Configuration\nEach nf-core workflow has its own configuration and parameter defaults. The default parameters are required for testing as the workflow is being developed and when it is pushed to GitHub. While the workflow configuration defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements.\nWhen a workflow is launched, Nextflow will look for configuration files in several locations. As each configuration file can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority:\n\nParameters specified on the command line (--parameter)\nParameters that are provided using the -params-file option\nConfig file that are provided using the -c option\nThe config file named nextflow.config in the current directory\nThe config file named nextflow.config in the workflow project directory\nThe config file $HOME/.nextflow/config\nValues defined within the workflow script itself (e.g., main.nf)\n\n\n\n\n\n\n\nWarning\n\n\n\nnf-core workflow parameters must be passed via the command line (--&lt;parameter&gt;) or Nextflow -params-file option. Custom config files, including those provided by the -c option, can be used to provide any configuration except for parameters.\n\n\nNotably, while some of these files are already included in the nf-core workflow repository (e.g., the nextflow.config file in the nf-core workflow repository), others are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file, and -c). Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a workflows execution.\n\n\n1.3.3. Viewing parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used.\n\n\n\n\n\nParameters and their descriptions can also be viewed in the command line using the run command with the --help parameter:\nnextflow run nf-core/&lt;workflow&gt; --help\n\n\n\n\n\n\nChallenge\n\n\n\nView the parameters for the Sydney-Informatics-Hub/nf-core-demo workflow using the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Sydney-Informatics-Hub/nf-core-demo workflow parameters can be printed using the run command and the --help option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --help\n\n\n\n\n\n1.3.4. Parameters in the command line\nAt the highest level, parameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--):\nnextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n\n\n\n\n\n\nTip\n\n\n\nNextflow options are prefixed with a single dash (-) and workflow parameters are prefixed with a double dash (--).\n\n\nDepending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag:\nnextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt; string\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite animal using the multiqc_title parameter using a command line flag:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. Use the -resume option to save time:\nnextflow run Sydney-Informatics-Hub/nf-core-demo --multiqc_title koala -resume\nIn this example, you can check your parameter has been applied by listing the files created in the results folder (my_results):\nls my_results/multiqc/\n--multiqc_title is a parameter that directly impacts a result file. For parameters that are not as obvious, you may need to check your log to ensure your changes have been applied. You can not rely on the changes to parameters printed to the command line when you execute your run:\nnextflow log\nnextflow log &lt;run name&gt; -f \"process,script\"\n\n\n\n\n\n1.3.5. Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file.\nThere are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by the workflow by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by the workflow by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by the workflow by default.\nModule-specific configuration options (both mandatory and optional).\n\ntest.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a small test dataset.\n\ntest_full.config\n\nOnly included if specified as a profile.\nA configuration profile to test the workflow with a full-size test dataset.\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;workflow&gt; -profile test,singularity\nnf-core workflows are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the workflows with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more convenient and more reproducible.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‘out of the box’. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the workflow is working as expected and is a great way to trial a workflow. Some workflows have multiple test profiles for you to test.\n\n\n\n\n1.3.6. Shared configuration files\nAn includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core workflows will fetch these configuration profiles from the nf-core config repository and make them available.\nFor shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help.\n\n\n\n\n\n\nTip\n\n\n\nPawsey has a shared profile that can be used to run workflows.\n\n\n\n\n1.3.7. Custom configuration files\nNextflow will also look for custom configuration files that are external to the workflow project directory. These files include:\n\nThe config file $HOME/.nextflow/config\nA config file named nextflow.config in your current directory\nCustom files specified using the command line\n\nA parameter file that is provided using the -params-file option\nA config file that are provided using the -c option\n\n\nYou don’t need to use all of these files to execute your workflow.\nParameter files\nParameter files are .json files that can contain an unlimited number of parameters:\n{\n   \"&lt;parameter1_name&gt;\": 1,\n   \"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n   \"&lt;parameter3_name&gt;\": true\n}\nYou can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option.\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -param-file &lt;path/to/params.json&gt;\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite food using the multiqc_title parameter in a parameters file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .json file that contains your favourite food, e.g., cheese:\n{\n   \"multiqc_title\": \"cheese\"\n}\nInclude the custom .json file in your execution command with the -params-file option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -params-file my_custom_params.json\nCheck that it has been applied:\nls my_results/multiqc/\n\n\n\nConfiguration files\nConfiguration files are .config files that can contain various workflow properties. Custom paths passed in the command-line using the -c option:\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example:\nalpha.x  = 1\nalpha.y  = 'string value..'\nIs equivalent to:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the Sydney-Informatics-Hub/nf-core-demo workflow the name of your favorite color using the multiqc_title parameter in a custom .config file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom .config file that contains your favourite colour, e.g., blue:\nparams.multiqc_title = \"blue\"\nInclude the custom .config file in your execution command with the -c option:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -c my_custom_config.config\nCheck that it has been applied:\nls my_results/multiqc/\nWhy did this fail?\nYou can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While it parameter is listed as a parameter on the STDOUT, it was not applied to the executed command:\nnextflow log\nnextflow log &lt;run name&gt; -f \"process,script\"\n\n\n\nThe process scope allows you to configure workflow processes and is used extensively to define resources and additional arguments for modules.\nBy default, process resources are allocated in the conf/base.config file using the withLabel selector:\nprocess {\n    withLabel: BIG_JOB {\n        cpus = 16\n        memory = 64.GB\n    }\n}\nSimilarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file:\nprocess {\n    withName: MYPROCESS {\n        cpus = 4\n        memory = 8.GB\n    }\n}\nWhile some tool arguments are included as a part of a module. To make modules sharable across workflows, most tool arguments are defined in the conf/modules.conf file in the workflow code under the ext.args entry.\nFor example, if you were trying to add arguments in the MULTIQC process in the Sydney-Informatics-Hub/nf-core-demo workflow, you could use the process scope:\nprocess {\n    withName : \".*:MULTIQC\" {\n        ext.args   = { \"&lt;your custom parameter&gt;\" }\n\n    }\nHowever, if a process is used multiple times in the same workflow, an extended execution path of the module may be required to make it more specific:\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"&lt;your custom parameter&gt;\"\n    }\n}\nThe extended execution path is built from the workflows, subworkflows, and modules used to execute the process.\nIn the example above, the nf-core MULTIQC module, was called by the DEMO workflow, which was called by the NFCORE_DEMO workflow in the main.nf file.\n\n\n\n\n\n\nTip\n\n\n\nIt can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the conf/modules.conf file. How arguments are added to a process can also vary. Be vigilant when you are modifying parameters.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a new .config file that uses the process scope to overwrite the args for the MULTIQC process. Change the args to your favourite month of the year, e.g, \"--title \\\"october\\\"\".\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMake a custom config file that uses the process scope to replace the args for the MULTIQC process:\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"--title \\\"october\\\"\"\n    }\n}\nExecute your run command again with the custom configuration file:\nnextflow run Sydney-Informatics-Hub/nf-core-demo -c my_custom_config.config -resume\nCheck that it has been applied:\nls my_results/multiqc/\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nDemonstrate the configuration hierarchy using the Sydney-Informatics-Hub/nf-core-demo workflow by adding a params file (-params-file), and a command line flag (--multiqc_title) to your execution. You can use the files you have already created.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the .json file you created previously:\n{\n   \"multiqc_title\": \"cheese\"\n}\nExecute your command with your params file (-params-file) and a command line flag (--multiqc_title):\nnextflow run Sydney-Informatics-Hub/nf-core-demo -resume -params-file my_custom_params.json --multiqc_title \"koala\"\nIn this example, as the command line is at the top of the hierarchy, the multiqc_title will be “koala”.\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows follow a similar structure.\nnf-core workflows are configured using multiple configuration sources.\nConfiguration sources are ranked to decide which settings to apply.\nWorkflow parameters must be passed via the command line (--&lt;parameter&gt;) or Nextflow -params-file option.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
  },
  {
    "objectID": "notebooks/2.6_wrapUp.html",
    "href": "notebooks/2.6_wrapUp.html",
    "title": "Workshop wrap up",
    "section": "",
    "text": "Key points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
<<<<<<< HEAD
    "objectID": "notebooks/2.1_design.html",
    "href": "notebooks/2.1_design.html",
    "title": "2.1. Design and execute an nf-core run command",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand the levels of customisation available for nf-core pipelines\nUse the nf-core documentation to select appropriate parameters for a run command\nWrite and run a nf-core rnaseq command on the command line\nExplore pipeline deployment and outputs\n\n\n\nBefore scaling the nf-core/rnaseq pipeline up to a full dataset, we’ll explore the functionality of the workflow and identify processes that may need to be adjusted or customised. While nf-core pipelines are designed to run with ‘sensible’ default settings, these may not always suit the needs of your experiment of compute environment. Designing a custom run command requires you to identify which parameters you need to specify to suit your circumstances and experimental design. In this lesson, we will download an nf-core pipeline, then design and execute a customised run command using various parameters.\n\n2.1.1. Download the pipeline code\nWe recommend that you keep a local copy of a pipeline’s code for the sake of reproducibility and good record keeping.\n➤ In this session we are using Singularity containers to manage software installation for all nf-core/rnaseq tools. Confirm the Singularity cache directory we set in the previous session using the $NXF_SINGULARITY_CACHEDIR Nextflow environmental variable:\necho $NXF_SINGULARITY_CACHEDIR\nThis should match the directory you set in the previous session:\n/home/ubuntu/singularity_cache\n\n\n\n\n\n\nChallenge\n\n\n\nUse the nf-core download command to download a local copy of the nf-core/rnaseq workflow that:\n\nDownloads pipeline version 3.11.1\n\nOutputs the code base to ~/session2/nf-core-rnaseq-3.11.1/\nDownloads Singularity containers\nUses the preset Singluarity cache and does not copy images to the output directory\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we explored in Lesson 1.4.2 we can fetch the workflow code base with the following command:\nnf-core download rnaseq \\\n  --revision 3.11.1 \\\n  --outdir ~/session2/nf-core-rnaseq-3.11.1 \\\n  --container singularity \\\n  --compress none \\\n  --singularity-cache-only\n\n\n\nThe pipeline files and institutional configuration files from nf-core/configs will be downloaded to ~/session2/nf-core-rnaseq-3.11.1. Inside this directory you will see 2 subdirectories.\n➤ Take a look at the directory structure:\nls -l nf-core-rnaseq-3.11.1/*\nnf-core-rnaseq-3.11.1/configs:\ntotal 64\n-rwxrwxr-x 1 ubuntu ubuntu  1562 Apr  21 09:17 CITATION.cff\n-rwxrwxr-x 1 ubuntu ubuntu  1064 Apr  21 09:17 LICENSE\n-rwxrwxr-x 1 ubuntu ubuntu 17476 Apr  21 09:17 README.md\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 bin\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  21 09:17 conf\n-rwxrwxr-x 1 ubuntu ubuntu   204 Apr  21 09:17 configtest.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  21 09:17 docs\n-rwxrwxr-x 1 ubuntu ubuntu    70 Apr  21 09:17 nextflow.config\n-rwxrwxr-x 1 ubuntu ubuntu  8249 Apr  21 09:17 nfcore_custom.config\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 pipeline\n\nnf-core-rnaseq-3.11.1/workflow:\ntotal 216\n-rwxrwxr-x 1 ubuntu ubuntu 58889 Apr  21 09:17 CHANGELOG.md\n-rwxrwxr-x 1 ubuntu ubuntu  9681 Apr  21 09:17 CITATIONS.md\n-rwxrwxr-x 1 ubuntu ubuntu  9078 Apr  21 09:17 CODE_OF_CONDUCT.md\n-rwxrwxr-x 1 ubuntu ubuntu  1096 Apr  21 09:17 LICENSE\n-rwxrwxr-x 1 ubuntu ubuntu 10002 Apr  21 09:17 README.md\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  21 09:17 assets\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 bin\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 conf\ndrwxrwxr-x 3 ubuntu ubuntu  4096 Apr  21 09:17 docs\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 lib\n-rwxrwxr-x 1 ubuntu ubuntu  2736 Apr  21 09:17 main.nf\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  21 09:17 modules\n-rwxrwxr-x 1 ubuntu ubuntu 13970 Apr  21 09:17 modules.json\n-rwxrwxr-x 1 ubuntu ubuntu 10847 Apr  21 09:17 nextflow.config\n-rwxrwxr-x 1 ubuntu ubuntu 42576 Apr  21 09:17 nextflow_schema.json\n-rwxrwxr-x 1 ubuntu ubuntu   359 Apr  21 09:17 pyproject.toml\ndrwxrwxr-x 4 ubuntu ubuntu  4096 Apr  21 09:17 subworkflows\n-rwxrwxr-x 1 ubuntu ubuntu  1684 Apr  21 09:17 tower.yml\ndrwxrwxr-x 2 ubuntu ubuntu  4096 Apr  21 09:17 workflows\nThe public institutional configs were downloaded to the configs directory. The code base for our pipeline will be stored in the workflow directory. The files and directories we will be working with in this session are:\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nconf/\nContains standard configuration files for various profiles that build on global settings set by nextflow.config\n\n\nmain.nf\nThe executable Nextflow script that defines the structure and flow of the workflow. It calls workflows/rnaseq.nf\n\n\nmodules/\nContains Nextflow processes used by the workflow. They are called by the main.nf file\n\n\nworkflows/rnaseq.nf\nAll the modules, subworkflows, channels, workflow structure for running the rnaseq pipeline\n\n\n\n\n\n\n\n\n\nAlternate installation method\n\n\n\n\n\nIn situations where you might not wish to use the nf-core tools utility, download the nf-core/rnaseq source code from it’s GitHub repository with git.\nClone the nf-core/rnaseq repository:\ngit clone https://github.com/nf-core/rnaseq.git\n⛔ BEWARE ⛔ this method will download a copy of the pipeline with a different directory name and slightly different structure. If you choose to use this method, you will need to adjust some paths specified in the upcoming lessons accordingly.\n\n\n\n\n\n2.1.2. Design your run command\nAs we learned in lesson 1.3.3, all nf-core pipelines have a unique set of pipeline-specific parameters that can be used in conjunction with Nextflow parameters to configure the workflow. Generally, nf-core pipelines can be customised at a few different levels:\n\n\n\n\n\n\n\nLevel of effect\nCustomisation feature\n\n\n\n\nThe workflow\nWhere diverging workflows are available for a pipeline, you may choose a path to follow\n\n\nA process\nWhere more than one tool is available for a single process, you may choose which to use\n\n\nA tool\nApply specific thresholds or optional flags for a tool on top of the default run command\n\n\nCompute resources\nSpecify resource thresholds or software execution methods for the workflow or a process\n\n\n\nLooking at the nf-core/rnaseq pipeline structure provided in the below, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided a choice of multiple workflows different methods and specified which is the default\nProvided a choice of tool for some processes\n\nThe structure of the pipeline gives us a sense of the high-level customisation options available to us.\n\n\n\n\n\n\n\nChallenge\n\n\n\nObserving the diagram above, which statement is true regarding the choice of alignment and quantification methods provided by the nf-core/rnaseq pipeline?\nA. The pipeline uses a fixed method for read alignment and quantification.\nB. Users can choose between several different methods for read alignment and quantification.\nC. The pipeline always performs read alignment and quantification.\nD. The choice of alignment and quantification method is determined automatically based on the input data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correct answer is B. The nf-core/rnaseq pipeline allows users to choose between pseudo-alignment and quantification or several different methods for genome-based read alignment and quantification.\n\nA is incorrect because the pipeline is not limited to a single method.\n\nC is incorrect because while read alignment and quantification using STAR and Salmon are the default method, users can choose other methods if desired.\nD is also incorrect, as the pipeline only accepts fastq files as input and the choice of alignment and quantification method must be specified by the user.\n\n\n\n\nUsers can apply parameters to customise their run command in 3 different ways:\n\nProvided pipeline-specific parameters (i.e. --param_name)\nNextflow parameters (i.e. -profile)\nAdditional process-specific parameters using the ext.args variable (See lesson 2.5)\n\nWhile all nf-core pipelines are provided with comprehensive documentation that explains workflow structure, process outputs, and available parameters it can be challenging to piece these different sources of information together to determine which parameters you should be using.\nWe recommend the following approach to deciding on which parameters to use:\n\nRead the pipeline’s Usage docs to understand required inputs, workflow structure, and available customisation options.\nRead the pipeline’s Output docs to understand which processes are run and which files are output for each process.\nRead the pipeline’s Parameter docs to understand all provided parameters for the pipeline.\nCheck the original documentation of any tools you wish to customise for any additional parameters you may wish to apply.\nAsk yourself the following questions:\n\n\n\n\n\n\n\n\nHow do I know if ext.args is used by a process?\n\n\n\nThe inclusion of ext.args is currently standard practice for all DSL2 nf-core modules where additional parameters may be required to run a process. However, this may not be implemented for all processes. Depending on the pipeline, these process modules may not have defined the ext.args variable.\nTake a look at what is available for the nf-core/rnaseq pipeline:\ngrep -r \"ext.args\" nf-core-rnaseq-3.11.1/workflow/modules/\n\n\nThe number and type of default and optional parameters an nf-core pipeline accepts is at the discretion of it’s developers. However at a minimum, nf-core pipelines typically:\n\nRequire users to specify a sample sheet (--input) detailing sample data and relevant metadata\nAutogenerate or acquire missing reference files from iGenomes (--genome) if not provided by the user.\n\n➤ You can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline by running:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help \n\n\n\n\n\n\nBeware the hidden parameters!\n\n\n\nNotice the message at the bottom of the screen:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nKeep in mind that both this help command and the nf-core parameters documentation hides less common parameters.\n\n\nThe typical or recommended run command for this pipeline is provided at the top of the screen:\nnextflow run nf-core/rnaseq --input samplesheet.csv --outdir <OUTDIR> --genome GRCh37 -profile docker\nIt outlines a requirement for a few basic things:\n\nAn input file\nA location to store outputs\nRelevant reference data\nA software management method\n\n\n\n\n\n\n\nReminder: hyphens matter in Nextflow!\n\n\n\nNextflow-specific parameters use one (-) hyphen, whereas pipeline-specific parameters use two (--). In the typical run command above -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\nMost of us will need to adjust the command a little more for our experiments though. Today we’ll be adjusting the typical nf-core/rnaseq run command by:\n\nProviding our own reference files\nUsing the Singularity profile, instead of Docker\nCustomising some processes\nSpecifying the computing resource limitations of our instances\n\n➤ Our input fastq files (fastqs/), reference data (mm10_reference/), and full sample sheet are already available on an external file system called CernVM-FS that we can access from our Nimbus instances. Take a look at the files:\nls -l /cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Feb 14 05:36 fastqs\ndrwxrwxr-x 3 ubuntu ubuntu 4096 Feb 14 05:46 mm10_reference\n-rw-rw-r-- 1 ubuntu ubuntu  641 Feb 16 05:57 samplesheet.csv\n➤ Our CVMFS path is very long, for the sake of tidiness, store the CVMFS path in a variable for our run command:\nmaterials=/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the nextflow run command, identify which parameters and files we will need to use to provide our prepared fasta and gtf files, as well as STAR and Salmon index directories to the pipeline.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following command:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help\nIn the reference genome options section are the following flags:\n\n--fasta $materials/mm10_reference/mm10_chr18.fa\n\n--gtf $materials/mm10_reference/mm10_chr18.gtf\n--star_index $materials/mm10_reference/STAR\n--salmon_index $materials/mm10_reference/salmon-index\n\n\n\n\n➤ Given we are only testing the pipeline in this session, we only need to work with a couple of samples. Copy the first two samples from the full prepared sample sheet to a local version of the file:\nhead -n 3 $materials/samplesheet.csv > samplesheet.csv\nsample,fastq_1,fastq_2,strandedness\nSRR3473989,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473989_selected.fastq.gz,,forward\nSRR3473988,/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/fastqs/SRR3473988_selected.fastq.gz,,forward\nNow that we have prepared our input data, we will customise the typical run command by:\n\nProviding our own reference fasta, index files, and gtf using file-specific parameters instead of using the Illumina AWS iGenomes database parameter --genomes\nUsing Nextflow’s -profile parameter to specify that we will be running the Singularity profile as specified in nextflow.config instead of Docker\nAdding additional process-specific flags to skip duplicate read marking, save trimmed reads and save unaligned reads\nAdding additional max resource flags to specify the number of CPUs and amount of memory available to the pipeline\n\nYou can see how we’ve customised the typical run command in the diagram below:\n\n\n\n2.1.3. Run the pipeline\n➤ Now that we have prepared our data and chosen which parameters to apply, run the pipeline:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n    --input samplesheet.csv \\\n    --outdir Lesson-2.1 \\\n    --fasta $materials/mm10_reference/mm10_chr18.fa \\\n    --gtf $materials/mm10_reference/mm10_chr18.gtf \\\n    --star_index $materials/mm10_reference/STAR \\\n    --salmon_index $materials/mm10_reference/salmon-index \\\n    -profile singularity \\\n    --skip_markduplicates \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\nTake a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\nAs the workflow starts, you will also see a number of processes spawn out underneath this. Recall from lesson 1 that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.\nTo understand how this is coordinated, consider the STAR_ALIGN process that is being run. You’ll notice a few things:\n\nWe can see which inputs are being worked on by a process by looking at the round brackets at the end of the process name\nWhen a process starts it progressively spawns tasks for all inputs. For some processes this is a single input, for others it is multiple inputs (i.e. samples)\nA number of processes involving reference files and the samplesheet are completed before STAR_ALIGN begins\n\nA single TRIMGALORE process is run across both samples in our samplesheet.csv before STAR_ALIGN begins\nOnce a TRIMGALORE task is completed for a sample, the STAR_ALIGN task for that sample begins\nWhen the STAR_ALIGN process starts, it spawns 2 tasks. Take a look at the image below which explains a workflow’s process status output provided by Nextflow using the data dependencies for the STAR_ALIGN process as an example.\n\n\n\n\n2.1.4. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 03:58:56\nDuration    : 9m 16s\nCPU hours   : 0.3\nSucceeded   : 66\nThe pipeline ran successfully, however note the warning about all samples having failed the strandedness check. We’ll explore that in the next lesson.\n➤ In the meantime, list (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\ntotal 416\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:44 .\ndrwxr-x---  15 ubuntu ubuntu 4.0K Apr 21 01:56 ..\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 03:58 .nextflow\n-rw-rw-r--   1 ubuntu ubuntu 371K Apr 21 03:58 .nextflow.log\n-rw-rw-r--   1 ubuntu ubuntu  17K Apr 21 03:50 .nextflow.log.1\ndrwxrwxr-x   7 ubuntu ubuntu 4.0K Apr 21 03:58 Lesson-2.1\ndrwxrwxr-x   4 ubuntu ubuntu 4.0K Apr 21 02:08 nf-core-rnaseq-3.11.1\n-rw-rw-r--   1 ubuntu ubuntu  563 Apr 21 03:14 samplesheet.csv\ndrwxrwxr-x 143 ubuntu ubuntu 4.0K Apr 21 03:58 work\nNextflow has created 2 new output directories, work and Lesson-2.1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe Lesson-2.1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\nThe .nextflow directory\nThis directory contains a cache subdirectory to store cached data such as downloaded files and can be used to speed up subsequent pipeline runs. It also contains a history file which contains a record of pipeline executions including run time, the unique run name, and command line arguments used.\n\n\nThe .nextflow.log file\nThis file is created by Nextflow during the execution of a pipeline and contains information about all processes and any warnings or errors that occurred during execution.\n\n\n\n\n\n\nChallenge\n\n\n\nWas the runtime for the STAR_ALIGN process comparable for samples SRR3473988 and SRR3473989?\n💡 Hint: use the nextflow log <run_name> -f command and Nextflow trace fields.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following:\nnextflow log <run_name> -f name,realtime | grep \"STAR_ALIGN\" \nRead alignment was comparable for both samples:\nNFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN (SRR3473989) 2m 37s\nNFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN (SRR3473988) 2m 17s\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible default settings and required inputs.\nAn nf-core pipeline’s Usage, Output, and Parameters documentation can be used to design a suitable run command.\nParameters can be used to customise the workflow, processes, tools, and compute resources.\n\n\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible workflows with nf-core",
    "section": "",
    "text": "This course is currently under development\nThis workshop will set you up with the foundational knowledge required to run and customise nf-core workflows in a reproducible manner. Using the nf-core/rnaseq workflow as an example, we will step through essential features common across all nf-core workflows. We will explore ways to adjust the workflow parameters based on the needs of your dataset and configuration the workflow to run on your computational environment.\n\nTrainers\n\nGeorgie Samaha, Sydney Informatics Hub, University of Sydney\nCali Willet, Sydney Informatics Hub, University of Sydney\nChris Hakkaart, Seqera Labs\n\n\n\nTarget audience\nThis workshop is suitable for people who are familiar with working at the command line interface and have some experience running Nextflow and nf-core workflows.\n\n\nPrerequisites\n\nExperience navigating the Unix command line\nFamiliarity with Nextflow and nf-core workflows\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP.\n\n\nCode of Conduct\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up your computer\nFollow these instructions to install VSCode and login to your Nimbus instance.\n\n\nDay 1: Introduction to nf-core\n\n\n\nDay 2: Customising nf-core\nWrite, run, adjust, and re-run an nf-core workflow as we step through various customisation scenarios.\n\n\n\n\n\nCourse survey\nPlease fill out our course survey before you leave. Help us help you! 😁\n\n\nCredits and acknowledgements\nThis workshop event and accompanying materials were developed by the Sydney Informatics Hub, University of Sydney in partnership with Seqera Labs, Pawsey Supercomputing Research Centre, and Australia’s National Research Education Network (AARNet) enabled through the Australian BioCommons (NCRIS via Bioplatforms Australia). This workshop was developed as a part of the Australian BioCommons Bring Your Own Data Platforms project.\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
=======
    "objectID": "notebooks/0.0_template.html",
    "href": "notebooks/0.0_template.html",
    "title": "Lesson title",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nlearning outcome 1\nlearning outcome 2\n\n\n\nSome intro text to what this lesson is about\n\nSub-section heading\nCommands should be written like this:\ncommand \nWhere relevant include expected standard output:\nstdout here\nAny important notes for attendees should be present in information boxes. For example:\n\n\n\n\n\n\nCopying the code from the grey boxes on training materials\n\n\n\nIn this workshop we need to copy code from the grey boxes in the training materials and run it in the terminal. If you hover your mouse over a grey box on the website, a clipboard icon will appear on the right side. Click on the clipboard logo to copy the code. Test it out with:\nssh training@###.###.###.###\n\n\nChallenges/activites should be provided in challenge boxes:\n\n\n\n\n\n\nChallenge\n\n\n\nQuestion or activity\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution explanation and code where relevant\n\n\n\n\nFor other types of callout blocks see here. Any figures should be placed in figs directory and embeddeded like this: \n\n\n\n\n\n\n:shrug: Zoom check-in! :shrug:\n\n\n\nIs everyone ok?\nYes, move on :clap: :clap:\nNo, help! :cry: :cry:\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/1.2_nfcore.html",
    "href": "notebooks/1.2_nfcore.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of nf-core.\nLearn how to use nf-core tooling.\nUse Nextflow to pull the nf-core/rnaseq workflow\n\n\n\n\n1.2.1. What is nf-core?\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won’t be left in the dark.\n\nCI Testing\n\nEvery time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.\n\n\nIt is important to remember all nf-core workflows are open-source and community driven. Most pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results*.\n\n\n1.2.2. Events\nnf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.\nUpcoming events are listed on the nf-core event page and announced on Slack and Twitter.\n\n\n1.2.3. Join the community!\nThere are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe nf-core Slack is one of the primary resources for nf-core users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start.\n\n\n\n\n\n\nQuestions about Nextflow\n\n\n\nIf you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack. It’s worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before.\n\n\nJoining multiple nf-core and Nextflow channels is important to keep up to date with the latest community developments and updates. In particular, following the nf-core and Nextflow Twitter accounts will keep you up-to-date with community announcements. If you are looking for more information about a workflow, the nf-core YouTube channel regularly shares ByteSize seminars about best practises, workflows, and community developments.\n\n\n\n\n\n\nChallenge\n\n\n\nJoin the nf-core Slack and fill in your profile information. If you’re joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself! 👋\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFollow this link to join the nf-core Slack. Follow the instructions to enter your credentials and update your profile. Even if you are already a member of the nf-core Slack, it’s a great time to check your profile is up-to-date.\n\n\n\n\n\n1.2.4. nf-core tools\nThis workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using, developing, and testing workflows.\n\n\n\n\n\n\nHow to download nf-core tools\n\n\n\nnf-core tools is written in Python and is available from the Python Package Index (PyPI):\npip install nf-core\nAlternatively, nf-core tools can be installed from Bioconda:\nconda install -c bioconda nf-core\n\n\nThe nf-core --version option can be used to print your version of nf-core tools:\nnf-core --version\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what version of nf-core tools you have available using the nf-core --version option. If nf-core tools is not installed then install it using the commands above:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the nf-core --version option to print your nf-core tools version:\nnf-core --version\nIf you get the message “nf-core: command not found” - install nf-core it using the commands above:\npip install nf-core\nAdd the path to the installed scripts and tools to your PATH:\nexport PATH=$PATH:/home/ubuntu/.local/bin\nUse the nf-core --version option to print your nf-core tools version:\nnf-core --version\n\n\n\nnf-core tools are for everyone and has commands to help both users and developers. For users, the tools make it easier to execute workflows. For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line.\n\n\n\n\n\n\nChallenge\n\n\n\nFind out what nf-core tools commands and options are available using the --help option:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the --help option to list the options, commands for users, and commands for developers:\nnf-core --help\n\n\n\n\nnf-core tools is updated with new features and fixes regularly so it’s best to keep your version of nf-core tools up-to-date.\n\n\n1.2.5. Executing an nf-core workflow\nThere are currently 80 workflows (April 2023) available as part of nf-core. These workflows are at various stages of development with 49 released, 19 under development, and 12 archived.\nThe nf-core website has a full list of workflows, as well as their documentation, which can be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to execute the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nUnless you are actively developing workflow code, you don’t need to clone the workflow code from GitHub and can use Nextflow’s built-in functionality to pull and a workflow. As shown in the introduction to Nextflow, the Nextflow pull command can download and cache workflows from GitHub repositories:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag.\n\n\n\n\n\n\nChallenge\n\n\n\nUse Nextflow to pull the latest version of the nf-core/rnaseq workflow directly from GitHub:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse Nextlfow to pull the rnaseq workflow from the nf-core GitHub repository:\nnextflow pull nf-core/rnaseq\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nYou can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter…)\nnf-core has its own tooling that can be used by users and developers.\nNextflow can be used to pull nf-core workflows.\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.5_extArgs.html",
    "href": "notebooks/2.5_extArgs.html",
    "title": "2.5. Passing external arguments to a process",
    "section": "",
    "text": "Objectives\n\n\n\n\nUnderstand how to use the ext.args feature to pass additional command-line arguments to a process\nImplement additional external arguments to a process that are not hardcoded in the process script\nObserve the behaviour of Nextflow’s cache functionality\n\n\n\n\nAll nf-core modules are currently required to provide the mandatory arguments for a tool to run in the process’ main.nf file. Remember that nf-core pipelines generally run with sensible parameters as default, so these processes may also contain commonly changed or applied optional arguments for a tool. Despite this, nf-core pipelines allow you the flexibility to customise the command a process runs, whether it is permitted by the nf-core workflow parameters or not. In this lesson, we will be using Nextflow’s ext directive to specify an optional argument to a process within the workflow.\n\n2.5.1. External directives in action\nnf-core modules provide all optional non-file arguments as a string using the ext directive via the $task.ext.args variable. In the diagram below, on the left is an example of the standard main.nf format of a process stored in the modules/ directory. Above the script block, the $args variable is defined. Inside the script block the $args variable is applied to the process command.\n\n\n\n\n\n\nWhat’s that Nextflow expression doing?\n\n\n\nThe task.ext.args ?: '' expression checks if the ext.args parameter has already been defined already for a task. If it isn’t defined it will assign an empty string by default.\n\n\nBut where is the $ext.args string actually defined? In the middle of the diagram below there is an example of a custom configuration file targeting the modules/example/main.nf process. This configuration file is using the EXAMPLE process ext.args to pass -flag1 to the tool command. Running the pipeline having included -c example-custom.config in the run command, you will observe the application of -flag1 in the .command.sh inside that task’s work directory.\n\n\n\n2.5.2. Customisation at the process level\nTo practice this, we are going to pass an optional flag to the Trim Galore process. It is up to you to decipher what customisations are required to make your analysis suitable for your data and research questions. In order to understand what customisations you may want to specify as extra arguments, we recommend you:\n\nRead the tool documentation to understand all the available parameters for that tool\nView the list of nf-core parameters for that tool\nView which paramteres are hard-coded within the nextflow process\n\nFor the sake of the lesson, let’s assume we want to increase the minimum quality Phred score from the default of 20 to a much more stringent value of 40.\n\n\n\n\n\n\nWarning!\n\n\n\nFor RNAseq datasets, this Phred score is way too high!\n\n\n\nExplore the full list of Trim Galore parameters either using the Trim Galore documentation.\nLook at the nf-core/rnaseq pipeline parameter options in the nf-core/rnaseq pipeline documentation\nView the nf-core/rnaseq pipeline Trim Galore main.nf file\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat Trim Galore flag would you use to change the default Phred score?\nIs this flag offered as an input parameter by the nf-core/rnaseq pipeline?\nWhat optional flags are applied to the Trim Galore process?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n--quality or -q\nNo\n--cores and --gzip\n\n\n\n\nWe will follow the Trim Galore user guide to apply the --quality flag with a Phred score of 40:\n--quality 40\n\n\n2.5.3. Observe the application of ext.args\nTo parse that to the Trim Galore process, we need to specify this in a custom config. This time, our Trim Galore custom configuration file will restrict the usage of the parameter to the only place it applies, the Trim Galore process. We can do this using the Nextflow process {} scope and withName: selector.\nOpen a new file trim-galore.config and add the following content:\n// Trim Galore Phred score custom configuration \n\nprocess {\n    withName: '.*:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE' {   \n        ext.args = '--quality 40'\n    }\n}\n\n\n\n\n\n\nHow did you find that process name?! 🤯\n\n\n\nTracking down process names in nf-core workflow code can be confusing as developers increasingly include subworkflow modules comprising multiple individual modules from the nf-core/modules repository. A process’ name will depend on whether it is included in the workflow as part of a submodule or a module. If we search the workflow code for ‘trimgalore’ we find trimgalore is included in the FASTQ_FASTQC_UMITOOLS_TRIMGALORE subworkflow:\ngrep \"trimgalore\" nf-core-rnaseq-3.11.1/workflow/workflows/rnaseq.nf\nInside that subworkflow, we can see Trim Galore process is named TRIMGALORE:\ncat nf-core-rnaseq-3.11.1/workflow/subworkflows/nf-core/fastq_fastqc_umitools_trimgalore/main.nf\nHence, our arrival at the name:\nFASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE\n\n\nWe need to instruct nextflow to use the custom config, and we do this with the -c flag:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config,trim-galore.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-5 \\\n  -resume\n\n\n\n\n\n\nChallenge\n\n\n\nGiven we have applied the -resume flag, what tasks do you expect to be re-run, and what outputs do you expect to be taken from cache?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTaken from cache: initial FastQC processes for each fastq file\nRerun: read trimming and all downstream processes\n\n\n\n\nThis time, the workflow should run for a while and then stop before performing read alignment with STAR with the following message 😦:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 6/6 samples skipped since they failed 10000 trimmed read threshold.-\nOpen the Lesson 5 multiqc_report.html, like you did in the previous lesson. Note the ‘WARNING: Fail Trimming Check’ on the navigation headings on the left. Click on this warning. Our Phred threshold was way too high, all 6 samples have &lt; 5,000 reads remaining after trimming!\nWhat has caused these samples to fail though? Let’s take a look at the –min_trimmed_reads parameter documentation. Our samples have less reads after trimming than the default value of 10,000.\nOn your multiqc_report.html file, view the section ‘nf-core/rnaseq Workflow Summary’. The quality parameter is not described. We can view the full parameters that were supplied to trimgalore in two ways:\n\nView one of the Trim Galore log files for a sample\n\nmore Lesson-5/trimgalore/SRR3473988.fastq.gz_trimming_report.txt\n\nView the process execution script\n\nDefine the run_name and tool variables, like we did in lesson 2:\nrun_name=&lt;ENTER_YOUR_RUN_NAME&gt;\ntool=trim_galore\nThe run the following custom bash command again:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nOpen one of the .command.sh files, you’ll see that the quality threshold has been applied.\n\n\n\n\n\n\nStay vigilant!\n\n\n\nThis lesson highlights the need to thoroughly check your output to ensure that the intended anlysis has been run and the results are what you require. The message Pipeline completed successfully printed to your terminal, everytime a run completes (and also exit status of zero for invidual tasks or cluster jobs if you are running on a cluster) indicates that there were no errors running the pipeline, not that your samples have produced the desired output!\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.4_multiConfig.html",
    "href": "notebooks/2.4_multiConfig.html",
    "title": "2.4. Using multiple configuration files at once",
    "section": "",
    "text": "Objectives\n\n\n\n\nCreate and apply a custom configuration file for resource tracing\nCreate and apply a custom configuration file for MultiQC\nAdd a custom configuration file details to the params YAML file\nApply multiple custom configuration files in workflow execution\nObserve the heirarchy of parameter configuration in action\n\n\n\nUsing multiple configuration files allows you to customise nf-core pipelines to your specific needs. Depending on how you work, where you work, and what pipelines you run, you may have configuration files that handle distinct issues, can be applied to specific infrastructures, or shared by multiple nf-core pipelines. In this lesson we will be applying multiple configuration files to the one run command.\n\n2.4.1. Apply multiple configurations\nYou may need to apply multiple configuration files at once to an nf-core pipeline run. A few scenarios in which you would need to do this include:\n\n\n\n\n\n\n\nScenario\nWhy?\n\n\n\n\nDebugging and testing\nIf you are toubleshooting a particular process in the workflow, you may want to create a separate config file with process-specific modifications.\n\n\nResource optimisation\nDepending on the scale and complexity of your data, some steps of the pipeline may require different settings or resources to achieve optimal performance. By using different config files with different parameters or settings, you can fine-tune the pipeline run.\n\n\nMultiQC configuration\nMultiQC allows users to specify configuration options in a YAML file called multiqc_config.yaml. This file allows users to customize the behavior and appearance of the MultiQC report.\n\n\n\n\n2.4.2. Customised resource tracing\nRemember we can use the Nextflow log command to show information about previous pipeline executions. We can customise which fields to print using the -fields flag. This flag accepts options used by the trace report.\n\n\n\n\n\n\nChallenge\n\n\n\nUsing the trace report fields, write a nextflow log command to query the following for a previous workflow run:\n\nThe name of the task\nThe exit status of the task\nThe number of CPUs requested for task execution\nThe disk space requested for the task execution\nThe memory requested for the task execution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nnextflow log &lt;runName&gt; -f name,exit,cpus,disk,memory\n\n\n\nLet’s make a portable configuration file for resource tracing called trace-benchmark.config. We can apply this config when we want to collect resource benchmarks. Because this config is not infrastructure- or pipeline-specific, we can share it with collaborators and use it across multiple pipelines:\n// Define timestamp, to avoid overwriting existing trace \ndef trace_timestamp = new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')\n\n// Generate custom resource trace file \ntrace {\n  enabled = true \n  file = \"workshop-resource-trace-${trace_timestamp}.txt\"\n  fields = 'name,status,realtime,cpus,%cpu,memory,%mem,rss'\n}\nRerun the workflow, adding trace-benchmark.config to the config flag and observe the extra file that is saved to our session2 directory:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config,trace-benchmark.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-4 \\\n  -resume\nTake a look at the launch log printed to the screen. You can see your profile, custom config, and parameters have all been applied successfully! Once the workflow has run, take a look at the workshop-resource-trace-${trace_timestamp}.txt file:\nname    status  realtime        cpus    %cpu    memory  %mem    rss\nNFCORE_RNASEQ:RNASEQ:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet.csv)    CACHED  1s      1       28.0%   6 GB    0.2%    16.1 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (mm10_chr18.gtf)    CACHED  1s      2       51.3%   6 GB    0.3%    23.8 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (mm10_chr18.fa)        CACHED  1s      1       34.1%   6 GB    0.0%    4.7 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_GENE_FILTER (mm10_chr18.fa)     CACHED  1s      1       34.7%   6 GB    0.1%    13.9 MB\nNFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:FQ_SUBSAMPLE (SRR3473984)        CACHED  2s      1       43.7%   6 GB    0.0%    6.5 MB\nNFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:MAKE_TRANSCRIPTS_FASTA (rsem/mm10_chr18.fa) CACHED  4s      2       42.0%   6 GB    1.2%    94.4 MB\nNFCORE_RNASEQ:RNASEQ:FASTQ_SUBSAMPLE_FQ_SALMON:FQ_SUBSAMPLE (SRR3473985)        CACHED  2s      1       44.1%   6 GB    0.0%    6.3 MB\n\n\n\n\n\n\n☠️ Don’t confuse double and single quotes! ☠️\n\n\n\nDouble quotes (\"\") allow for variable interpolation, which means that variables can be evaluated and their values are substituted within the string.\nSingle quotes ('') denote a string literally, which means that the string is treated as-is without any variable interpolation.\nTry swapping the double quoted file value in your custom config above for a single-quoted value, rerun the workflow and note the creation of a\nfile = 'workshop-resource-trace-${trace_timestamp}.txt'\n\n\n\n\n\n2.4.2. Configure MultiQC reports\nMany nf-core pipelines use MultiQC to generate a summary report at the end of a workflow. MultiQC is a reporting tool that can aggregate results and statistics output by various bioinformatics tools. It helps to summarise experiments containing multiple samples and multiple analysis steps. In nf-core pipelines, MultiQC is used to summarise statistics and results output by processes in the workflow and its reports can be quite verbose. MultiQC can be configured using custom YAML files to customise the outputs of reports.\nWe are going to configure MultiQC to aid interpretation of the FastQC output it plots. Create a custom MultiQC configuration YAML file called multiqc-config.yaml and add the following:\nfastqc_config:\n  fastqc_theoretical_gc: \"mm10_txome\" \nHere, we’re plotting the theoretical GC content for the mouse reference transcript.\nTo the workshop-params.yaml file, add:\nmultiqc_config: \"multiqc-config.yaml\" \nMake sure both YAML files are saved, then re-run the workflow:\nnextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \\\n  -profile workshop \\\n  -c custom-nimbus.config \\\n  -params-file workshop-params.yaml \\\n  --outdir Lesson-4 \\\n  -resume\nThe logging information printed to the screen shows us again that the params.yaml file with the output directory set to Lesson-2 has been overridden by the use of the flag --outdir Lesson-4 in the run command. As the workflow runs, notice all the completed processes that are pulled from the cache and not rerun. Our use of the multiqc-config.yaml file adjusted the MultiQC process, as such this process was repeated using the updated settings.\nThe changes we made above added the normal mouse transcriptome GC profile as a track to the fastQC per-sequence GC content plot. Take a look at the changes, open the Exercise4 multiqc_report.html file with Live Server as per previosuly (or use scp to take a copy to your local computer if you are not on VS Code). Compare this report with the MultiQC report from lesson 1 by looking at the section FastQC: Per Sequence GC Content. Compare the two plots to observe the custom track has been successfully added.\n\n\n\n\n\n\n\nChallenge\n\n\n\nGiven the normal mouse transcriptome GC profile indicated by the black dotted line, can you detect any GC bias in the input sequence data?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, all samples’ GC profiles follow a normal distribution.\n\n\n\nNote that if we did detect GC bias, we could go back and correct for this by adding the custom salmon flag --gcBias to the nf-core parameter --extra_salmon_quant_args. If we were to do this, we would have to add the following line to our workflow-params.yaml file:\nextra_salmon_quant_args : '--writeUnmappedNames --gcBias'\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/2.6_launch.html",
    "href": "notebooks/2.6_launch.html",
    "title": "Using nf-core launch",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUse the nf-core launch web GUI to adjust parameters\nUnderstand different options to using launch and when each would be applicable\nRun a workflow in an offline mode\n\n\n\n\nConstruct an execution command\nOpen the nf-core launch website and select the rnaseq pipeline from the select a pipeline menu and select pipeline release 3.11.1. Then select 🚀 Launch.\n\nOn the far right, unhide Show hidden params and then fill out the Nextflow command-line flags section:\n\nLeave -name blank. Doing this will ensure a random name will be applied, like it has been for our CLI runs so far\nFor -profile, enter singularity,c2r8\nLeave -work-dir as default\nToggle -resume to true\n\n\nNext, fill out the input section:\n\nCopy and paste the full path to the samplesheet.csv\nFor -outdir, specify Exercise6\nEnter your email address\nLeave the other sections blank\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nNow scroll through the remaining options, and fill in all of the parameters we have applied via our params file (including the multiqc_yaml file!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nOnce you are satisified that the params are all accounted for (remember that the extra_args.config should have the quality decreased from 40 to 30), click Launch workflow\n\n\nLaunch the workflow without internet access\nBecause we are on a VM, which has internet connection, we could use the first option ‘If your system has an internet connection’. However, as this was covered in Part 1, we will practice with the ‘no internet connection’ option, which is usually the case if you are running on a HPC. Follow the instructions for this method:\n\nCopy the JSON params to a file in your working directory and save it as nf-params.json\n\n{\n    \"input\": \"\\/home\\/ubuntu\\/materials\\/samplesheet.csv\",\n    \"outdir\": \"Exercise6\",\n    \"email\": \"georgina.samaha@sydney.edu.au\",\n    \"fasta\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.fa\",\n    \"gtf\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/mm10_chr18.gtf\",\n    \"star_index\": \"\\/home\\/ubuntu\\/materials\\/mm10_reference\\/STAR\",\n    \"save_trimmed\": true,\n    \"salmon_quant_libtype\": \"A\",\n    \"extra_salmon_quant_args\": \"'--numBootstraps 10'\",\n    \"save_unaligned\": true,\n    \"config_profile_name\": \"pawsey_nimbus.config,extra_args.config\",\n    \"max_cpus\": 2,\n    \"max_memory\": \"6.GB\",\n    \"multiqc_config\": \"\\/home\\/ubuntu\\/run_then_launch\\/multiqc_config.yaml\",\n    \"show_hidden_params\": true\n}\n\nCopy the nextflow run command and execute it in your VM\n\nnextflow run nf-core/rnaseq -r 3.11.1 -profile singularity,c2r8 -resume -params-file nf-params.json\n\n\n\n\n\n\nKey points\n\n\n\n\ntakeaway 1\ntakeaway 2\n\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
>>>>>>> 87d5d0810621d0186df861ac873d291fe747b4c0
  }
]