---
title: "**2.2. Why and how to use a parameters file**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

::: callout-tip
### Objectives{.unlisted}

- Investigate a warning message provided by an nf-core pipeline run
- Use the `nextflow log` command to trace processes cached in the work directory
- Observe how a process is implemented at the task level
- Rerun a workflow using a parameter file to specify pipeline-specific parameters
- Understand the use of the parameter file for reproducible and transparent research
:::

In Nextflow, [parameters](https://www.nextflow.io/docs/latest/config.html#scope-params) are values that can be set by the user and used to control the behaviour of a workflow or process within the workflow. Within the Nextflow code base, they are defined by the `params{}` scope. 

In the previous lesson we supplied pipeline parameters as flags in our run command (`--`). We can also pass pipeline-specific parameters to any pipeline using Nextflow's `-params-file` flag and a JSON or YAML file. You can imagine our run command can quickly become very difficult to read if we were to provide a few more additional parameters. In this lesson we will add another parameter and rerun the pipeline using a parameter file, rather than specifying all parameters as separate flags in the run command. 

### **2.2.1. Why should I use a parameters file?**

You saw how many parameters the nf-core/rnaseq pipeline offers in the previous lesson! Can you imagine what your run command would look like if you needed to use a lot more of these parameters. Using a parameters file can offer several advantages over specifying mulitple flags for an nf-core run command: 

1. Code readability: By using a params file, you can ensure your run command is readable by storing all your parameters customisations in one place and easily make changes or additions as needed.
2. Reproducibility: You can save the exact parameters used for a particular run of the pipeline in a parameters file. This makes it easier to reproduce the same results later, or to share your pipeline parameters with collaborators.
3. Flexibility: If you need to run the same nf-core pipeline with slightly different settings, using a parameters file makes it easier to make those changes without modifying the run command each time. 
4. Version control: Using version controlled parameter files allows you to track changes to your pipeline configuration over time and revert to previous versions if needed.

### **2.2.2. Revise the run command**

While our pipeline completed successfully, there were a couple of warning messages that may be cause for concern:

```default
-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-
-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-
Completed at: 21-Apr-2023 03:58:56
Duration    : 9m 16s
CPU hours   : 0.3
Succeeded   : 66
```

::: {.callout-warning}
### Handling dodgy error messages :cursing_face:
The first warning message isn't very descriptive (see this [pull request](https://github.com/nf-core/rnaseq/pull/963)). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline's developers.
:::

Take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the `Lesson-2.1/` directory: 

```default
ls -la Lesson-2.1/multiqc/star_salmon/
```

```defaul
total 1468
drwxrwxr-x 4 ubuntu ubuntu    4096 Apr 12 04:13 .
drwxrwxr-x 3 ubuntu ubuntu    4096 Apr 12 04:13 ..
drwxrwxr-x 2 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_data
drwxrwxr-x 5 ubuntu ubuntu    4096 Apr 12 04:13 multiqc_plots
-rw-rw-r-- 1 ubuntu ubuntu 1483297 Apr 12 04:13 multiqc_report.html
```

Open the `multiqc_report.html` the file navigator panel on the left side of your VS Code window by clicking on it. Then open the rendered html file using the Live Server extension:

1. `Ctrl`+`Shift`+`P` to open the command palette 
2. Select `Live Server: Open with Live Server` to open html file in your browser window.

Take a look a the section labelled **WARNING: Fail Strand Check**

![](../figs/2.2_multiqc.png)

The warning we have received is indicating that the read strandedness we specified in our `samplesheet.csv` and **inferred strandedness** identified by the RSeqQC process in the pipeline do not match. Look's like I have incorrectly specified strandedness as forward in the `samplesheet.csv` when our raw reads actually show an equal distribution of sense and antisense reads (my mistake! :expressionless:). 

For those not familiar with RNAseq data, incorrectly specified strandedness may negatively impact the read quantification step (process: Salmon quant) and give us inaccurate results. So, let's clarify how the Salmon quant process is gathering strandedness information for our input files by default and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline. 

### **2.2.3. Identify the run command for a process**

To observe exactly what command is being run for a process, we can attempt to infer this information from a process `main.nf` script in the `modules/` directory. However, given all the different parameters that may be applied at the process level, this may not be very clear. Take a look at the Salmon quant `main.nf` file: 

```default
cat nf-core-rnaseq-3.11.1/workflow/modules/nf-core/salmon/quant/main.nf
```

It is very hard to see what is actually happening, given all the different variables, conditional arguments, and loops inside this script. 

Recall from [lesson 1.1.9](1.1_nextflow.html#nextflow-log) that the `nextflow log` command has multiple options to facilitate the queries and is especially useful while debugging a pipeline and while inspecting pipeline execution metadata. 

To understand how Salmon quant is interpreting strandedness, we're going to use this command to track down the hidden `.command.sh` scripts for each Salmon quant task that was run. This will allow us to find out how Salmon quant handles strandedness and if there is a way for us to override this. 

Use the [Nextflow log](https://www.nextflow.io/docs/latest/tracing.html#execution-log) command to reveal information about previously executed pipelines: 

```default
nextflow log
```
This will print a list of executed pipelines: 

```default
TIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                                                                                                                                                                                                                                                                                                                                                    
2023-04-21 00:30:30     -               friendly_montalcini     -       f421ddc35d      685266bb-b99b-4945-9a54-981e8f4b1b07    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --help                                                                                                                                                                                                                                                                                                 
2023-04-21 00:40:58     9m 16s         mighty_swanson        OK      f421ddc35d      055e7b7f-c3ea-4fd9-a915-02343099939e    nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf --input samplesheet.csv -profile singularity --fasta /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.fa --gtf /home/ubuntu/session2/materials/mm10_reference/mm10_chr18.gtf --star_index /home/ubuntu/session2/materials/mm10_reference/STAR --max_memory 6.GB --max_cpus 2 --outdir Lesson-1
```

All recent runs will be listed in this file, with the most recent at the bottom. Run the command below after filling in your unique run name for our previous lesson. For example: 

```default
nextflow log mighty_swanson
```

That command will list out all the work subdirectories for all processes run.  Recall from Session 1 that the actual command issued by a processes are all recorded in hidden script files called `.command.sh` within the execution process directory. One way of observing the actual run commands issued by the workflow is to view these command scripts. 

But how to find them? :thinking: 

Let's add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:

```default
run_name=mighty_swanson
```

And let's save the tool of interest (salmon) in another bash variable to pull it from a run command:
```default
tool=salmon
```

Next, run the following bash command:
```default
nextflow log ${run_name} | while read line;
    do
      cmd=$(ls ${line}/.command.sh 2>/dev/null);
      if grep -q $tool $cmd;
      then  
        echo $cmd;     
      fi; 
    done 
```

That will list all process `.command.sh` scripts containing 'salmon'. There are a few different processes that run Salmon in the workflow. We are looking for Salmon quant which performs the read quantification. For example: 

```default
/home/ubuntu/session2/work/50/d4462ece237213ace901a779a45286/.command.sh
/home/ubuntu/session2/work/2f/11774c859f9f55f816b754a65290a7/.command.sh
/home/ubuntu/session2/work/bc/0478d8de4d1c6df1413c50f4bffcb1/.command.sh
/home/ubuntu/session2/work/af/57d1741b614927225fe6381333d615/.command.sh
/home/ubuntu/session2/work/e6/6a644b0d85f03ec91cd2efe5a485d2/.command.sh
/home/ubuntu/session2/work/7d/ff697b987403d2f085b8b538260b67/.command.sh
/home/ubuntu/session2/work/3e/1b7b0f03c7c7c462a4593f77be544e/.command.sh
/home/ubuntu/session2/work/31/5e6865dbbbb164a87d2254b68670fa/.command.sh
/home/ubuntu/session2/work/79/93034bd48f5a0de82e79a1fd12f6ac/.command.sh
/home/ubuntu/session2/work/ca/bbfba0ea604d479bdc4870e9b3b4ce/.command.sh
/home/ubuntu/session2/work/ec/0a013bfb1f96d3c7170137262294e7/.command.sh
/home/ubuntu/session2/work/b7/37428bc5be1fd2c34e3911fb827334/.command.sh
/home/ubuntu/session2/work/57/a18fcea6a06565b14140ab06a3d077/.command.sh
```

Compared with the salmon quant `main.nf` file, we get a lot more fine scale details from the `.command.sh` process scripts: 

![](../figs/2.2_salmon-quant.png)

Looking at the nf-core/rnaseq documentation, we can see:

* Library type is automatically inferred based on the `$strandedness` variable
* Library type can be adjusted using Salmon's `--libType=` flag and the nf-core `$strandedness` variable. 

Following the [Salmon documentation](https://salmon.readthedocs.io/en/latest/salmon.html#what-s-this-libtype), we can override this default with the nf-core/rnaseq pipeline's `--salmon_quant_libtype U` [parameter](https://nf-co.re/rnaseq/3.11.1/parameters#salmon_quant_libtype) to indicate our data is unstrands. 

If we want to get rid of the warning message `Please check MultiQC report: 2/2 samples failed strandedness check`, we'll have to change the strandedness fields in our `samplesheet.csv`. Keep in mind, this will cause the pipeline to run from the beginning.  

### **2.2.4. Write a parameter file**

Nextflow accepts either YAML or JSON formats for parameter files. Any of the pipeline-specific [parameters](https://nf-co.re/rnaseq/3.11.1/parameters) can be supplied to a pipeline run command in this way. 

::: callout-tip
### **Challenge**{.unlisted}

Fill in the parameters file below and save as `workshop-params.yaml`:

```yaml
input: ""
outdir: ""
fasta: ""
gtf: ""
star_index: ""
salmon_index: ""
salmon_quant_libtype: ""
skip_markduplicates: 
save_trimmed: 
save_unaligned: 
```
:::

::: {.callout-caution collapse="true"}
### Solution

```yaml
input: "samplesheet.csv"
outdir: "Lesson-2.2"
fasta: "/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.fa"
gtf: "/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/mm10_chr18.gtf"
star_index: "/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/STAR"
salmon_index: "/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/UnlockNfcore_0523/mm10_reference/salmon-index"
salmon_quant_libtype: "U"
skip_markduplicates: true
save_trimmed: true
save_unaligned: true
```
:::

### **2.2.5. Use a parameter file in a pipeline run**

Once your params file has been saved, run:

```default
nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \
  --max_memory 6.GB \
  --max_cpus 2 \
  -profile singularity \
  -params-file workshop-params.yaml \
  -resume                        
```

The number of pipeline-specific parameters we've added to our run command has been significantly reduced. The only `--` parameters we've provided to the run command relate to how the pipeline is executed on our instances. These resource limits won't be applicable to our imaginary collaborator who will run the pipeline on a different infrastructure. 

As the workflow runs a second time, you will notice 4 things:

1. The command is much tidier thanks to offloading some parameters to the params file
2. The `-resume` flag. Nextflow has lots of [run options](https://www.nextflow.io/docs/latest/cli.html?highlight=resume#run) including the ability to use cached output!
3. Some processes will be pulled from the cache. These processes remain unaffected by our addition of a new parameter.  
4. This run of the pipline will complete in a much shorter time.

```default
-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-
-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-
Completed at: 21-Apr-2023 05:58:06
Duration    : 1m 51s
CPU hours   : 0.3 (82.2% cached)
Succeeded   : 11
Cached      : 55
```

::: {.callout-note}
### **Key points**
- A parameter file can be used to specify input parameters for any Nextflow workflow.
- Specify parameter files in a workflow run command using the `-params-file` flag. 
- Parameter files can be written in YAML or JSON file formats.  
:::