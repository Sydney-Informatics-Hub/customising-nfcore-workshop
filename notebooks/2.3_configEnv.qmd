---
title: "**2.3. Configuring a run for your environment**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

::: callout-tip
### Objectives{.unlisted}

- Learn how to check the default configurations that are applied to nf-core workflows
- Understand how to over-ride default configurations with custom configuration files
- Write a custom config file for your local environment and apply this configuration to your run  
- Use an alternative container source for a workflow process  
:::

In the previous exercises, we have explored how to customise a run with **workflow parameters** on the command line or within a parameters file. We will now look at **configuration settings**, which manage **how the workflow is implemented on your system**. 

A key feature of Nextflow is its ease of portability across different computational infrastructures. It achieves this by separating workflow implementation from the configuration settings required to execute it. This enables **reproducibility**, as simply sharing the parameters applied to a run (eg via your params file) is sufficient to enable any other researcher to replicate your results on any other computational platform. They would just need to adjust the configurations of their run to suit their infrastructure, and the results would be the same. 

In this lesson we will use configuration files to define specifications required to execute an nf-core pipeline on our compute environment. While nf-core workflows are designed to be portable and work out of the box, sometimes you will need to customise the workflow's configuration so that it can run on your environment. The nf-core developer community currently offer a number of ways to [configure nf-core workflows](https://nf-co.re/usage/configuration). 
<br/><br/>

### **2.3.1. Default nf-core configuration**

Recall that when a `main.nf` file is run for any Nextflow workflow, Nextflow looks for [configuration files in multiple locations](https://www.nextflow.io/docs/latest/config.html#configuration-file) to determine how to execute the workflow and its processes. 

Currently, all nf-core workflows use a `nextflow.config` file and a `conf/base.config` file to define the default execution settings and parameters of a workflow.   

The critical configuration aspects can be grouped into 3 key areas:

1. Parameters
2. Compute resources
3. Software/tool access
<br/><br/>

&#x27A4; Let's take a look at these two configuration files to gain an undertsanding of how defaults are applied.

Using the `more` command, take a few moments to scroll through both the `nextflow.config` and `base.config` files:
``` bash
more nf-core-rnaseq-3.11.1/workflow/conf/base.config
more nf-core-rnaseq-3.11.1/workflow/nextflow.config
```
<br/>

The `base.config` sets the default compute resource settings to be used by the processes in the nf-core workflow. It uses **process labels** to enable different sets of resources to be applied to groups of processes that require similar compute. These labels are specified within the `main.nf` file for a process. <span style="color:#000080;"> We can over-ride these default compute resources using a custom configuration file.</span>

The `nextflow.config` file is more workflow-specific, and sets the defaults for the workflow parameters, as well as defines profiles to change the default software access from `$PATH` to the specified access method, eg Singularity. <span style="color:#000080;"> We can over-ride these parameters on the command line or with a parameters file, and over-ride the default behaviour of searching for tools on `$PATH` by specifying a `-profile`.</span>  
<br/><br/>

::: callout-tip
### **Challenge**{.unlisted}
Thinking of the resource configuration settings we have applied to our rnaseq runs so far (`-profile singularity`, `--max_cpus`, `--max_memory`), if we were to run the nf-core/rnaseq workflow now without these custom configurations, do you think the run would complete successfully? 

If not, why?
:::

::: {.callout-caution collapse="true"}
### Solution
- The tools required by the different processes (eg STAR, salmon) would not be accessible, as they are not installed locally and saved to `$PATH` (we have been accessing them via the 'singularity' profile and using the saved images from our `$NXF_SINGULARITY_CACHEDIR` directory
- The first process to use more than 2 CPUs or 8 GB RAM would fail and cause the pipeline to exit with a fatal error 
:::
<br/>

The parameters `--max_cpus`, `--max_memory` and `--max_time` are applied as a cap for a workflow, to ensure that no single process attempts to use more resources than you have available on your platform. <span style="color:#000080;"> Setting these values to the maximum or near-maximum on your machine/node is an important customisation to prevent uneccessary failed runs.</span>  

Within `base.config`, the `check_max()` function over-rides the process resources *if the max setting is lower than the default setting for that process* 

![](../figs/2.3_check_max.png)


Challenge:

- What are the default settings for CPU, memory and walltime for the STAR align process?
- Have these defaults been changed from our applied customisations in the previous runs?

To uncover these answers, we need to understand what process label has been assigned to the STAR align process. 
```
more workflow/modules/nf-core/star/align/main.nf 
```

STAR align has the label `process_high` which has the settings 12 CPUs, 72 GB mem, 16 hrs walltime applied from within `base.config`. We have previosuly applied --max_cpus 2 and --max_memory 6, so the `check_max()`  function would have reduced the resources given to the STAR align process to 2 CPUs and 6 GB RAM. 

Just as we can reduce the maximum resources to the workflow, we can increase the resources through custom configuration, by utilising these process labels or isolating specific processes int he workflow using the 'withName' process selector (coming soon)

::: callout-tip
### **Challenge**{.unlisted}

1. What is the default aligner parameter being applied? 
2. What default max memory, cpu, and walltime resources have been specified?
3. What config file is loaded by default for all nf-core workflows? 
:::

::: {.callout-caution collapse="true"}
### Solution

1. Inside the Global default `params {}` section, on line 58 of the `nextflow.config` under `// Alignment`:
```default
aligner = 'star_salmon'
```

2. Inside the Global default `params {}` section, on lines 120-124 of the `nextflow.config` under `// Max resource options`:
```default
max_memory = '128.GB'
max_cpus   = 16
max_time   = '240.h'
```

3. Inside Global default `params {}` section, on line 128 of the `nextflow.config` under `// Load base.config by default for all pipelines`:
```default
includeConfig 'conf/base.config'
```
:::


### **2.3.2. When to use a custom config file**

There are a number of situations in which you may want to write a custom configuration file:

1. To override the default resource allocations of the workflow specified in the `nextflow.config` 
2. To override the default resource allocations for a process specified in `conf/base.config`
3. To use a different software installation method than those supported by nf-core 
4. To run a workflow on an HPC and interact with a job scheduler like PBSpro or SLURM 

Using a custom configuration file is good practice to ensure that your pipeline runs efficiently and reproducibly on your compute environment. It also allows you to easily share the pipeline with others who can use your custom config file to run it in the same computational environment. 

We will write a custom configuration file to override the default configurations of the workflow with those that are suitable for our Nimbus instances. We're going to replace 3 flags in our run command with this file:

1. `-profile singularity`
2.  `--max_memory 6.GB`
3. `--max_cpus 2`

::: {.callout-note}
### **Why should I be concerned with computational efficiency?** :earth_asia: 

Bioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices. This [paper](https://academic.oup.com/mbe/article/39/3/msac034/6526403) makes for an interesting read about the carbon footprint of bioinformatics workflows and tools!
:::

### **2.3.3. Customise resource configuration**

Open a new file called `custom-nimbus.config` and start writing some Nextflow code by adding: 

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {}
}
```
Using the [profiles scope](https://www.nextflow.io/docs/latest/config.html?highlight=scope#config-profiles) in a configuration file groups attributes that belong to the same profile, in our case **workshop**. Inside this **workflow** profile, let's remove the need for the `-profile singularity` flag from our run command by enabling Singularity by adding another scope called **Singularity**:

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {
    singularity {
      enabled     = true
      autoMounts  = true
      cacheDir    = "/home/ubuntu/singularity_cache"
    }}
  }
```

Nextflow has a number of [options for using Singularity](https://www.nextflow.io/docs/latest/config.html?highlight=singularity#scope-singularity) that allow you to control how containers are executed. We are using:

* `enabled` to use Singularity to manage containers automatically 
* `autoMounts` to allow Nextflow to automatically mount host paths when a container is executed
* `cacheDir` to specify the directory Singularity images can be pulled from 

Now let's address those two resource parameters `--max_memory 6.GB` and `--max_cpus 2`. At the same level as the `singularity {}` scope, add a parameters scope and specify each parameter underneath: 

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {
    singularity {
      enabled     = true
      autoMounts  = true
      cacheDir    = "/home/ubuntu/singularity_cache"
    }
    params {
      max_cpus   = 2
      max_memory = '6.GB'      
    }}
  }
```

::: {.callout-tip}
### Take note!
In Nextflow, scope organisation and heirarchy is indicated by curly bracket (`{}`) notation, not by text indentation!
:::

Rerun the pipeline:

```default
nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \
  -profile workshop \
  -c custom-nimbus.config \
  -params-file workshop-params.yaml \
  --outdir Lesson-3 \
  -resume
```

### **2.3.4. Apply an institutional config file**

We have created an nf-core config for Pawsey's Nimbus cloud and shared it at the [nf-core/configs repository](https://github.com/nf-core/configs/blob/master/docs/pawsey_nimbus.md). This config file was downloaded with the workflow code. Take a look:

```default
cat nf-core-rnaseq-3.11.1/configs/conf/pawsey_nimbus.config
```

This configuration file provides a few different profiles that match the [Nimbus instance flavours](https://support.pawsey.org.au/documentation/pages/viewpage.action?pageId=58000088#GetStartedwiththeNewNimbus-Flavours) currently available, as well as Docker and Singularity container engines. Take a look at the c2r8 scope, looks a lot like our `custom-nimbus.config`: 

```default
  c2r8 {
    params {
      max_cpus   = 2
      max_memory = '6.GB'
    }
  }
```

Let's rerun the workflow with the `-resume` function and the [institutional config parameters](https://nf-co.re/rnaseq/3.11.2/parameters#institutional-config-options), instead of our custom file and see if anything changes:

```default
nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \
  -profile pawsey_nimbus,singularity,c2r8 \
  -params-file workshop-params.yaml \
  --outdir Lesson-3 \
  -resume
```

In the nf-core/rnaseq pipeline's configuration message printed to the screen at run time, we can see the institutional config has been picked up and our resource parameters are correctly being applied. 

![](../figs/2.3_institutional_config.png)

::: {.callout-note}
### **Key points**
- takeaway 1
- takeaway 2
:::
