---
title: "**2.3. Configuring a run for your environment**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

::: callout-tip
### Objectives{.unlisted}

- Learn how to check the default configurations that are applied to nf-core workflows
- Understand how to over-ride default configurations with custom configuration files
- Write a custom config file for your local environment and apply this configuration to your run  
- Use an alternative container source for a workflow process  
:::

In the previous exercises, we have explored how to customise a run with **workflow parameters** on the command line or within a parameters file. We will now look at **configuration settings**, which manage **how the workflow is implemented on your system**. 

Nextflow's portability is achieved by separating **workflow implementation** (input data, custom parameters, etc) from the **configuration settings** (tool access, compute resources, etc) required to execute it. This portability facilitates **reproducibility**: by applying the same parameters as a colleague, and adjusting configurations to suit your platform, you can achieve the same results on any machine with no requirement to edit the code.  

In this lesson we will explroe the default nf-core configuration files then write and apply a **custom configuration file** to define specifications required to execute an nf-core pipeline on our compute environment. 

While nf-core workflows are designed to be portable and work 'out of the box', sometimes you will need to customise the workflow's configuration so that it can run on your environment. The nf-core developer community currently offer a number of ways to [configure nf-core workflows](https://nf-co.re/usage/configuration). 
<br/><br/>

### **2.3.1. Default nf-core configuration**

Recall that when a `main.nf` file is run for any Nextflow workflow, Nextflow looks for [configuration files in multiple locations](https://www.nextflow.io/docs/latest/config.html#configuration-file) to determine how to execute the workflow and its processes. 

Currently, all nf-core workflows use a `nextflow.config` file and a `conf/base.config` file to define the default execution settings and parameters of a workflow.   

The critical nf-core configuration aspects can be grouped into 3 key areas:

1. Parameters
2. Compute resources
3. Software/tool access
<br/><br/>

&#x27A4; Let's take a look at these two configuration files to gain an undertsanding of how defaults are applied.

Using the `more` command, take a few moments to scroll through both the `nextflow.config` and `base.config` files:
``` bash
more nf-core-rnaseq-3.11.1/workflow/conf/base.config
more nf-core-rnaseq-3.11.1/workflow/nextflow.config
```
<br/>

The `base.config` sets the default compute resource settings to be used by the processes in the nf-core workflow. It uses **process labels** to enable different sets of resources to be applied to groups of processes that require similar compute. These labels are specified within the `main.nf` file for a process. <span style="color:#000080;"> We can over-ride these default compute resources using a custom configuration file.</span>

The `nextflow.config` file is more workflow-specific, and sets the defaults for the workflow parameters, as well as defines profiles to change the default software access from `$PATH` to the specified access method, eg Singularity. <span style="color:#000080;"> We can over-ride these parameters on the command line or with a parameters file, and over-ride the default behaviour of searching for tools on `$PATH` by specifying a `-profile`.</span>  
<br/><br/>

::: callout-tip
### **Challenge**{.unlisted}
- Thinking of the resource configuration settings we have applied to our rnaseq runs so far (`-profile singularity`, `--max_cpus`, `--max_memory`), if we were to run the nf-core/rnaseq workflow now without these custom configurations, do you think the run would complete successfully on our training VMs? 
- If not, why?
:::

::: {.callout-caution collapse="true"}
### Solution
- The tools required by the different processes (eg STAR, salmon) would not be accessible, as they are not installed locally and saved to `$PATH` (we have been accessing them via the 'singularity' profile and using the saved images from our `$NXF_SINGULARITY_CACHEDIR` directory
- The first process to use more than 2 CPUs or 8 GB RAM would fail and cause the pipeline to exit with a fatal error; this is because nf-core pipelines check that the requested resources are available before attempting to execute a module.  
:::
<br/>

The parameters `--max_cpus`, `--max_memory` and `--max_time` are applied as a **cap** for a workflow, to ensure that no single process attempts to use more resources than you have available on your platform. <span style="color:#000080;"> Setting these values to the maximum or near-maximum on your machine/node is an important customisation to prevent uneccessary failed runs.</span>  

Default settings for `--max_cpus`, `--max_memory` and `--max_time` are applied within the nf-core workflow `nextflow.config` - these are generous values expecting to be over-ridden with your custom settings. 

Within `base.config`, the `check_max()` function over-rides the process resources *if the custom 'max' setting is lower than the default setting for that process*. 

![](../figs/2.3_check_max.png)
<br/><br/>

::: callout-tip
### **Challenge**{.unlisted}

- What are the default settings for CPU, memory and walltime for the `STAR_ALIGN` module?
- How have these defaults been changed from our applied customisations in the previous runs?
:::

::: {.callout-caution collapse="true"}
### Solution
To uncover these answers, we need to understand what **process label** has been assigned to the `STAR_ALIGN` module. 
```bash
more workflow/modules/nf-core/star/align/main.nf 
# or
grep label workflow/modules/nf-core/star/align/main.nf 
# then 
more nf-core-rnaseq-3.11.1/workflow/conf/base.config
```

`STAR_ALIGN` has the label `process_high` which has the settings 12 CPUs, 72 GB mem, 16 hrs walltime applied by the default `base.config`. We have previosuly applied `--max_cpus 2` and `--max_memory 6`, so the `check_max()` function would have reduced the resources given to the STAR alignment process to 2 CPUs and 6 GB RAM, while retaining the default max walltime. 
:::
<br/>

Just as we can reduce the maximum resources to the workflow, we can increase the resources through custom configuration, by utilising these process labels or isolating specific processes in the workflow using the 'withName' process selector. We will practice this in a later exercise. 
<br/><br/>


### **2.3.2. When to use a custom config file**

As we have just observed, nf-core pipelines ship with default configurations for software access and compute resources. These may not be suited to running on your compute environment. In these circumstances, you may benefit from using a **custom configuration file**. 

In our runs so far, we have avoided the use of custom configuration files by taking advantage of `-profile singularity` to access the required pipeline tools from singularity containers rather than from `$PATH`, and by taking advantage of the `max` parameters for CPU and memory. 

These are basic configurations. What if:

- We wanted to increase the resources used to take advantage of high CPU or high memory infrastructures?
- We wanted to run on a HPC with PBS Pro or SLURM job scheduling? 
- We wanted to execute specific modules on specific queues on a cluster?


*<span style="color:#000080;">These applications are where a custom configuration file is warranted.</span>* 
<br/><br/>

By recording these configurations in a file, we can:
  
  - Ensure that our pipeline runs **efficiently** and **reproducibly** on our compute environment
  - Easily **share** the custom config file with others to run the workflow in the same computational environment 
<br/><br/>

::: {.callout-note}
### **Why should I be concerned with computational efficiency?** :earth_asia: 

Bioinformatics relies on large-scale computational infrastructures and has a signficant carbon footprint due to the energy required to run computational workflows. We can optimise our worklfows to not only reduce their runtime, but also adopt more sustainable computing practices. This [paper](https://academic.oup.com/mbe/article/39/3/msac034/6526403) makes for an interesting read about the carbon footprint of bioinformatics workflows and tools!
:::
<br/><br/>

### **2.3.3. Institutional config files**

We can set these and other configurations within a custom configuration file that is specific to our institution; this is referred to as an **institutional config**. 

There is a [repository of institutional configs](https://github.com/nf-core/configs/tree/master/conf) for nf-core pipelines. These have been contributed to by the community. 

We have created an nf-core config for Pawseyâ€™s Nimbus cloud: this (and other institutional configs) was downloaded along with the workflow code. 
<br/><br/>

&#x27A4; View the available list of institutional configs we pulled down along with the workflow code:

``` bash
ls  nf-core-rnaseq-3.11.1/configs/conf
```


&#x27A4; Let's take a look at the Pawsey Nimbus config: 

```bash
more  nf-core-rnaseq-3.11.1/configs/conf/pawsey_nimbus.config
```
<br/>

::: callout-tip
### **Challenge**{.unlisted}

- What arguments would we apply to our command line to utilise the pawsey_nimbus.config and set the right configurations for software and compute for our VMs?

:bulb: Hint: the Nimbus training VM 'instance flavour' is 'c2r8' ie 2 CPU and 8 GB RAM. See the [Pawsey Nimbus nf-core config](https://github.com/nf-core/configs/blob/master/docs/pawsey_nimbus.md) documentation for help. 
:::

::: {.callout-caution collapse="true"}
### Solution
``` bash
-profile pawsey_nimbus,singularity,c2r8
``` 
To select the institutional config, we apply `-profile <config_basename>`. To select the desired profiles from within that config, we further add the `singularity` and `c2r8` profiles in a comma-delimited list. 
:::
<br/>


In the event where your institution does not have a publicly availabe configuration file and/or you want to apply your own customisations, you will need to **write your own config file**. 

:bulb: <span style="color:#000080;"> You can contribute to the nf-core community by [sharing your config!](https://github.com/nf-core/configs/tree/master#adding-a-new-config)</span>

For the sake of the exercise, let's assume there wasn't a Pawsey Nimbus config publicly available, and write our own that is specific to our 'c2r8' VMs. 
<br/><br/>

&#x27A4; Open a new file called `custom-nimbus.config` and start writing some Nextflow code by adding: 

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {}
}
```
Using the [profiles scope](https://www.nextflow.io/docs/latest/config.html?highlight=scope#config-profiles) in a configuration file groups attributes that belong to the same profile, in our case a profile we have chosen to name **workshop**. 
<br/><br/>

&#x27A4; Inside this **workflow** profile, let's remove the need for the `-profile singularity` flag from our run command by adding another scope called **singularity**:

::: {.callout-note collapse="true"}
### Singularity options
Nextflow has a number of [options for using Singularity](https://www.nextflow.io/docs/latest/config.html?highlight=singularity#scope-singularity) that allow you to control how containers are executed. We are using:

* `enabled` to use Singularity to manage containers automatically 
* `autoMounts` to allow Nextflow to automatically mount host paths when a container is executed
* `cacheDir` to specify the directory Singularity images can be pulled from 
:::

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {
    singularity {
      enabled     = true
      autoMounts  = true
      cacheDir    = "/home/ubuntu/singularity_cache"
    }
  }
}
```
<br/><br/>

&#x27A4; Now let's address those two resource parameters `--max_memory 6.GB` and `--max_cpus 2`. At the same level as the `singularity {}` scope, add a parameters scope and specify each parameter underneath: 

```default
// Nimbus nf-core workshop configuration profile

profiles {
  workshop {
    singularity {
      enabled     = true
      autoMounts  = true
      cacheDir    = "/home/ubuntu/singularity_cache"
    }
    params {
      max_cpus   = 2
      max_memory = '6.GB'      
    }
  }
}
```
<br/><br/>

&#x27A4; Add finally, add a profile description and set the cache behaviour to lenient:


::: {.callout-warning collapse="true"}
### Nextflow cache and resume
Workflow execution is [sometimes not resumed as expected](https://www.nextflow.io/blog/2019/demystifying-nextflow-resume.html). The [default behaviour of Nextflow cache keys](https://www.nextflow.io/docs/latest/process.html#cache) is to index the input files meta-data information. Reducing the cache stringency to 'lenient' means the files cache keys are based only on filesize and path, and can help to avoid unexpected re-run processes when `-resume` is in use. 
:::

```default
// Nimbus nf-core workshop configuration profile

params {
    config_profile_description  = 'Pawsey Nimbus c2r8 profile'
}

process {
    cache = 'lenient'
}

profiles {
  workshop {
    singularity {
      enabled     = true
      autoMounts  = true
      cacheDir    = "/home/ubuntu/singularity_cache"
    }
    params {
      max_cpus   = 2
      max_memory = '6.GB'      
    }
  }
}
```
<br/><br/>

::: {.callout-caution collapse="false"}
When customising nf-core workflows, **DO NOT ADD PARAMETERS to custom config files!** The case of 'max' resource settings is a rare exception to this rule.
:::

https://nf-co.re/usage/configuration#custom-configuration-files




### **2.3.4. Further customise a resource configuration file**


Recall how the Pawsey Nimbus config restricted the CPU and memory usage by setting max_cpus and max_memory within the `params` scope of the profile. This can be a simple and effective method, however does not provide a lot of granularity. 

For example, consider that you had access to a 16 core machine. If you appled `max_cpus 16` to the nf-core rnaseq workflow, the STAR_ALIGN module would still only utilise 12 CPUs, as this module (as we learned in 2.3.1) has the label `process_high` which sets 12 CPUs. In this case, you would have *up to 4 idle CPUs*\* while the STAR_ALIGN module completed for each sample. 

>\* Nextflow will fill available resources as the input channels for processes are fulfilled. It will depend on the specific workflow and the requirements of the modules as to whether the resources can be filled. For 'bottleneck' steps like alignment, optimising the utilisation of available compute so that no resources are idle is critical to ensuring an efficient run. 

<br/>
We can write a custom configuration file that fine-tunes resources per module using the process labels as well as the 'withName' process selector, to make the best use of our compute environment. 

> Note that the following exercise is trivial given the limitations of our VMs. In exercise 2.3.5, we will explore how this approach can be really powerful when working on a high performance cluster. 

<br/><br/>
&#x27A4; Open a file for editing and name it `custom_resource.config`

::: callout-tip
### **Challenge**{.unlisted}
Write a custom configuration file that adjusts the CPU, memory and time values for the process labels `process_low`, `process_medium` and `process_high`, setting all to 2 CPU, 6.GB and 2.m, respectively. 

:bulb: Hint: View the file `nf-core-rnaseq-3.11.1/workflow/conf/base.config` for syntax example. 
:::

 

> Recall that `base.config` is loaded by default by nf-core pipelines. Also recall the order of priority in which parameters and configurations are applied by nextflow. The settings we specify with  `-c custom_resource.config` will over-ride those that appear in the default nf-core configurations `conf/base.config` and `workflow/nextflow.config`. Settings that are not over-ridden by `-c <config>` or any parameter from params file or provided on the command line will still be set to the defaults. 

&#x27A4; Re-run the previous workflow with our custom configuration, setting `outdir` parameter to `Lesson-2.3.4`. 

::: callout-tip
### **Challenge**{.unlisted}
Apart from the `outdir` parameter, what needs to be added or omitted from the command line run to execute this workflow applying the resources set within our  `custom_resource.config`?
:::

::: {.callout-caution collapse="true"}
### Solution
- We need to add `-c custom_resource.config`
- We need to omit `max_cpus` and `max_memory`

The full run command will be: 
```bash
nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf  \
  -profile singularity \
  -params-file params.yaml \
  --outdir Lesson-2.3.4 \
  -c custom_resource.config \
  -resume
```
:::
<br/><br/>


&#x27A4; Open a file for editing and name it `star_align.config`

This exercise will demonstrate how to adjust the resource configurations for a specific process using the `withName` process selector 

To provide customised resources that apply *only* to the STAR alignment module, we need to ensure we have the right process name in our configuration file. Recall from [Session 1
configuring nf-core workflows](1.3_configure.md#1.3.5.-default-configuration-files) the following tips: 

- The extended execution path is built from the pipeline, workflow, subworkflow, and module  name
- It can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the `conf/modules.conf` file

You can [view the modules.conf file on Github](https://github.com/nf-core/rnaseq/blob/master/conf/modules.config) or search your local copy:

```bash
grep withName nf-core-rnaseq-3.11.1/workflow/conf/modules.config | grep STAR
```

::: callout-tip
### **Challenge**{.unlisted}
What extended execution path could we use to ensure our customisations were applied only to the STAR alignment process? 
:::

::: {.callout-caution collapse="true"}
### Solution
Any of the following would be correct and specific:
```
'NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN'
'.*:RNASEQ:ALIGN_STAR:STAR_ALIGN'
'.*:ALIGN_STAR:STAR_ALIGN'
```
:::
<br/><br/>

Now, edit your `star_align.config` to apply 24 CPU to the STAR_ALIGN process. 

Execute your run, applying your two custom resource configuration files: 

```bash
nextflow run nf-core-rnaseq-3.11.1/workflow/main.nf \
  -profile singularity \
  -params-file params-withSalmon.yaml \
  --outdir Lesson-2.3.4 \
  -c custom_resource.config,star_align.config \
  -resume
```

If your execution path for the STAR align process was specified correctly, your run should have died with the following error:

**insert star fatal screenshot** 
<br/><br/>

### **2.3.5. Specify a custom container**

>Note that for deploying nf-core workflows, it is not recommended to replace the tools within the workflow, as this will decrease portability and reproducibilty! This exercise is to demonstrate how you can specify containers, as this may aid you in developing and testing your own Nextflow workflows

For this example, we are going to test out the latest version of STAR. We have been using the default version of STAR that is executed with nf-core/rnaseq revision 3.11.1. We can easily view the versions of all tools used from the `software_versions.yml` file created by default in the `<outdir>/pipeline_info` directory by nf-core workflows OR by viewing the same information from the 'nf-core/rnaseq Software Versions' section of the MultiQC html report.  
<br/><br/>

&#x27A4; Identify the version of STAR that has been used in our runs so far from the `software_versions.yml` file. 

You might view the file by opening it from the VS Code explorer pane, using `more` command, or `grep` for the tool name.

```bash
more Exercise_2.3.4/pipeline_info/software_versions.yml

##
#STAR_ALIGN:
#  gawk: 5.1.0
#  samtools: 1.16.1
#  star: 2.7.9a
##
```

The process names are listed in alphabetical order, with the tools and versions used in the `script` section of the process listed below the process name. We can see for STAR_ALIGN process that version 2.7.9a of STAR was used. 
<br/><br/>

There is a [newer version of STAR](https://github.com/alexdobin/STAR) available, version 2.7.10b. Recall that we have been accessing some materials on [CernVM-FS](https://support.pawsey.org.au/documentation/display/US/Nimbus+for+Bioinformatics#Â ), a read-only filesystem which provides shared access to common bioinformatics reference datasets as well as Singularity containers of everything stored in [Biocontainers](https://biocontainers.pro/). CernVM-FS comes pre-mounted on Pawsey Nimbus VMs using the 'Pawsey Bio - Ubuntu 22.04 - 2023-03' image, so we can access and apply the STAR biocontainer by specifying the path to this remote image. 
<br/><br/>

&#x27A4; Edit your `star_align.config` file to apply the custom container 

> Containers are mounted at the path `/cvmfs/singularity.galaxyproject.org`, with sub-directories based on the first 2 letters of the tool name, eg `/cvmfs/singularity.galaxyproject.org/s/t/` for STAR containers. 

First, identify the container for the 2.7.10b version of STAR:

``` bash
ls /cvmfs/singularity.galaxyproject.org/s/t/star:*

##
# /cvmfs/singularity.galaxyproject.org/s/t/star:2.7.10b--h9ee0642_0
##
```
<br/>

Then, specify this container within the STAR_ALIGN process scope of your custom config.
Delete or comment out the resource requests from the previosu exercise.

``` bash
// Custom resources for STAR_ALIGN process 

process {
    withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {
       // cpus   = 24
       // memory = 96.GB
       // time   = 4.h
       container = '/cvmfs/singularity.galaxyproject.org/s/t/star:2.7.10b--h9ee0642_0'
    }
}
```

Execute the run
It fails because of the custom dump software versions process
It does not write the software_versions.yml file OR create any mutliqc output
Cna verify that the newer version of star was applied using samtools to view the bam header-
singularity run ~/singularity/depot.galaxyproject.org-singularity-samtools-1.16.1--h6899075_1.img
samtools view -H SRR3473988.markdup.sorted.bam | head


### **2.3.6. Institutional config files for HPC**

Nextflow's `executor` configuration scope allows us to easily change from the default execution (local) to [any other supported executor](https://www.nextflow.io/docs/latest/executor).html)

**Demo a HPC confi from the repo ** 



::: {.callout-note}
### **Key points**
- takeaway 1
- takeaway 2
:::
